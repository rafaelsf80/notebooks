{
  
    
        "post0": {
            "title": "AutoML NLP Classifier with large Confussion matrices",
            "content": "This notebook shows how to predict an existing trained model using AutoML NLP, in order to manually get bigger than 10x10 confussion matrix. . We will use a public dataset about Stack Overflow questions available in Google Cloud marketplace, that has been trained using AutoML NLP. You can explore the dataset in BigQuery just by following the instructions of the former link. In this notebook,the model is already built and deployed in AutoML NLP service. To keep things simple our pre-processed table includes questions containing 4 possible programming-related tags: Java, Javascript, Python or C#. Confussion matrix bigger than 10x10 will be implied. . BigQuery has a public dataset that includes more than 17 million Stack Overflow questions. We are going to download some posts labeled as one of the four most used languages today: java, javascript, python and C#, but to make this a harder problem to our model, we have replaced every instance of that word with another less used language today (but well-known some decades ago) called blank. Otherwise, it will be very easy for the model to detect that a post is a java-related post just by finding the word java on it. . You can access the pre-processed fortran-filled dataset as a tar file here. Each of the four labels has approximate 10k samples for training/eval and 10k samples for test. . Authenticate in Google Cloud from Colab . from google.colab import auth auth.authenticate_user() print(&#39;Authenticated&#39;) . Batch predict on AutoML NLP . We will make a batch predict on an already deployed AutoML NLP model . # Batch predict for an already trained AutoML NLP model from google.cloud import automl project_id = &quot;windy-site-254307&quot; model_id = &quot;TCN627409922111307776&quot; input_uri = &quot;gs://stackoverflow-automl-nlp/dataset_batchpredict.csv&quot; output_uri = &quot;gs://stackoverflow-automl-nlp&quot; prediction_client = automl.PredictionServiceClient() # Get the full path of the model. model_full_id = prediction_client.model_path( project_id, &quot;us-central1&quot;, model_id ) gcs_source = automl.types.GcsSource(input_uris=[input_uri]) input_config = automl.types.BatchPredictInputConfig(gcs_source=gcs_source) gcs_destination = automl.types.GcsDestination(output_uri_prefix=output_uri) output_config = automl.types.BatchPredictOutputConfig( gcs_destination=gcs_destination ) response = prediction_client.batch_predict( model_full_id, input_config, output_config ) print(&quot;Waiting for operation to complete...&quot;) print( &quot;Batch Prediction results saved to Cloud Storage bucket. {}&quot;.format( response.result() ) ) . # Download results from GCS. See first line for reference and modify URI accordingly result = !gsutil ls gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z file_list = [] for i in range(len(result)): file = result[i] !gsutil cp $file . file_list.append( result[i].split(&#39;/&#39;)[4].strip() ) . Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_1.jsonl... Operation completed over 1 objects/120.0 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_10.jsonl... Operation completed over 1 objects/221.5 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_11.jsonl... Operation completed over 1 objects/96.5 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_12.jsonl... Operation completed over 1 objects/24.6 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_13.jsonl... Operation completed over 1 objects/26.7 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_14.jsonl... Operation completed over 1 objects/10.7 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_15.jsonl... Operation completed over 1 objects/37.2 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_16.jsonl... Operation completed over 1 objects/60.5 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_17.jsonl... Operation completed over 1 objects/21.3 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_18.jsonl... Operation completed over 1 objects/14.8 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_19.jsonl... Operation completed over 1 objects/47.1 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_2.jsonl... Operation completed over 1 objects/87.8 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_20.jsonl... Operation completed over 1 objects/27.4 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_21.jsonl... Operation completed over 1 objects/8.2 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_22.jsonl... Operation completed over 1 objects/6.6 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_23.jsonl... Operation completed over 1 objects/206.2 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_24.jsonl... Operation completed over 1 objects/199.6 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_25.jsonl... Operation completed over 1 objects/60.5 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_26.jsonl... Operation completed over 1 objects/46.7 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_27.jsonl... Operation completed over 1 objects/59.4 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_28.jsonl... Operation completed over 1 objects/31.6 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_29.jsonl... Operation completed over 1 objects/9.2 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_3.jsonl... Operation completed over 1 objects/107.3 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_30.jsonl... Operation completed over 1 objects/4.1 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_31.jsonl... Operation completed over 1 objects/103.1 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_32.jsonl... Operation completed over 1 objects/94.4 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_4.jsonl... Operation completed over 1 objects/18.5 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_5.jsonl... Operation completed over 1 objects/45.5 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_6.jsonl... Operation completed over 1 objects/35.1 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_7.jsonl... Operation completed over 1 objects/64.5 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_8.jsonl... Operation completed over 1 objects/42.6 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_9.jsonl... / [0 files][ 0.0 B/ 98.1 KiB]/ [1 files][ 98.1 KiB/ 98.1 KiB] Operation completed over 1 objects/98.1 KiB. . Process result files . # Example of annotations # [{&#39;annotationSpecId&#39;: &#39;1249570775711612928&#39;, &#39;classification&#39;: {&#39;score&#39;: 0.020207971}, &#39;displayName&#39;: &#39;java&#39;}, {&#39;annotationSpecId&#39;: &#39;2402492280318459904&#39;, &#39;classification&#39;: {&#39;score&#39;: 0.96145684}, &#39;displayName&#39;: &#39;csharp&#39;}, {&#39;annotationSpecId&#39;: &#39;5861256794139000832&#39;, &#39;classification&#39;: {&#39;score&#39;: 0.0013875663000000001}, &#39;displayName&#39;: &#39;javascript&#39;}, {&#39;annotationSpecId&#39;: &#39;7014178298745847808&#39;, &#39;classification&#39;: {&#39;score&#39;: 0.017511099999999998}, &#39;displayName&#39;: &#39;python&#39;}] # Example of textSnippet # {&#39;contentUri&#39;: &#39;gs://stackoverflow-automl-nlp/test/csharp/1003.txt&#39;} import pandas as pd # Init enum from enum import Enum class Language(Enum): java = 0 csharp = 1 javascript = 2 python = 3 # Init vectors for the confussion matrix y_true = [] y_pred = [] # Read downloaded files one by one and generate y_true and y_pred vectos for the confussion matrix for file in file_list: #file=&quot;text_classification_1.jsonl&quot; # Read file df = pd.read_json(file, lines=True) print(&quot;Reading {0} annotations and {1} text snippets of {2}&quot;.format(len(df[&#39;annotations&#39;].to_list()), len(df[&#39;textSnippet&#39;].to_list()), file)) import json for i in range(len(df[&#39;annotations&#39;].to_list())): # Decode textSnippet and get true_label textsnippet_str = str(df[&#39;textSnippet&#39;].to_list()[i]).replace(&quot;&#39;&quot;, &#39;&quot;&#39;) textsnippet_decoded = json.loads(textsnippet_str) true_label = textsnippet_decoded[&#39;contentUri&#39;].split(&#39;/&#39;)[4].strip() y_true.append(true_label) # Decode annotations annotation_str = str(df[&#39;annotations&#39;].to_list()[i]).replace(&quot;&#39;&quot;, &#39;&quot;&#39;) annotation_decoded = json.loads(annotation_str) # Decode scores and add them to the corresponding line in the confussion matrix scores = [annotation_decoded[Language[&#39;java&#39;].value][&#39;classification&#39;][&#39;score&#39;], annotation_decoded[Language[&#39;csharp&#39;].value][&#39;classification&#39;][&#39;score&#39;], annotation_decoded[Language[&#39;javascript&#39;].value][&#39;classification&#39;][&#39;score&#39;], annotation_decoded[Language[&#39;python&#39;].value][&#39;classification&#39;][&#39;score&#39;]] max_value = max(scores) max_index = scores.index(max_value) y_pred.append(Language(max_index).name) . Reading 236 annotations and 236 text snippets of text_classification_1.jsonl Reading 435 annotations and 435 text snippets of text_classification_10.jsonl Reading 190 annotations and 190 text snippets of text_classification_11.jsonl Reading 48 annotations and 48 text snippets of text_classification_12.jsonl Reading 52 annotations and 52 text snippets of text_classification_13.jsonl Reading 21 annotations and 21 text snippets of text_classification_14.jsonl Reading 73 annotations and 73 text snippets of text_classification_15.jsonl Reading 119 annotations and 119 text snippets of text_classification_16.jsonl Reading 42 annotations and 42 text snippets of text_classification_17.jsonl Reading 29 annotations and 29 text snippets of text_classification_18.jsonl Reading 92 annotations and 92 text snippets of text_classification_19.jsonl Reading 172 annotations and 172 text snippets of text_classification_2.jsonl Reading 54 annotations and 54 text snippets of text_classification_20.jsonl Reading 16 annotations and 16 text snippets of text_classification_21.jsonl Reading 13 annotations and 13 text snippets of text_classification_22.jsonl Reading 405 annotations and 405 text snippets of text_classification_23.jsonl Reading 392 annotations and 392 text snippets of text_classification_24.jsonl Reading 119 annotations and 119 text snippets of text_classification_25.jsonl Reading 92 annotations and 92 text snippets of text_classification_26.jsonl Reading 116 annotations and 116 text snippets of text_classification_27.jsonl Reading 62 annotations and 62 text snippets of text_classification_28.jsonl Reading 18 annotations and 18 text snippets of text_classification_29.jsonl Reading 211 annotations and 211 text snippets of text_classification_3.jsonl Reading 8 annotations and 8 text snippets of text_classification_30.jsonl Reading 203 annotations and 203 text snippets of text_classification_31.jsonl Reading 185 annotations and 185 text snippets of text_classification_32.jsonl Reading 36 annotations and 36 text snippets of text_classification_4.jsonl Reading 89 annotations and 89 text snippets of text_classification_5.jsonl Reading 69 annotations and 69 text snippets of text_classification_6.jsonl Reading 127 annotations and 127 text snippets of text_classification_7.jsonl Reading 84 annotations and 84 text snippets of text_classification_8.jsonl Reading 192 annotations and 192 text snippets of text_classification_9.jsonl . Get confusion matrix . from sklearn.metrics import confusion_matrix matriz_de_confusion = confusion_matrix(y_true, y_pred, labels=[&quot;java&quot;, &quot;javascript&quot;, &quot;csharp&quot;, &quot;python&quot;]) #DO NOT USE THIS MATRIX #conf_max_12x12 = confusion_matrix(y_train, y_train_pred) #conf_max_12x12=([[1000, 3, 24, 9, 10, 49, 49, 50, 26, 23, 12, 98 ], # [ 23, 2000, 24, 9, 10, 49, 49, 50, 26, 23, 12, 98 ], # [ 56, 3, 1300, 9, 10, 49, 49, 50, 26, 23, 12, 98 ], # [ 23, 3, 24, 1400, 10, 49, 49, 50, 26, 23, 12, 98 ], # [ 35, 3, 24, 9, 1500, 49, 49, 50, 26, 23, 12, 98 ], # [ 35, 3, 24, 9, 10, 1400, 49, 50, 26, 23, 12, 98 ], # [ 35, 3, 24, 9, 10, 49, 1300, 50, 26, 23, 12, 98 ], # [ 35, 3, 24, 9, 10, 49, 49, 1200, 26, 23, 12, 98 ], # [ 35, 3, 24, 9, 10, 49, 49, 50, 1100, 23, 12, 98 ], # [ 35, 3, 24, 9, 10, 49, 49, 50, 26, 1000, 12, 98 ], # [ 35, 3, 24, 9, 10, 49, 49, 50, 26, 23, 1100, 98 ], # [ 35, 3, 24, 9, 10, 49, 49, 50, 26, 23, 12, 1200 ]]) . #!pip3 install seaborn import matplotlib.pyplot as plt import numpy as np import seaborn as sns # Normalise and Plot target_names = [&#39;java&#39;, &#39;csharp&#39;, &#39;javascript&#39;, &#39;python&#39;] cmn = matriz_de_confusion.astype(&#39;float&#39;) / matriz_de_confusion.sum(axis=1)[:, np.newaxis] fig, ax = plt.subplots(figsize=(10,10)) sns.heatmap(cmn, annot=True, fmt=&#39;.2f&#39;, xticklabels=target_names, yticklabels=target_names) plt.ylabel(&#39;Actual&#39;) plt.xlabel(&#39;Predicted&#39;) plt.show(block=False) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt;",
            "url": "https://rafaelsf80.github.io/notebooks/googlecloud/automl/2020/06/26/automl-nlp-classifier-with-confussion-matrix.html",
            "relUrl": "/googlecloud/automl/2020/06/26/automl-nlp-classifier-with-confussion-matrix.html",
            "date": " • Jun 26, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Text Classification using TextVectorization layer",
            "content": "##### Copyright 2020 The TensorFlow Authors. . #@title Licensed under the Apache License, Version 2.0 (the &quot;License&quot;); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an &quot;AS IS&quot; BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. . #@title MIT License # # Copyright (c) 2017 François Chollet # # Permission is hereby granted, free of charge, to any person obtaining a # copy of this software and associated documentation files (the &quot;Software&quot;), # to deal in the Software without restriction, including without limitation # the rights to use, copy, modify, merge, publish, distribute, sublicense, # and/or sell copies of the Software, and to permit persons to whom the # Software is furnished to do so, subject to the following conditions: # # The above copyright notice and this permission notice shall be included in # all copies or substantial portions of the Software. # # THE SOFTWARE IS PROVIDED &quot;AS IS&quot;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL # THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING # FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER # DEALINGS IN THE SOFTWARE. . Multiclass text classification from scratch . View on TensorFlow.org | Run in Google Colab | View source on GitHub | Download notebook | This notebook contains a walkthrough of text classification from scratch, starting from a directory of plain text files (a common scenario in practice). The first part demonstrates sentiment analysis, using an IMDB dataset. We demonstrate multiclass text classification, using a dataset of Stack Overflow questions. . TODO Also in the CL, update the title of the other text classification notebook to use transfer learning. . !pip install -q tf-nightly import tensorflow as tf . ERROR:root:code for hash md5 was not found. Traceback (most recent call last): File &#34;/usr/local/Cellar/python@2/2.7.16/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py&#34;, line 147, in &lt;module&gt; globals()[__func_name] = __get_hash(__func_name) File &#34;/usr/local/Cellar/python@2/2.7.16/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py&#34;, line 97, in __get_builtin_constructor raise ValueError(&#39;unsupported hash type &#39; + name) ValueError: unsupported hash type md5 ERROR:root:code for hash sha1 was not found. Traceback (most recent call last): File &#34;/usr/local/Cellar/python@2/2.7.16/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py&#34;, line 147, in &lt;module&gt; globals()[__func_name] = __get_hash(__func_name) File &#34;/usr/local/Cellar/python@2/2.7.16/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py&#34;, line 97, in __get_builtin_constructor raise ValueError(&#39;unsupported hash type &#39; + name) ValueError: unsupported hash type sha1 ERROR:root:code for hash sha224 was not found. Traceback (most recent call last): File &#34;/usr/local/Cellar/python@2/2.7.16/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py&#34;, line 147, in &lt;module&gt; globals()[__func_name] = __get_hash(__func_name) File &#34;/usr/local/Cellar/python@2/2.7.16/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py&#34;, line 97, in __get_builtin_constructor raise ValueError(&#39;unsupported hash type &#39; + name) ValueError: unsupported hash type sha224 ERROR:root:code for hash sha256 was not found. Traceback (most recent call last): File &#34;/usr/local/Cellar/python@2/2.7.16/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py&#34;, line 147, in &lt;module&gt; globals()[__func_name] = __get_hash(__func_name) File &#34;/usr/local/Cellar/python@2/2.7.16/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py&#34;, line 97, in __get_builtin_constructor raise ValueError(&#39;unsupported hash type &#39; + name) ValueError: unsupported hash type sha256 ERROR:root:code for hash sha384 was not found. Traceback (most recent call last): File &#34;/usr/local/Cellar/python@2/2.7.16/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py&#34;, line 147, in &lt;module&gt; globals()[__func_name] = __get_hash(__func_name) File &#34;/usr/local/Cellar/python@2/2.7.16/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py&#34;, line 97, in __get_builtin_constructor raise ValueError(&#39;unsupported hash type &#39; + name) ValueError: unsupported hash type sha384 ERROR:root:code for hash sha512 was not found. Traceback (most recent call last): File &#34;/usr/local/Cellar/python@2/2.7.16/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py&#34;, line 147, in &lt;module&gt; globals()[__func_name] = __get_hash(__func_name) File &#34;/usr/local/Cellar/python@2/2.7.16/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py&#34;, line 97, in __get_builtin_constructor raise ValueError(&#39;unsupported hash type &#39; + name) ValueError: unsupported hash type sha512 Traceback (most recent call last): File &#34;/usr/local/bin/pip&#34;, line 8, in &lt;module&gt; sys.exit(main()) File &#34;/usr/local/lib/python2.7/site-packages/pip/_internal/cli/main.py&#34;, line 73, in main command = create_command(cmd_name, isolated=(&#34;--isolated&#34; in cmd_args)) File &#34;/usr/local/lib/python2.7/site-packages/pip/_internal/commands/__init__.py&#34;, line 96, in create_command module = importlib.import_module(module_path) File &#34;/usr/local/Cellar/python@2/2.7.16/Frameworks/Python.framework/Versions/2.7/lib/python2.7/importlib/__init__.py&#34;, line 37, in import_module __import__(name) File &#34;/usr/local/lib/python2.7/site-packages/pip/_internal/commands/install.py&#34;, line 24, in &lt;module&gt; from pip._internal.cli.req_command import RequirementCommand File &#34;/usr/local/lib/python2.7/site-packages/pip/_internal/cli/req_command.py&#34;, line 15, in &lt;module&gt; from pip._internal.index.package_finder import PackageFinder File &#34;/usr/local/lib/python2.7/site-packages/pip/_internal/index/package_finder.py&#34;, line 21, in &lt;module&gt; from pip._internal.index.collector import parse_links File &#34;/usr/local/lib/python2.7/site-packages/pip/_internal/index/collector.py&#34;, line 12, in &lt;module&gt; from pip._vendor import html5lib, requests File &#34;/usr/local/lib/python2.7/site-packages/pip/_vendor/requests/__init__.py&#34;, line 43, in &lt;module&gt; from pip._vendor import urllib3 File &#34;/usr/local/lib/python2.7/site-packages/pip/_vendor/urllib3/__init__.py&#34;, line 7, in &lt;module&gt; from .connectionpool import HTTPConnectionPool, HTTPSConnectionPool, connection_from_url File &#34;/usr/local/lib/python2.7/site-packages/pip/_vendor/urllib3/connectionpool.py&#34;, line 29, in &lt;module&gt; from .connection import ( File &#34;/usr/local/lib/python2.7/site-packages/pip/_vendor/urllib3/connection.py&#34;, line 40, in &lt;module&gt; from .util.ssl_ import ( File &#34;/usr/local/lib/python2.7/site-packages/pip/_vendor/urllib3/util/__init__.py&#34;, line 7, in &lt;module&gt; from .ssl_ import ( File &#34;/usr/local/lib/python2.7/site-packages/pip/_vendor/urllib3/util/ssl_.py&#34;, line 8, in &lt;module&gt; from hashlib import md5, sha1, sha256 ImportError: cannot import name md5 . import numpy as np from tensorflow.keras import preprocessing print(tf.__version__) . 2.2.0-dev20200507 . Multiclass text classification . The second part of this notebook classifies Stack Overflow posts as one of the most used languages today, namely Java, Javascript, Python or C#. This is an example of multiclass classification, different from the binary classification showed in Part 1. . We will use a public dataset about Stack Overflow questions available in Google Cloud marketplace. You can explore the dataset in BigQuery just by following the instructions of the former link. In this notebook, you will build a model to predict the tags of questions from Stack Overflow, using a pre-processed table already built and coming from the BigQuery dataset. To keep things simple our pre-processed table includes questions containing 4 possible programming-related tags: Java, Javascript, Python or C# . As in Part 1, Part 2 of this notebook uses tf.keras to build and train models in TensorFlow, as well as some TensorFlow experimental features, like the TextVectorization layer for word splitting &amp; indexing. . Download the BigQuery dataset . BigQuery has a public dataset that includes more than 17 million Stack Overflow questions. We are going to download some posts labeled as one of the four most used languages today: java, javascript, python and C#, but to make this a harder problem to our model, we have replaced every instance of that word with another less used language today (but well-known some decades ago) called fortran. Otherwise, it will be very easy for the model to detect that a post is a java-related post just by finding the word java on it. . You can access the pre-processed fortran-filled dataset as a tar file here. Each of the four labels has approximate 10k samples for training/eval and 10k samples for test. . Let&#39;s first import libraries to make Part 2 of this notebook independent from Part 1, and then download our pre-processed dataset. . !gsutil cp gs://tensorflow-blog-rnn/so_posts_4labels_blank_80k.tar.gz . # TODO: MOVE DATASET TO TF.ORG !tar -xf so_posts_4labels_blank_80k.tar.gz . Updates are available for some Cloud SDK components. To install them, please run: $ gcloud components update Copying gs://tensorflow-blog-rnn/so_posts_4labels_blank_80k.tar.gz... / [0 files][ 0.0 B/ 29.0 MiB]-- [0 files][ 2.1 MiB/ 29.0 MiB] [0 files][ 13.4 MiB/ 29.0 MiB]|// [0 files][ 24.0 MiB/ 29.0 MiB]-- [1 files][ 29.0 MiB/ 29.0 MiB] Operation completed over 1 objects/29.0 MiB. . batch_size = 32 raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory( &#39;train&#39;, batch_size=batch_size, validation_split=0.2, subset=&#39;training&#39;, seed=42) raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory( &#39;train&#39;, batch_size=batch_size, validation_split=0.2, subset=&#39;validation&#39;, seed=42) raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory( &#39;test&#39;, batch_size=batch_size) . Found 40000 files belonging to 4 classes. Using 32000 files for training. Found 40000 files belonging to 4 classes. Using 8000 files for validation. Found 40000 files belonging to 4 classes. . import time start = time.time() for text_batch, label_batch in raw_train_ds: pass end = time.time() print(end - start) . 8.353302001953125 . import time start = time.time() for text_batch, label_batch in raw_train_ds: pass end = time.time() print(end - start) . 2.060072183609009 . raw_train_ds = raw_train_ds.cache() import time start = time.time() for text_batch, label_batch in raw_train_ds: pass end = time.time() print(end - start) . 2.046962022781372 . import time start = time.time() for text_batch, label_batch in raw_train_ds: pass end = time.time() print(end - start) . 0.14096736907958984 . Explore the data . The dataset comes pre-processed, by replacing key words java, javascript, C# or python by fortran. In total, there are 4 labels (classes). . Note again that, as in Part 1, we can evaluate tensors using .numpy(), thanks to the eager execution of TensorFlow 2: . import time for text_batch, label_batch in raw_train_ds.take(1): for i in range(5): print(text_batch.numpy()[i]) print(label_batch.numpy()[i]) . b&#39;how do i find missing dates in a list of sorted dates? in blank how do i find all the missing days in a sorted list of dates? n&#39; 3 b&#39;&#34;find the sequences of numbers in the list? there is a task to find all the sequences of numbers in the list, then add them another list. for example, there is such a sequence of numbers in list ... 12222533343332...only numbers must appear in the resulting list like this 44 77 88 000 a prerequisite is that repeated numbers must stand side by side .for example, so ... 5122225333433325...5 should not fall into the resulting list because they are not near each other, respectively (not a sequence)..list&amp;lt;integer&amp;gt; toplist = new arraylist&amp;lt;&amp;gt;();. list&amp;lt;integer&amp;gt; result = new arraylist&amp;lt;&amp;gt;();. int count = 0;. boolean flag = true;.. while (count &amp;lt; toplist.size()){. while (flag) {. for (int j = count + 1; j &amp;lt; toplist.size(); j++) {. if (toplist.get(count).equals(toplist.get(j))) {. result.add(toplist.get(j));. system.out.println(result);. flag = false;. }else {. flag = true;. }. }. count++;. }. }...i try to compare the elements in pairs and add them to the sheet, but it is added to a couple of more elements for example instead of 22222, i get 222222. and instead of 333 and one more sequence 333. i get 333 and 33. how can i improve?&#34; n&#39; 1 b&#39;&#34;is there a standard function code like `lambda x, y: x.custom_method(y)`? i know that i can call magic methods using functions from operator module, for example:..operator.add(a, b)...is equal to..a.__add__(b)...is there a standard function for calling a custom method (like operator.methodcaller but also accepts method arguments when called)?.currently i have code like this:..def methodapply(name):. &#34;&#34;&#34;&#34;&#34;&#34;apply a custom method... usage:. methodapply( &#39;some &#39;)(a, *args, **kwargs) =&amp;gt; a.some(*args, **kwargs).. &#34;&#34;&#34;&#34;&#34;&#34;. def func(instance, *args, **kwargs):. return getattr(instance, name)(*args, **kwargs). func.__doc__ = &#34;&#34;&#34;&#34;&#34;&#34;call {!r} instance method&#34;&#34;&#34;&#34;&#34;&#34;.format(name). return func&#34; n&#39; 3 b&#39;&#34;blank: refer to objects dynamically apologies if this is a silly question...i have a list of potential dictionary keys here:.. form_fields = [ &#39;sex &#39;,. &#39;birth &#39;,. &#39;location &#39;,. &#39;politics &#39;]...i am currently manually adding values to these keys like so:.. self.participant.vars[&#34;&#34;sex&#34;&#34;] = [constants.fields_dict[&#34;&#34;sex&#34;&#34;][0], constants.fields_dict[&#34;&#34;sex&#34;&#34;][1], self.player.name]. self.participant.vars[&#34;&#34;birth&#34;&#34;] = [constants.fields_dict[&#34;&#34;birth&#34;&#34;][0], constants.fields_dict[&#34;&#34;birth&#34;&#34;][1],self.player.age]. self.participant.vars[&#34;&#34;location&#34;&#34;] = [constants.fields_dict[&#34;&#34;location&#34;&#34;][0], constants.fields_dict[&#34;&#34;location&#34;&#34;][1],self.player.politics]...i &#39;d like to be able to do a use a for loop to do this all at once like so:..for i in form_fields:. self.participant.vars[i] = [constants.fields_dict[i][0], constants.fields_dict[i][1], self.player.`i`]...obviously, however, i can &#39;t reference the object self.player.i like that. is there a way to reference that object dynamically?&#34; n&#39; 3 b&#39;&#34;list.split output trouble i was practicing some coding and i decided to make a parrot translator. the basic point of this game is, that after every word in a sentence, you should put the syllable &#34;&#34;pa&#34;&#34;. i had written the code for that:.. print(&#34;&#34;this is the parrot translator!&#34;&#34;). original = input(&#34;&#34;please enter a sentence you want to translate: &#34;&#34;).. words = list(original.split()).. for words in words:. print(words + &#34;&#34;pa&#34;&#34;)...but the problem i have and i dont know how to fix is, when i split the sentence, the output wont be in the same line, but every word will be at it &#39;s own.&#34; n&#39; 3 . Each label is an integer value between 0 and 3, correponsing to one of our four labels (0 to 3): . Prepare data for training . Since the data is pre-processed, we do not need to make any additional steps like removing HTML tags, as we did in Part 1 of this notebook. . We can go directly to instantiate our text vectorization layer (experimental feature). We are using this layer to normalize, split, and map strings to integers, so we set our output_mode to int. We also set the same constants as Part 1 for the model, like an explicit maximum sequence_length. . from tensorflow.keras.layers.experimental.preprocessing import TextVectorization max_features = 5000 embedding_dim = 128 sequence_length = 500 vectorize_layer = TextVectorization( max_tokens=max_features, output_mode=&#39;int&#39;, output_sequence_length=sequence_length) . # Make a text-only dataset (no labels) and call adapt text_ds = raw_train_ds.map(lambda x, y: x) vectorize_layer.adapt(text_ds) . Vectorize the data . def vectorize_text(text, label): text = tf.expand_dims(text, -1) return vectorize_layer(text), label # Vectorize the data. train_ds = raw_train_ds.map(vectorize_text) val_ds = raw_val_ds.map(vectorize_text) test_ds = raw_test_ds.map(vectorize_text) # Do async prefetching / buffering of the data for best performance on GPU. train_ds = train_ds.cache().prefetch(buffer_size=10) val_ds = val_ds.cache().prefetch(buffer_size=10) test_ds = test_ds.cache().prefetch(buffer_size=10) . The vectorization layer transforms each input word of a sentence into a numerical representation, i.e. a list of token indices or vocabulary, with size defined by max_features (5000). Note that the output size is fixed, truncated by sequence_length (500), regardless of how many tokens resulted from the previous step, and this will be the input to our model. . Let&#39;s take a moment to understand the output of the vectorization layer. The output of each sentence is fixed to 500 integers, as stated by sequence_length. It should be noted that most of the values are zero, and this is due to the fact that there is no corresponding token in our vocabulary. . for text_batch, label_batch in train_ds.take(1): for i in range(5): print(text_batch.numpy()[i]) print(label_batch.numpy()[i]) . [ 22 40 3 139 490 1037 6 5 53 9 1131 1037 6 16 22 40 3 139 73 2 490 711 6 5 1131 53 9 1037 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] 3 [ 139 2 3705 9 170 6 2 53 66 7 5 604 4 139 73 2 3705 9 170 6 2 53 87 132 181 159 53 12 138 66 7 300 5 846 9 170 6 53 1 170 310 918 6 2 2217 53 46 13 2046 3045 2928 1177 5 1 7 14 1732 170 310 4466 779 80 779 12 138 51 1 91 21 3240 99 2 2217 53 193 208 60 21 2668 115 142 3880 21 5 1 1 15 1790 3831 131 15 1790 29 185 19 248 1008 89 111 185 61 1 111 1008 12 29 172 185 27 172 61 1 172 11 1 1 1 1008 113 54 1008 89 185 3 118 4 501 2 283 6 1491 8 132 181 4 2 1854 23 10 7 407 4 5 1512 9 169 283 12 138 262 9 1 3 41 1 8 262 9 1 8 71 169 846 1 3 41 1 8 1325 22 34 3 2094 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] 1 [ 7 66 5 1013 39 28 46 1390 85 157 1 3 95 14 3 34 148 2146 251 48 303 33 570 415 12 1 1 580 1 66 5 1013 39 12 446 5 648 64 46 1 23 173 1859 64 503 47 1 3 17 28 46 2810 1 1120 5 648 64 1315 1 155 2061 58 1 2061 162 1 155 2061 25 1 1 2061 1 148 296 256 1 25 1432 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] 3 [ 16 1652 4 222 736 4105 11 13 7 5 2685 3811 17 5 53 9 3815 367 623 105 1 1 2954 492 1 35 412 1252 421 128 4 227 623 46 51 1 1 1 1 1 1 1 1 1 1 46 4 32 229 4 40 5 70 5 12 143 4 40 13 73 62 441 46 1 3 6 1 1 1 1 1 241 3 158 339 2 63 1 46 14 7 66 5 81 4 339 14 63 736 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] 3 [ 1 126 718 3 120 1 83 832 8 3 1972 4 112 5 1 1 2 668 311 9 13 253 7 14 151 236 213 6 5 891 56 91 286 2 1 1 3 532 486 2 28 12 14 1 7 2 1 1 562 2389 187 5 891 56 43 4 2175 330 1 12 330 6 330 1 1 2 121 3 17 8 3 130 95 22 4 399 7 47 3 502 2 891 2 126 612 32 6 2 116 72 23 236 213 74 32 62 97 622 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] 3 . Build the model . The input data consists of an array of integer-encoded vocabulary, with a fixed size. The labels to predict are between 0 and 3, so instead of using a binary classifier, we will use a softmax classifier. We compile the model with an Adam optimizer and a different loss function from Part 1 (Sparse categorical crossentropy). . One of the parameters of the embedding layer is max_features+1and not max_features, and the reason is to add an extra token for an unknown word to our vocabulary in the input string. . from tensorflow.keras import layers # A integer input for vocab indices. inputs = tf.keras.Input(shape=(None,), dtype=&#39;int64&#39;) x = layers.Embedding(max_features + 1, embedding_dim)(inputs) x = layers.Bidirectional(layers.LSTM(128))(x) predictions = layers.Dense(4, activation=&#39;softmax&#39;, name=&#39;predictions&#39;)(x) model = tf.keras.Model(inputs, predictions) model.compile( loss=&#39;sparse_categorical_crossentropy&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;]) . Train the model . Train the model by passing the Dataset object to the model&#39;s fit function. Set the number of epochs. . epochs = 5 # Fit the model using the train and test datasets. history = model.fit( train_ds, validation_data=val_ds, epochs=epochs) . Epoch 1/5 11/1000 [..............................] - ETA: 8:49 - loss: 1.3928 - accuracy: 0.2472 . KeyboardInterrupt Traceback (most recent call last) &lt;ipython-input-18-25f03af6a77d&gt; in &lt;module&gt; 5 train_ds, 6 validation_data=val_ds, -&gt; 7 epochs=epochs) ~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs) 70 def _method_wrapper(self, *args, **kwargs): 71 if not self._in_multi_worker_mode(): # pylint: disable=protected-access &gt; 72 return method(self, *args, **kwargs) 73 74 # Running inside `run_distribute_coordinator` already. ~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing) 905 batch_size=batch_size): 906 callbacks.on_train_batch_begin(step) --&gt; 907 tmp_logs = train_function(iterator) 908 if data_handler.should_sync: 909 context.async_wait() ~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds) 764 else: 765 compiler = &#34;nonXla&#34; --&gt; 766 result = self._call(*args, **kwds) 767 768 new_tracing_count = self._get_tracing_count() ~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds) 791 # In this case we have created variables on the first call, so we run the 792 # defunned version which is guaranteed to never create variables. --&gt; 793 return self._stateless_fn(*args, **kwds) # pylint: disable=not-callable 794 elif self._stateful_fn is not None: 795 # Release the lock early so that multiple threads can perform the call ~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs) 2810 with self._lock: 2811 graph_function, args, kwargs = self._maybe_define_function(args, kwargs) -&gt; 2812 return graph_function._filtered_call(args, kwargs) # pylint: disable=protected-access 2813 2814 @property ~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs, cancellation_manager) 1836 resource_variable_ops.BaseResourceVariable))), 1837 captured_inputs=self.captured_inputs, -&gt; 1838 cancellation_manager=cancellation_manager) 1839 1840 def _call_flat(self, args, captured_inputs, cancellation_manager=None): ~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager) 1913 # No tape is watching; skip to running the function. 1914 return self._build_call_outputs(self._inference_function.call( -&gt; 1915 ctx, args, cancellation_manager=cancellation_manager)) 1916 forward_backward = self._select_forward_and_backward_functions( 1917 args, ~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager) 547 inputs=args, 548 attrs=attrs, --&gt; 549 ctx=ctx) 550 else: 551 outputs = execute.execute_with_cancellation( ~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name) 58 ctx.ensure_initialized() 59 tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name, &gt; 60 inputs, attrs, num_outputs) 61 except core._NotOkStatusException as e: 62 if name is not None: KeyboardInterrupt: . model.summary() . Evaluate the model . And let&#39;s see how the model performs. Two values will be returned: loss (a number which represents our error, lower values are better), and accuracy. . loss, accuracy = model.evaluate(test_ds) print(&quot;Loss: &quot;, loss) print(&quot;Accuracy: &quot;, accuracy) . Learn more . This notebook uses tf.keras, a high-level API to build and train models in TensorFlow. For a more advanced text classification tutorial using tf.keras, see the MLCC Text Classification Guide. In this notebook, we also use some TensorFlow experimental features, like the TextVectorization layer for word splitting &amp; indexing. .",
            "url": "https://rafaelsf80.github.io/notebooks/tensorflow/keras/2020/06/25/text-classifier-with-textvectorization-layer.html",
            "relUrl": "/tensorflow/keras/2020/06/25/text-classifier-with-textvectorization-layer.html",
            "date": " • Jun 25, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Explainable AI in Spanish",
            "content": "Disclaimer: La versión original en inglés de este notebook está en este Codelab . import itertools import numpy as np import pandas as pd import tensorflow as tf import json import matplotlib.pyplot as plt from sklearn.utils import shuffle from sklearn.metrics import confusion_matrix . Descargar dataset . !gsutil cp gs://financial_fraud_detection/fraud_data_kaggle.csv . . Copying gs://financial_fraud_detection/fraud_data_kaggle.csv... | [1 files][470.7 MiB/470.7 MiB] Operation completed over 1 objects/470.7 MiB. . Es un dataset sintético de Kaggle, con trasacciones fraudulentas. Son 6.3 millones de registros, y 8000 son fraudulentas (sólo el 0.1!) . data = pd.read_csv(&#39;fraud_data_kaggle.csv&#39;) data.head() data.size . 69988820 . data[&#39;isFraud&#39;].value_counts() . 0 6354407 1 8213 Name: isFraud, dtype: int64 . Corregir datos desbalanceados . Usamos DOWNSAMPLING: consiste en usar todos los fraudulentos (8000) y solo 0.005 (31000) de los no-fraudulentos (clase minoritaria) . fraud = data[data[&#39;isFraud&#39;] == 1] not_fraud = data[data[&#39;isFraud&#39;] == 0] . # Take a random sample of non fraud rows not_fraud_sample = not_fraud.sample(random_state=2, frac=.005) # Put it back together and shuffle df = pd.concat([not_fraud_sample,fraud]) df = shuffle(df, random_state=2) # Remove a few columns (isFraud is the label column we&#39;ll use, not isFlaggedFraud) df = df.drop(columns=[&#39;nameOrig&#39;, &#39;nameDest&#39;, &#39;isFlaggedFraud&#39;]) # Preview the updated dataset df.head() . step type amount oldbalanceOrg newbalanceOrig oldbalanceDest newbalanceDest isFraud . 5777870 400 | PAYMENT | 65839.41 | 0.00 | 0.00 | 0.0 | 0.0 | 0 | . 6362412 726 | TRANSFER | 561446.32 | 561446.32 | 0.00 | 0.0 | 0.0 | 1 | . 5927827 404 | PAYMENT | 3828.08 | 10455.17 | 6627.09 | 0.0 | 0.0 | 0 | . 5987904 410 | TRANSFER | 557950.06 | 557950.06 | 0.00 | 0.0 | 0.0 | 1 | . 5706694 398 | PAYMENT | 1376.57 | 368349.14 | 366972.57 | 0.0 | 0.0 | 0 | . df[&#39;isFraud&#39;].value_counts() . 0 31772 1 8213 Name: isFraud, dtype: int64 . Dividir entre set de entrenamiento y prueba (test) . train_test_split = int(len(df) * .8) train_set = df[:train_test_split] test_set = df[train_test_split:] train_labels = train_set.pop(&#39;isFraud&#39;) test_labels = test_set.pop(&#39;isFraud&#39;) . Definir features . fc = tf.feature_column CATEGORICAL_COLUMNS = [&#39;type&#39;] NUMERIC_COLUMNS = [&#39;step&#39;, &#39;amount&#39;, &#39;oldbalanceOrg&#39;, &#39;newbalanceOrig&#39;, &#39;oldbalanceDest&#39;, &#39;newbalanceDest&#39;] . def one_hot_cat_column(feature_name, vocab): return tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocab)) feature_columns = [] for feature_name in CATEGORICAL_COLUMNS: vocabulary = train_set[feature_name].unique() feature_columns.append(one_hot_cat_column(feature_name, vocabulary)) for feature_name in NUMERIC_COLUMNS: feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.float32)) . Funciones . NUM_EXAMPLES = len(train_labels) def make_input_fn(X, y, n_epochs=None, shuffle=True): def input_fn(): dataset = tf.data.Dataset.from_tensor_slices((dict(X), y)) if shuffle: dataset = dataset.shuffle(NUM_EXAMPLES) dataset = dataset.repeat(n_epochs) dataset = dataset.batch(NUM_EXAMPLES) return dataset return input_fn # Define training and evaluation input functions train_input_fn = make_input_fn(train_set, train_labels) eval_input_fn = make_input_fn(test_set, test_labels, shuffle=False, n_epochs=1) . Entrenar modelo Boosted Tree . n_batches = 1 model = tf.estimator.BoostedTreesClassifier(feature_columns, n_batches_per_layer=n_batches) . INFO:tensorflow:Using default config. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp8ko2kwtc INFO:tensorflow:Using config: {&#39;_model_dir&#39;: &#39;/tmp/tmp8ko2kwtc&#39;, &#39;_tf_random_seed&#39;: None, &#39;_save_summary_steps&#39;: 100, &#39;_save_checkpoints_steps&#39;: None, &#39;_save_checkpoints_secs&#39;: 600, &#39;_session_config&#39;: allow_soft_placement: true graph_options { rewrite_options { meta_optimizer_iterations: ONE } } , &#39;_keep_checkpoint_max&#39;: 5, &#39;_keep_checkpoint_every_n_hours&#39;: 10000, &#39;_log_step_count_steps&#39;: 100, &#39;_train_distribute&#39;: None, &#39;_device_fn&#39;: None, &#39;_protocol&#39;: None, &#39;_eval_distribute&#39;: None, &#39;_experimental_distribute&#39;: None, &#39;_experimental_max_worker_delay_secs&#39;: None, &#39;_session_creation_timeout_secs&#39;: 7200, &#39;_service&#39;: None, &#39;_cluster_spec&#39;: &lt;tensorflow.python.training.server_lib.ClusterSpec object at 0x7fcb4fb77810&gt;, &#39;_task_type&#39;: &#39;worker&#39;, &#39;_task_id&#39;: 0, &#39;_global_id_in_cluster&#39;: 0, &#39;_master&#39;: &#39;&#39;, &#39;_evaluation_master&#39;: &#39;&#39;, &#39;_is_chief&#39;: True, &#39;_num_ps_replicas&#39;: 0, &#39;_num_worker_replicas&#39;: 1} WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/canned/boosted_trees.py:369: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version. Instructions for updating: The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead. . model.train(train_input_fn, max_steps=100) . WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating: If using Keras pass *_constraint arguments to layers. WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version. Instructions for updating: Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts. INFO:tensorflow:Calling model_fn. WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4271: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version. Instructions for updating: The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead. WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/canned/boosted_trees.py:214: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating: Use `tf.cast` instead. WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/canned/head.py:437: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating: Use `tf.cast` instead. WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where INFO:tensorflow:Done calling model_fn. INFO:tensorflow:Create CheckpointSaverHook. WARNING:tensorflow:Issue encountered when serializing resources. Type is unsupported, or the types of the items don&#39;t match field type in CollectionDef. Note this is a warning and probably safe to ignore. &#39;_Resource&#39; object has no attribute &#39;name&#39; INFO:tensorflow:Graph was finalized. INFO:tensorflow:Running local_init_op. INFO:tensorflow:Done running local_init_op. WARNING:tensorflow:Issue encountered when serializing resources. Type is unsupported, or the types of the items don&#39;t match field type in CollectionDef. Note this is a warning and probably safe to ignore. &#39;_Resource&#39; object has no attribute &#39;name&#39; INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmp8ko2kwtc/model.ckpt. WARNING:tensorflow:Issue encountered when serializing resources. Type is unsupported, or the types of the items don&#39;t match field type in CollectionDef. Note this is a warning and probably safe to ignore. &#39;_Resource&#39; object has no attribute &#39;name&#39; INFO:tensorflow:loss = 0.6931538, step = 0 WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize. INFO:tensorflow:global_step/sec: 1.56855 INFO:tensorflow:loss = 0.022666413, step = 99 (63.755 sec) INFO:tensorflow:Saving checkpoints for 100 into /tmp/tmp8ko2kwtc/model.ckpt. WARNING:tensorflow:Issue encountered when serializing resources. Type is unsupported, or the types of the items don&#39;t match field type in CollectionDef. Note this is a warning and probably safe to ignore. &#39;_Resource&#39; object has no attribute &#39;name&#39; INFO:tensorflow:Loss for final step: 0.022666413. . &lt;tensorflow_estimator.python.estimator.canned.boosted_trees.BoostedTreesClassifier at 0x7fcb4fb71dd0&gt; . result = model.evaluate(eval_input_fn) print(pd.Series(result)) . INFO:tensorflow:Calling model_fn. WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/metrics_impl.py:2026: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating: Deprecated in favor of operator or tf.math.divide. WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to &#34;careful_interpolation&#34; instead. WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to &#34;careful_interpolation&#34; instead. INFO:tensorflow:Done calling model_fn. INFO:tensorflow:Starting evaluation at 2020-06-22T08:11:13Z INFO:tensorflow:Graph was finalized. INFO:tensorflow:Restoring parameters from /tmp/tmp8ko2kwtc/model.ckpt-100 INFO:tensorflow:Running local_init_op. INFO:tensorflow:Done running local_init_op. INFO:tensorflow:Finished evaluation at 2020-06-22-08:11:15 INFO:tensorflow:Saving dict for global step 100: accuracy = 0.9949981, accuracy_baseline = 0.8006753, auc = 0.99874365, auc_precision_recall = 0.99814403, average_loss = 0.024392342, global_step = 100, label/mean = 0.19932474, loss = 0.024392342, precision = 0.97903824, prediction/mean = 0.2005636, recall = 0.9962359 WARNING:tensorflow:Issue encountered when serializing resources. Type is unsupported, or the types of the items don&#39;t match field type in CollectionDef. Note this is a warning and probably safe to ignore. &#39;_Resource&#39; object has no attribute &#39;name&#39; INFO:tensorflow:Saving &#39;checkpoint_path&#39; summary for global step 100: /tmp/tmp8ko2kwtc/model.ckpt-100 accuracy 0.994998 accuracy_baseline 0.800675 auc 0.998744 auc_precision_recall 0.998144 average_loss 0.024392 label/mean 0.199325 loss 0.024392 precision 0.979038 prediction/mean 0.200564 recall 0.996236 global_step 100.000000 dtype: float64 . pred_dicts = list(model.predict(eval_input_fn)) probabilities = pd.Series([pred[&#39;logistic&#39;][0] for pred in pred_dicts]) for i,val in enumerate(probabilities[:30]): print(&#39;Predicted: &#39;, round(val), &#39;Actual: &#39;, test_labels.iloc[i]) print() . INFO:tensorflow:Calling model_fn. INFO:tensorflow:Done calling model_fn. INFO:tensorflow:Graph was finalized. INFO:tensorflow:Restoring parameters from /tmp/tmp8ko2kwtc/model.ckpt-100 INFO:tensorflow:Running local_init_op. INFO:tensorflow:Done running local_init_op. Predicted: 0 Actual: 0 Predicted: 1 Actual: 1 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 1 Actual: 1 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 1 Actual: 1 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 . Confusion matrix . y_pred = [] for i in probabilities.values: y_pred.append(int(round(i))) . cm = confusion_matrix(test_labels.values, y_pred) print(cm) . [[6369 34] [ 6 1588]] . def plot_confusion_matrix(cm, classes, normalize=False, title=&#39;Confusion matrix&#39;, cmap=plt.cm.Blues): &quot;&quot;&quot; This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. &quot;&quot;&quot; plt.imshow(cm, interpolation=&#39;nearest&#39;, cmap=cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes, rotation=45) plt.yticks(tick_marks, classes) if normalize: cm = np.round(cm.astype(&#39;float&#39;) / cm.sum(axis=1)[:, np.newaxis], 3) thresh = cm.max() / 2. for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): plt.text(j, i, cm[i, j], horizontalalignment=&quot;center&quot;, color=&quot;white&quot; if cm[i, j] &gt; thresh else &quot;black&quot;) plt.tight_layout() plt.ylabel(&#39;True label&#39;) plt.xlabel(&#39;Predicted label&#39;) . classes = [&#39;not fraud&#39;, &#39;fraud&#39;] plot_confusion_matrix(cm, classes, normalize=True) . Preparando el despliegue del modelo en AI Platform (con Explainability) . GCP_PROJECT = &#39;windy-site-254307&#39; MODEL_BUCKET = &#39;gs://fraud-detection-explainable-ai&#39; !gsutil mb $MODEL_BUCKET . Creating gs://fraud-detection-explainable-ai/... ServiceException: 409 Bucket fraud-detection-explainable-ai already exists. . Exportamos el modelo en TensorFlow 1.x . import tensorflow.compat.v1 as tf tf.disable_v2_behavior() def json_serving_input_fn(): inputs = {} for feat in feature_columns: if feat.name == &quot;type_indicator&quot;: inputs[&#39;type&#39;] = tf.placeholder(shape=[None], name=feat.name, dtype=tf.string) else: inputs[feat.name] = tf.placeholder(shape=[None], name=feat.name, dtype=feat.dtype) return tf.estimator.export.ServingInputReceiver(inputs, inputs) export_path = model.export_saved_model( MODEL_BUCKET + &#39;/explanations&#39;, serving_input_receiver_fn=json_serving_input_fn ).decode(&#39;utf-8&#39;) tf.enable_v2_behavior() . !saved_model_cli show --dir $export_path --all . MetaGraphDef with tag-set: &#39;serve&#39; contains the following SignatureDefs: signature_def[&#39;predict&#39;]: The given SavedModel SignatureDef contains the following input(s): inputs[&#39;amount&#39;] tensor_info: dtype: DT_FLOAT shape: (-1) name: amount:0 inputs[&#39;newbalanceDest&#39;] tensor_info: dtype: DT_FLOAT shape: (-1) name: newbalanceDest:0 inputs[&#39;newbalanceOrig&#39;] tensor_info: dtype: DT_FLOAT shape: (-1) name: newbalanceOrig:0 inputs[&#39;oldbalanceDest&#39;] tensor_info: dtype: DT_FLOAT shape: (-1) name: oldbalanceDest:0 inputs[&#39;oldbalanceOrg&#39;] tensor_info: dtype: DT_FLOAT shape: (-1) name: oldbalanceOrg:0 inputs[&#39;step&#39;] tensor_info: dtype: DT_FLOAT shape: (-1) name: step:0 inputs[&#39;type&#39;] tensor_info: dtype: DT_STRING shape: (-1) name: type_indicator:0 The given SavedModel SignatureDef contains the following output(s): outputs[&#39;all_class_ids&#39;] tensor_info: dtype: DT_INT32 shape: (-1, 2) name: boosted_trees/head/predictions/Tile:0 outputs[&#39;all_classes&#39;] tensor_info: dtype: DT_STRING shape: (-1, 2) name: boosted_trees/head/predictions/Tile_1:0 outputs[&#39;class_ids&#39;] tensor_info: dtype: DT_INT64 shape: (-1, 1) name: boosted_trees/head/predictions/ExpandDims:0 outputs[&#39;classes&#39;] tensor_info: dtype: DT_STRING shape: (-1, 1) name: boosted_trees/head/predictions/str_classes:0 outputs[&#39;logistic&#39;] tensor_info: dtype: DT_FLOAT shape: (-1, 1) name: boosted_trees/head/predictions/logistic:0 outputs[&#39;logits&#39;] tensor_info: dtype: DT_FLOAT shape: (-1, 1) name: boosted_trees/BoostedTreesPredict:0 outputs[&#39;probabilities&#39;] tensor_info: dtype: DT_FLOAT shape: (-1, 2) name: boosted_trees/head/predictions/probabilities:0 Method name is: tensorflow/serving/predict . not_fraud.median() . step 239.00 amount 74684.72 oldbalanceOrg 14069.00 newbalanceOrig 0.00 oldbalanceDest 133311.80 newbalanceDest 214881.70 isFraud 0.00 isFlaggedFraud 0.00 dtype: float64 . not_fraud[&#39;type&#39;].value_counts() . CASH_OUT 2233384 PAYMENT 2151495 CASH_IN 1399284 TRANSFER 528812 DEBIT 41432 Name: type, dtype: int64 . !gsutil cp explanation_metadata.json $export_path . Copying file://explanation_metadata.json [Content-Type=application/json]... / [1 files][ 718.0 B/ 718.0 B] Operation completed over 1 objects/718.0 B. . !gsutil cp gs://fraud-detection-explainable-ai/explanations/1592218671/explanation_metadata.json . !gsutil cp explanation_metadata.json $export_path . Despliegue a AI Platform explanations . MODEL = &#39;fraud_detection_4&#39; VERSION = &#39;v3&#39; . !gcloud ai-platform models create $MODEL . WARNING: Using endpoint [https://ml.googleapis.com/] WARNING: Please explicitly specify a region. Using [us-central1] by default on https://ml.googleapis.com. Please note that your model will be inaccessible from https://us-central1-ml.googelapis.com Learn more about regional endpoints and see a list of available regions: https://cloud.google.com/ai-platform/prediction/docs/regional-endpoints ERROR: (gcloud.ai-platform.models.create) Resource in project [windy-site-254307] is the subject of a conflict: Field: model.name Error: A model with the same name already exists. - &#39;@type&#39;: type.googleapis.com/google.rpc.BadRequest fieldViolations: - description: A model with the same name already exists. field: model.name . !gcloud beta ai-platform versions create v3 --model $MODEL --origin $export_path --runtime-version 1.15 --framework TENSORFLOW --python-version 3.7 --machine-type n1-standard-4 --explanation-method &#39;sampled-shapley&#39; --num-paths 10 . WARNING: Using endpoint [https://ml.googleapis.com/] Explanations reflect patterns in your model, but don&#39;t necessarily reveal fundamental relationships about your data population. See https://cloud.google.com/ml-engine/docs/ai-explanations/limitations for more information. ERROR: (gcloud.beta.ai-platform.versions.create) ALREADY_EXISTS: Field: version.name Error: A version with the same name already exists. - &#39;@type&#39;: type.googleapis.com/google.rpc.BadRequest fieldViolations: - description: A version with the same name already exists. field: version.name . !gcloud ai-platform versions describe $VERSION --model $MODEL . WARNING: Using endpoint [https://ml.googleapis.com/] createTime: &#39;2020-06-15T14:20:08Z&#39; deploymentUri: gs://fraud-detection-explainable-ai/explanations/1592230714 etag: yuDcVfyBSAQ= explanationConfig: sampledShapleyAttribution: numPaths: 10 framework: TENSORFLOW isDefault: true lastUseTime: &#39;2020-06-18T07:20:48Z&#39; machineType: n1-standard-4 name: projects/windy-site-254307/models/fraud_detection_4/versions/v3 pythonVersion: &#39;3.7&#39; runtimeVersion: &#39;1.15&#39; state: READY . Predeici&#243;n con explicabilidad . Step 1: preparamos datos (data.txt) . fraud_indices = [] for i,val in enumerate(test_labels): if val == 1: fraud_indices.append(i) . num_test_examples = 5 import numpy as np def convert(o): if isinstance(o, np.generic): return o.item() raise TypeError for i in range(num_test_examples): test_json = {} ex = test_set.iloc[fraud_indices[i]] keys = ex.keys().tolist() vals = ex.values.tolist() for idx in range(len(keys)): test_json[keys[idx]] = vals[idx] print(test_json) with open(&#39;data.txt&#39;, &#39;a&#39;) as outfile: json.dump(test_json, outfile, default=convert) outfile.write(&#39; n&#39;) . {&#39;step&#39;: 476, &#39;type&#39;: &#39;TRANSFER&#39;, &#39;amount&#39;: 1048.63, &#39;oldbalanceOrg&#39;: 1048.63, &#39;newbalanceOrig&#39;: 0.0, &#39;oldbalanceDest&#39;: 0.0, &#39;newbalanceDest&#39;: 0.0} {&#39;step&#39;: 390, &#39;type&#39;: &#39;TRANSFER&#39;, &#39;amount&#39;: 638693.49, &#39;oldbalanceOrg&#39;: 638693.49, &#39;newbalanceOrig&#39;: 0.0, &#39;oldbalanceDest&#39;: 0.0, &#39;newbalanceDest&#39;: 0.0} {&#39;step&#39;: 355, &#39;type&#39;: &#39;CASH_OUT&#39;, &#39;amount&#39;: 5338162.8, &#39;oldbalanceOrg&#39;: 5338162.8, &#39;newbalanceOrig&#39;: 0.0, &#39;oldbalanceDest&#39;: 181895.58, &#39;newbalanceDest&#39;: 5520058.37} {&#39;step&#39;: 356, &#39;type&#39;: &#39;TRANSFER&#39;, &#39;amount&#39;: 357226.8, &#39;oldbalanceOrg&#39;: 357226.8, &#39;newbalanceOrig&#39;: 0.0, &#39;oldbalanceDest&#39;: 0.0, &#39;newbalanceDest&#39;: 0.0} {&#39;step&#39;: 345, &#39;type&#39;: &#39;TRANSFER&#39;, &#39;amount&#39;: 128936.95, &#39;oldbalanceOrg&#39;: 128936.95, &#39;newbalanceOrig&#39;: 0.0, &#39;oldbalanceDest&#39;: 0.0, &#39;newbalanceDest&#39;: 0.0} . !cat data.txt . {&#34;step&#34;: 476, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 1048.63, &#34;oldbalanceOrg&#34;: 1048.63, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 390, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 638693.49, &#34;oldbalanceOrg&#34;: 638693.49, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 355, &#34;type&#34;: &#34;CASH_OUT&#34;, &#34;amount&#34;: 5338162.8, &#34;oldbalanceOrg&#34;: 5338162.8, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 181895.58, &#34;newbalanceDest&#34;: 5520058.37} {&#34;step&#34;: 356, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 357226.8, &#34;oldbalanceOrg&#34;: 357226.8, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 345, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 128936.95, &#34;oldbalanceOrg&#34;: 128936.95, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 476, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 1048.63, &#34;oldbalanceOrg&#34;: 1048.63, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 390, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 638693.49, &#34;oldbalanceOrg&#34;: 638693.49, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 355, &#34;type&#34;: &#34;CASH_OUT&#34;, &#34;amount&#34;: 5338162.8, &#34;oldbalanceOrg&#34;: 5338162.8, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 181895.58, &#34;newbalanceDest&#34;: 5520058.37} {&#34;step&#34;: 356, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 357226.8, &#34;oldbalanceOrg&#34;: 357226.8, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 345, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 128936.95, &#34;oldbalanceOrg&#34;: 128936.95, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 476, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 1048.63, &#34;oldbalanceOrg&#34;: 1048.63, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 390, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 638693.49, &#34;oldbalanceOrg&#34;: 638693.49, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 355, &#34;type&#34;: &#34;CASH_OUT&#34;, &#34;amount&#34;: 5338162.8, &#34;oldbalanceOrg&#34;: 5338162.8, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 181895.58, &#34;newbalanceDest&#34;: 5520058.37} {&#34;step&#34;: 356, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 357226.8, &#34;oldbalanceOrg&#34;: 357226.8, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 345, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 128936.95, &#34;oldbalanceOrg&#34;: 128936.95, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 476, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 1048.63, &#34;oldbalanceOrg&#34;: 1048.63, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 390, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 638693.49, &#34;oldbalanceOrg&#34;: 638693.49, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 355, &#34;type&#34;: &#34;CASH_OUT&#34;, &#34;amount&#34;: 5338162.8, &#34;oldbalanceOrg&#34;: 5338162.8, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 181895.58, &#34;newbalanceDest&#34;: 5520058.37} {&#34;step&#34;: 356, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 357226.8, &#34;oldbalanceOrg&#34;: 357226.8, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 345, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 128936.95, &#34;oldbalanceOrg&#34;: 128936.95, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 476, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 1048.63, &#34;oldbalanceOrg&#34;: 1048.63, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 390, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 638693.49, &#34;oldbalanceOrg&#34;: 638693.49, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 355, &#34;type&#34;: &#34;CASH_OUT&#34;, &#34;amount&#34;: 5338162.8, &#34;oldbalanceOrg&#34;: 5338162.8, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 181895.58, &#34;newbalanceDest&#34;: 5520058.37} {&#34;step&#34;: 356, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 357226.8, &#34;oldbalanceOrg&#34;: 357226.8, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 345, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 128936.95, &#34;oldbalanceOrg&#34;: 128936.95, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 476, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 1048.63, &#34;oldbalanceOrg&#34;: 1048.63, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 390, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 638693.49, &#34;oldbalanceOrg&#34;: 638693.49, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 355, &#34;type&#34;: &#34;CASH_OUT&#34;, &#34;amount&#34;: 5338162.8, &#34;oldbalanceOrg&#34;: 5338162.8, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 181895.58, &#34;newbalanceDest&#34;: 5520058.37} {&#34;step&#34;: 356, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 357226.8, &#34;oldbalanceOrg&#34;: 357226.8, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 345, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 128936.95, &#34;oldbalanceOrg&#34;: 128936.95, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 476, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 1048.63, &#34;oldbalanceOrg&#34;: 1048.63, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 390, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 638693.49, &#34;oldbalanceOrg&#34;: 638693.49, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 355, &#34;type&#34;: &#34;CASH_OUT&#34;, &#34;amount&#34;: 5338162.8, &#34;oldbalanceOrg&#34;: 5338162.8, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 181895.58, &#34;newbalanceDest&#34;: 5520058.37} {&#34;step&#34;: 356, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 357226.8, &#34;oldbalanceOrg&#34;: 357226.8, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 345, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 128936.95, &#34;oldbalanceOrg&#34;: 128936.95, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 476, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 1048.63, &#34;oldbalanceOrg&#34;: 1048.63, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 390, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 638693.49, &#34;oldbalanceOrg&#34;: 638693.49, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 355, &#34;type&#34;: &#34;CASH_OUT&#34;, &#34;amount&#34;: 5338162.8, &#34;oldbalanceOrg&#34;: 5338162.8, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 181895.58, &#34;newbalanceDest&#34;: 5520058.37} {&#34;step&#34;: 356, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 357226.8, &#34;oldbalanceOrg&#34;: 357226.8, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 345, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 128936.95, &#34;oldbalanceOrg&#34;: 128936.95, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 476, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 1048.63, &#34;oldbalanceOrg&#34;: 1048.63, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 390, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 638693.49, &#34;oldbalanceOrg&#34;: 638693.49, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 355, &#34;type&#34;: &#34;CASH_OUT&#34;, &#34;amount&#34;: 5338162.8, &#34;oldbalanceOrg&#34;: 5338162.8, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 181895.58, &#34;newbalanceDest&#34;: 5520058.37} {&#34;step&#34;: 356, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 357226.8, &#34;oldbalanceOrg&#34;: 357226.8, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 345, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 128936.95, &#34;oldbalanceOrg&#34;: 128936.95, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} . Step 2: enviamos data.txt al modelo con gcloud beta ai-platform explain . explanations = !gcloud beta ai-platform explain --model $MODEL --version $VERSION --json-instances=&#39;data.txt&#39; --verbosity error explanations.s . &#39;{ &#34;explanations&#34;: [ { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.07222782395555333, &#34;attributions&#34;: { &#34;amount&#34;: 0.6966777056455612, &#34;newbalanceDest&#34;: 0.1639272928237915, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.16475289762020112, &#34;oldbalanceOrg&#34;: -0.15600235760211945, &#34;step&#34;: 0.1192602276802063, &#34;type&#34;: -0.027230754494667053 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9744548797607422, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.004716743469869621, &#34;attributions&#34;: { &#34;amount&#34;: -0.0019459187984466552, &#34;newbalanceDest&#34;: 0.007967007160186768, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.008247834444046021, &#34;oldbalanceOrg&#34;: 0.9784113168716431, &#34;step&#34;: 0.0, &#34;type&#34;: -0.007150697708129883 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9985994100570679, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.0024439054423102693, &#34;attributions&#34;: { &#34;amount&#34;: -0.002712637186050415, &#34;newbalanceDest&#34;: -0.0005802184343338013, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;oldbalanceOrg&#34;: 0.9835709899663925, &#34;step&#34;: 0.0, &#34;type&#34;: 0.0 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9933480024337769, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.014761328724187625, &#34;attributions&#34;: { &#34;amount&#34;: -0.02181842029094696, &#34;newbalanceDest&#34;: 0.029705092310905457, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.038540315628051755, &#34;oldbalanceOrg&#34;: 0.9493075281381607, &#34;step&#34;: 0.0, &#34;type&#34;: -0.011098328232765197 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9977060556411743, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.036956189992043476, &#34;attributions&#34;: { &#34;amount&#34;: -0.05543297529220581, &#34;newbalanceDest&#34;: 0.09204374849796296, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.09319761395454407, &#34;oldbalanceOrg&#34;: 0.8222974985837936, &#34;step&#34;: 0.0, &#34;type&#34;: 0.028654450178146364 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9938302040100098, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.07222782395555333, &#34;attributions&#34;: { &#34;amount&#34;: 0.6966777056455612, &#34;newbalanceDest&#34;: 0.1639272928237915, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.16475289762020112, &#34;oldbalanceOrg&#34;: -0.15600235760211945, &#34;step&#34;: 0.1192602276802063, &#34;type&#34;: -0.027230754494667053 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9744548797607422, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.004716743469869621, &#34;attributions&#34;: { &#34;amount&#34;: -0.0019459187984466552, &#34;newbalanceDest&#34;: 0.007967007160186768, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.008247834444046021, &#34;oldbalanceOrg&#34;: 0.9784113168716431, &#34;step&#34;: 0.0, &#34;type&#34;: -0.007150697708129883 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9985994100570679, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.0024439054423102693, &#34;attributions&#34;: { &#34;amount&#34;: -0.002712637186050415, &#34;newbalanceDest&#34;: -0.0005802184343338013, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;oldbalanceOrg&#34;: 0.9835709899663925, &#34;step&#34;: 0.0, &#34;type&#34;: 0.0 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9933480024337769, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.014761328724187625, &#34;attributions&#34;: { &#34;amount&#34;: -0.02181842029094696, &#34;newbalanceDest&#34;: 0.029705092310905457, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.038540315628051755, &#34;oldbalanceOrg&#34;: 0.9493075281381607, &#34;step&#34;: 0.0, &#34;type&#34;: -0.011098328232765197 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9977060556411743, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.036956189992043476, &#34;attributions&#34;: { &#34;amount&#34;: -0.05543297529220581, &#34;newbalanceDest&#34;: 0.09204374849796296, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.09319761395454407, &#34;oldbalanceOrg&#34;: 0.8222974985837936, &#34;step&#34;: 0.0, &#34;type&#34;: 0.028654450178146364 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9938302040100098, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.07222782395555333, &#34;attributions&#34;: { &#34;amount&#34;: 0.6966777056455612, &#34;newbalanceDest&#34;: 0.1639272928237915, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.16475289762020112, &#34;oldbalanceOrg&#34;: -0.15600235760211945, &#34;step&#34;: 0.1192602276802063, &#34;type&#34;: -0.027230754494667053 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9744548797607422, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.004716743469869621, &#34;attributions&#34;: { &#34;amount&#34;: -0.0019459187984466552, &#34;newbalanceDest&#34;: 0.007967007160186768, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.008247834444046021, &#34;oldbalanceOrg&#34;: 0.9784113168716431, &#34;step&#34;: 0.0, &#34;type&#34;: -0.007150697708129883 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9985994100570679, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.0024439054423102693, &#34;attributions&#34;: { &#34;amount&#34;: -0.002712637186050415, &#34;newbalanceDest&#34;: -0.0005802184343338013, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;oldbalanceOrg&#34;: 0.9835709899663925, &#34;step&#34;: 0.0, &#34;type&#34;: 0.0 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9933480024337769, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.014761328724187625, &#34;attributions&#34;: { &#34;amount&#34;: -0.02181842029094696, &#34;newbalanceDest&#34;: 0.029705092310905457, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.038540315628051755, &#34;oldbalanceOrg&#34;: 0.9493075281381607, &#34;step&#34;: 0.0, &#34;type&#34;: -0.011098328232765197 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9977060556411743, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.036956189992043476, &#34;attributions&#34;: { &#34;amount&#34;: -0.05543297529220581, &#34;newbalanceDest&#34;: 0.09204374849796296, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.09319761395454407, &#34;oldbalanceOrg&#34;: 0.8222974985837936, &#34;step&#34;: 0.0, &#34;type&#34;: 0.028654450178146364 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9938302040100098, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.07222782395555333, &#34;attributions&#34;: { &#34;amount&#34;: 0.6966777056455612, &#34;newbalanceDest&#34;: 0.1639272928237915, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.16475289762020112, &#34;oldbalanceOrg&#34;: -0.15600235760211945, &#34;step&#34;: 0.1192602276802063, &#34;type&#34;: -0.027230754494667053 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9744548797607422, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.004716743469869621, &#34;attributions&#34;: { &#34;amount&#34;: -0.0019459187984466552, &#34;newbalanceDest&#34;: 0.007967007160186768, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.008247834444046021, &#34;oldbalanceOrg&#34;: 0.9784113168716431, &#34;step&#34;: 0.0, &#34;type&#34;: -0.007150697708129883 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9985994100570679, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.0024439054423102693, &#34;attributions&#34;: { &#34;amount&#34;: -0.002712637186050415, &#34;newbalanceDest&#34;: -0.0005802184343338013, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;oldbalanceOrg&#34;: 0.9835709899663925, &#34;step&#34;: 0.0, &#34;type&#34;: 0.0 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9933480024337769, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.014761328724187625, &#34;attributions&#34;: { &#34;amount&#34;: -0.02181842029094696, &#34;newbalanceDest&#34;: 0.029705092310905457, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.038540315628051755, &#34;oldbalanceOrg&#34;: 0.9493075281381607, &#34;step&#34;: 0.0, &#34;type&#34;: -0.011098328232765197 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9977060556411743, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.036956189992043476, &#34;attributions&#34;: { &#34;amount&#34;: -0.05543297529220581, &#34;newbalanceDest&#34;: 0.09204374849796296, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.09319761395454407, &#34;oldbalanceOrg&#34;: 0.8222974985837936, &#34;step&#34;: 0.0, &#34;type&#34;: 0.028654450178146364 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9938302040100098, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.07222782395555333, &#34;attributions&#34;: { &#34;amount&#34;: 0.6966777056455612, &#34;newbalanceDest&#34;: 0.1639272928237915, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.16475289762020112, &#34;oldbalanceOrg&#34;: -0.15600235760211945, &#34;step&#34;: 0.1192602276802063, &#34;type&#34;: -0.027230754494667053 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9744548797607422, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.004716743469869621, &#34;attributions&#34;: { &#34;amount&#34;: -0.0019459187984466552, &#34;newbalanceDest&#34;: 0.007967007160186768, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.008247834444046021, &#34;oldbalanceOrg&#34;: 0.9784113168716431, &#34;step&#34;: 0.0, &#34;type&#34;: -0.007150697708129883 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9985994100570679, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.0024439054423102693, &#34;attributions&#34;: { &#34;amount&#34;: -0.002712637186050415, &#34;newbalanceDest&#34;: -0.0005802184343338013, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;oldbalanceOrg&#34;: 0.9835709899663925, &#34;step&#34;: 0.0, &#34;type&#34;: 0.0 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9933480024337769, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.014761328724187625, &#34;attributions&#34;: { &#34;amount&#34;: -0.02181842029094696, &#34;newbalanceDest&#34;: 0.029705092310905457, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.038540315628051755, &#34;oldbalanceOrg&#34;: 0.9493075281381607, &#34;step&#34;: 0.0, &#34;type&#34;: -0.011098328232765197 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9977060556411743, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.036956189992043476, &#34;attributions&#34;: { &#34;amount&#34;: -0.05543297529220581, &#34;newbalanceDest&#34;: 0.09204374849796296, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.09319761395454407, &#34;oldbalanceOrg&#34;: 0.8222974985837936, &#34;step&#34;: 0.0, &#34;type&#34;: 0.028654450178146364 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9938302040100098, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.07222782395555333, &#34;attributions&#34;: { &#34;amount&#34;: 0.6966777056455612, &#34;newbalanceDest&#34;: 0.1639272928237915, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.16475289762020112, &#34;oldbalanceOrg&#34;: -0.15600235760211945, &#34;step&#34;: 0.1192602276802063, &#34;type&#34;: -0.027230754494667053 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9744548797607422, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.004716743469869621, &#34;attributions&#34;: { &#34;amount&#34;: -0.0019459187984466552, &#34;newbalanceDest&#34;: 0.007967007160186768, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.008247834444046021, &#34;oldbalanceOrg&#34;: 0.9784113168716431, &#34;step&#34;: 0.0, &#34;type&#34;: -0.007150697708129883 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9985994100570679, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.0024439054423102693, &#34;attributions&#34;: { &#34;amount&#34;: -0.002712637186050415, &#34;newbalanceDest&#34;: -0.0005802184343338013, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;oldbalanceOrg&#34;: 0.9835709899663925, &#34;step&#34;: 0.0, &#34;type&#34;: 0.0 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9933480024337769, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.014761328724187625, &#34;attributions&#34;: { &#34;amount&#34;: -0.02181842029094696, &#34;newbalanceDest&#34;: 0.029705092310905457, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.038540315628051755, &#34;oldbalanceOrg&#34;: 0.9493075281381607, &#34;step&#34;: 0.0, &#34;type&#34;: -0.011098328232765197 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9977060556411743, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.036956189992043476, &#34;attributions&#34;: { &#34;amount&#34;: -0.05543297529220581, &#34;newbalanceDest&#34;: 0.09204374849796296, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.09319761395454407, &#34;oldbalanceOrg&#34;: 0.8222974985837936, &#34;step&#34;: 0.0, &#34;type&#34;: 0.028654450178146364 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9938302040100098, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.07222782395555333, &#34;attributions&#34;: { &#34;amount&#34;: 0.6966777056455612, &#34;newbalanceDest&#34;: 0.1639272928237915, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.16475289762020112, &#34;oldbalanceOrg&#34;: -0.15600235760211945, &#34;step&#34;: 0.1192602276802063, &#34;type&#34;: -0.027230754494667053 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9744548797607422, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.004716743469869621, &#34;attributions&#34;: { &#34;amount&#34;: -0.0019459187984466552, &#34;newbalanceDest&#34;: 0.007967007160186768, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.008247834444046021, &#34;oldbalanceOrg&#34;: 0.9784113168716431, &#34;step&#34;: 0.0, &#34;type&#34;: -0.007150697708129883 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9985994100570679, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.0024439054423102693, &#34;attributions&#34;: { &#34;amount&#34;: -0.002712637186050415, &#34;newbalanceDest&#34;: -0.0005802184343338013, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;oldbalanceOrg&#34;: 0.9835709899663925, &#34;step&#34;: 0.0, &#34;type&#34;: 0.0 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9933480024337769, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.014761328724187625, &#34;attributions&#34;: { &#34;amount&#34;: -0.02181842029094696, &#34;newbalanceDest&#34;: 0.029705092310905457, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.038540315628051755, &#34;oldbalanceOrg&#34;: 0.9493075281381607, &#34;step&#34;: 0.0, &#34;type&#34;: -0.011098328232765197 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9977060556411743, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.036956189992043476, &#34;attributions&#34;: { &#34;amount&#34;: -0.05543297529220581, &#34;newbalanceDest&#34;: 0.09204374849796296, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.09319761395454407, &#34;oldbalanceOrg&#34;: 0.8222974985837936, &#34;step&#34;: 0.0, &#34;type&#34;: 0.028654450178146364 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9938302040100098, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.07222782395555333, &#34;attributions&#34;: { &#34;amount&#34;: 0.6966777056455612, &#34;newbalanceDest&#34;: 0.1639272928237915, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.16475289762020112, &#34;oldbalanceOrg&#34;: -0.15600235760211945, &#34;step&#34;: 0.1192602276802063, &#34;type&#34;: -0.027230754494667053 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9744548797607422, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.004716743469869621, &#34;attributions&#34;: { &#34;amount&#34;: -0.0019459187984466552, &#34;newbalanceDest&#34;: 0.007967007160186768, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.008247834444046021, &#34;oldbalanceOrg&#34;: 0.9784113168716431, &#34;step&#34;: 0.0, &#34;type&#34;: -0.007150697708129883 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9985994100570679, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.0024439054423102693, &#34;attributions&#34;: { &#34;amount&#34;: -0.002712637186050415, &#34;newbalanceDest&#34;: -0.0005802184343338013, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;oldbalanceOrg&#34;: 0.9835709899663925, &#34;step&#34;: 0.0, &#34;type&#34;: 0.0 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9933480024337769, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.014761328724187625, &#34;attributions&#34;: { &#34;amount&#34;: -0.02181842029094696, &#34;newbalanceDest&#34;: 0.029705092310905457, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.038540315628051755, &#34;oldbalanceOrg&#34;: 0.9493075281381607, &#34;step&#34;: 0.0, &#34;type&#34;: -0.011098328232765197 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9977060556411743, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.036956189992043476, &#34;attributions&#34;: { &#34;amount&#34;: -0.05543297529220581, &#34;newbalanceDest&#34;: 0.09204374849796296, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.09319761395454407, &#34;oldbalanceOrg&#34;: 0.8222974985837936, &#34;step&#34;: 0.0, &#34;type&#34;: 0.028654450178146364 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9938302040100098, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.07222782395555333, &#34;attributions&#34;: { &#34;amount&#34;: 0.6966777056455612, &#34;newbalanceDest&#34;: 0.1639272928237915, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.16475289762020112, &#34;oldbalanceOrg&#34;: -0.15600235760211945, &#34;step&#34;: 0.1192602276802063, &#34;type&#34;: -0.027230754494667053 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9744548797607422, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.004716743469869621, &#34;attributions&#34;: { &#34;amount&#34;: -0.0019459187984466552, &#34;newbalanceDest&#34;: 0.007967007160186768, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.008247834444046021, &#34;oldbalanceOrg&#34;: 0.9784113168716431, &#34;step&#34;: 0.0, &#34;type&#34;: -0.007150697708129883 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9985994100570679, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.0024439054423102693, &#34;attributions&#34;: { &#34;amount&#34;: -0.002712637186050415, &#34;newbalanceDest&#34;: -0.0005802184343338013, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;oldbalanceOrg&#34;: 0.9835709899663925, &#34;step&#34;: 0.0, &#34;type&#34;: 0.0 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9933480024337769, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.014761328724187625, &#34;attributions&#34;: { &#34;amount&#34;: -0.02181842029094696, &#34;newbalanceDest&#34;: 0.029705092310905457, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.038540315628051755, &#34;oldbalanceOrg&#34;: 0.9493075281381607, &#34;step&#34;: 0.0, &#34;type&#34;: -0.011098328232765197 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9977060556411743, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.036956189992043476, &#34;attributions&#34;: { &#34;amount&#34;: -0.05543297529220581, &#34;newbalanceDest&#34;: 0.09204374849796296, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.09319761395454407, &#34;oldbalanceOrg&#34;: 0.8222974985837936, &#34;step&#34;: 0.0, &#34;type&#34;: 0.028654450178146364 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9938302040100098, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] } ] }&#39; . explain_dict = json.loads(explanations.s) . ### Step 3: análisis de datos (notad el baseline) . print(&#39;Model baseline for fraud cases: &#39;, explain_dict[&#39;explanations&#39;][0][&#39;attributions_by_label&#39;][0][&#39;baseline_score&#39;], &#39; n&#39;) . Model baseline for fraud cases: 0.013069868087768555 . for i in explain_dict[&#39;explanations&#39;]: prediction_score = i[&#39;attributions_by_label&#39;][0][&#39;example_score&#39;] attributions = i[&#39;attributions_by_label&#39;][0][&#39;attributions&#39;] print(&#39;Model prediction:&#39;, prediction_score) fig, ax = plt.subplots() ax.barh(list(attributions.keys()), list(attributions.values()), align=&#39;center&#39;) plt.show() . Model prediction: 0.9744548797607422 . Model prediction: 0.9985994100570679 . Model prediction: 0.9933480024337769 . Model prediction: 0.9977060556411743 . Model prediction: 0.9938302040100098 . Model prediction: 0.9744548797607422 . Model prediction: 0.9985994100570679 . Model prediction: 0.9933480024337769 . Model prediction: 0.9977060556411743 . Model prediction: 0.9938302040100098 . Model prediction: 0.9744548797607422 . Model prediction: 0.9985994100570679 . Model prediction: 0.9933480024337769 . Model prediction: 0.9977060556411743 . Model prediction: 0.9938302040100098 . Model prediction: 0.9744548797607422 . Model prediction: 0.9985994100570679 . Model prediction: 0.9933480024337769 . Model prediction: 0.9977060556411743 . Model prediction: 0.9938302040100098 . Model prediction: 0.9744548797607422 . Model prediction: 0.9985994100570679 . Model prediction: 0.9933480024337769 . Model prediction: 0.9977060556411743 . Model prediction: 0.9938302040100098 . Model prediction: 0.9744548797607422 . Model prediction: 0.9985994100570679 . Model prediction: 0.9933480024337769 . Model prediction: 0.9977060556411743 . Model prediction: 0.9938302040100098 . Model prediction: 0.9744548797607422 . Model prediction: 0.9985994100570679 . Model prediction: 0.9933480024337769 . Model prediction: 0.9977060556411743 . Model prediction: 0.9938302040100098 . Model prediction: 0.9744548797607422 . Model prediction: 0.9985994100570679 . Model prediction: 0.9933480024337769 . Model prediction: 0.9977060556411743 . Model prediction: 0.9938302040100098 . Model prediction: 0.9744548797607422 . Model prediction: 0.9985994100570679 . Model prediction: 0.9933480024337769 . Model prediction: 0.9977060556411743 . Model prediction: 0.9938302040100098 .",
            "url": "https://rafaelsf80.github.io/notebooks/xai/2020/06/20/xai-fraud-detection.html",
            "relUrl": "/xai/2020/06/20/xai-fraud-detection.html",
            "date": " • Jun 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Intro to Dense Neural Networks in Spanish",
            "content": "Importar TensorFlow . Instalamos TensorFlow. Se puede instalar con pipen Colab, pero el magic command es más rápido. También accesible en este enlace. . %tensorflow_version 2.x import tensorflow as tf print(&quot;You are using TensorFlow version&quot;, tf.__version__) if len(tf.config.list_physical_devices(&#39;GPU&#39;)) &gt; 0: print(&quot;You have a GPU enabled.&quot;) else: print(&quot;Enable a GPU before running this notebook.&quot;) . Colab tiene varias GPUS disponibles (se asigna una aleatoria, dependiendo de la disponibilidad). Para ver tipos de GPUs, se debe ejecutar !nvidia-smi en una celda. . # In this notebook, we&#39;ll use Keras: TensorFlow&#39;s user-friendly API to # define neural networks. Let&#39;s import Keras now. from tensorflow import keras import matplotlib.pyplot as plt . Descargar el dataset de MNIST . MNIST contiene 70,000 imágenes en blanco y negro en 10 categorías. La resolución es baja (28 x 28 pixels). Siempre es importante explorar un dataset antes de usarlo. . dataset = keras.datasets.mnist (train_images, train_labels), (test_images, test_labels) = dataset.load_data() . Hay 60,000 imágenes para entrenar: . print(train_images.shape) . Y 10,000 imágenes en el set de prueba: . print(test_images.shape) . Cada etiqueta es un número entero 0-9: . print(train_labels) . Preprocesar los datos . Normalizamos los valores de píxeles entre 0 y 1. Importante hacerlo tanto en el set de entrenamiento como el de prueba: . train_images = train_images / 255.0 test_images = test_images / 255.0 . Vemos 25 imágenes con sus etiquetas: . plt.figure(figsize=(10,10)) for i in range(25): plt.subplot(5,5,i+1) plt.xticks([]) plt.yticks([]) plt.grid(False) plt.imshow(train_images[i], cmap=plt.cm.binary) plt.xlabel(train_labels[i]) plt.show() . Crear las capas . Neural networks are made up of layers. Here, you&#39;ll define the layers, and assemble them into a model. We will start with a single Dense layer. . What does a layer do? . The basic building block of a neural network is the layer. Layers extract representations from the data fed into them. For example: . The first layer in a network might receives the pixel values as input. From these, it learns to detect edges (combinations of pixels). . | The next layer in the network receives edges as input, and may learn to detect lines (combinations of edges). . | If you added another layer, it might learn to detect shapes (combinations of edges). . | . The &quot;Deep&quot; in &quot;Deep Learning&quot; refers to the depth of the network. Deeper networks can learn increasingly abstract patterns. Roughly, the width of a layer (in terms of the number of neurons) refers to the number of patterns it can learn of each type. . Most of deep learning consists of chaining together simple layers. Most layers, such as tf.keras.layers.Dense, have parameters that are initialized randomly, then tuned (or learned) during training by gradient descent. . # A linear model model = keras.Sequential([ keras.layers.Flatten(input_shape=(28, 28)), keras.layers.Dense(10, activation=&#39;softmax&#39;) ]) . La primera capa, tf.keras.layers.Flatten, transforma el formato de las imágenes desde un array 2D (de 28 x 28 pixels) a uno unidimensional (de 28 * 28 = 784 pixels). Es como aplanar la imagen y poner los pixels en línea. Esta capa no tiene parámetros para aprender y es necesaria porque las capas densas necesitan arrays como entrada. . Después de aplanar la imagen, el modelo tiene una única capa densa. Es una capa densa completamente conectada. La capa densa tiene 10 unidades con una activación tipo softmax, que devuelve un array con 10 notas de probabilidad que suman 1. . Después de clasificar cada imagen, cada neurona contiene una nota (puntuación) con la probabilidad de que la imagen pertenezca a uno de las 10 clases. . Compilar el modelo . Before the model is ready for training, it needs a few more settings. These are added during the model&#39;s compile step: . Loss function — This measures how accurate the model is during training. You want to minimize this function to &quot;steer&quot; the model in the right direction. . Optimizer — This is how the model is updated based on the data it sees and its loss function. . Metrics — Used to monitor the training and testing steps. The following example uses accuracy, the fraction of the images that are correctly classified. . model.compile(optimizer=&#39;adam&#39;, loss=&#39;sparse_categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) . Entrenar el modelo . Training the neural network model requires the following steps: . Feed the training data to the model. In this example, the training data is in the train_images and train_labels arrays. . | The model learns to associate images and labels. . | You ask the model to make predictions about a test set—in this example, the test_images array. . | Verify that the predictions match the labels from the test_labels array. . | To begin training, call the model.fit method — so called because it &quot;fits&quot; the model to the training data: . EPOCHS=10 model.fit(train_images, train_labels, epochs=EPOCHS) . As the model trains, the loss and accuracy metrics are displayed. This model reaches an accuracy of about 0.90 (or 90%) on the training data. Accuracy may be slightly different each time you run this code, since the parameters inside the Dense layer are randomly initialized. . Precisi&#243;n . Next, compare how the model performs on the test dataset: . test_loss, test_acc = model.evaluate(test_images, test_labels) print(&#39; nTest accuracy:&#39;, test_acc) . It turns out that the accuracy on the test dataset is a little less than the accuracy on the training dataset. This gap between training accuracy and test accuracy represents overfitting. Overfitting is when a machine learning model performs worse on new, previously unseen inputs than on the training data. An overfitted model &quot;memorizes&quot; the training data—with less accuracy on testing data. . Realizar una predicci&#243;n . Con el modelo ya entrenado, vamos a realizar una predicción sobre imágenes nuevas . predictions = model.predict(test_images) . Here, the model has predicted the label for each image in the testing set. Let&#39;s take a look at the first prediction: . print(predictions[0]) . A prediction is an array of 10 numbers. They represent the model&#39;s &quot;confidence&quot; that the image corresponds to each of the 10 digits. You can see which label has the highest confidence value: . print(tf.argmax(predictions[0])) .",
            "url": "https://rafaelsf80.github.io/notebooks/tensorflow/2020/06/01/mnist.html",
            "relUrl": "/tensorflow/2020/06/01/mnist.html",
            "date": " • Jun 1, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Intro to Convolutional neural networks (CNN) in Spanish",
            "content": "Redes convolucionales: Cats and Dogs . Se entrenará una CNN que distingue entre imágenes de perros y gatos (clasificación binaria) . Descargar dataset . La descarga no se hace sobre WiFi, sino sobre Colab, con lo que debería ser rápida. . import os import tensorflow as tf . # El dataset está en Internet origin = &#39;https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip&#39; path_to_zip = tf.keras.utils.get_file(&#39;cats_and_dogs.zip&#39;, origin=origin, extract=True) path_to_folder = os.path.join(os.path.dirname(path_to_zip), &#39;cats_and_dogs_filtered&#39;) . Contenido del zip descomprimido: . cats_and_dogs_filtered |__ train |______ cats: [cat.0.jpg, cat.1.jpg, cat.2.jpg ....] |______ dogs: [dog.0.jpg, dog.1.jpg, dog.2.jpg ...] |__ validation |______ cats: [cat.2000.jpg, cat.2001.jpg, cat.2002.jpg ....] |______ dogs: [dog.2000.jpg, dog.2001.jpg, dog.2002.jpg ...] . El dataset está dividido en train y validation. Creamos variables que apunten a esos directorios . train_dir = os.path.join(path_to_folder, &#39;train&#39;) validation_dir = os.path.join(path_to_folder, &#39;validation&#39;) . train_cats_dir = os.path.join(train_dir, &#39;cats&#39;) train_dogs_dir = os.path.join(train_dir, &#39;dogs&#39;) validation_cats_dir = os.path.join(validation_dir, &#39;cats&#39;) validation_dogs_dir = os.path.join(validation_dir, &#39;dogs&#39;) . Contamos el número de imágenes . num_cats_tr = len(os.listdir(train_cats_dir)) num_dogs_tr = len(os.listdir(train_dogs_dir)) num_cats_val = len(os.listdir(validation_cats_dir)) num_dogs_val = len(os.listdir(validation_dogs_dir)) total_train = num_cats_tr + num_dogs_tr total_val = num_cats_val + num_dogs_val print(&#39;Total training cat images:&#39;, num_cats_tr) print(&#39;Total training dog images:&#39;, num_dogs_tr) print(&#39;Total validation cat images:&#39;, num_cats_val) print(&#39;Total validation dog images:&#39;, num_dogs_val) print(&#39;&#39;) print(&quot;Total training images:&quot;, total_train) print(&quot;Total validation images:&quot;, total_val) . Hay 3000 imágenes (2000 para entrenar y 1000 para validar). Y está balanceado (mismo número de imágenes de perros y gatos) . Nota: se pueden ejecutar comandos shell en colab (ejemplo, !ls $train_cats_dir). . !ls $train_cats_dir . Mostramos algunas imágenes. . import matplotlib.pyplot as plt . _ = plt.imshow(plt.imread(os.path.join(train_cats_dir, &quot;cat.0.jpg&quot;))) . _ = plt.imshow(plt.imread(os.path.join(train_cats_dir, &quot;cat.1.jpg&quot;))) . Las imágenes tienen distinto tamaño. Hay que igualarlo antes de introducirlas en la red neuronal. . Preprocesado de datos . Para preprocesarva, vamos a: . Leer imágenes de disco. | Decodificar contenido y convertirlo en RGB. | Convertir valores de enteros a coma flotante (float). | Reescalado a valores entre 0 y 1 (mejor para redes neuronales, esto previene posibles overflows al multiplicar por pesos). | . Todas las operacfiones anteriores las realiza la clase ImageDataGenerator del paquete tf.keras. Lee las imágenes y las almacena en arrays. . from tensorflow.keras.preprocessing.image import ImageDataGenerator . # Let&#39;s resize images to this size IMG_HEIGHT = 150 IMG_WIDTH = 150 . # Rescale the pixel values to range between 0 and 1 train_generator = ImageDataGenerator(rescale=1./255) val_generator = ImageDataGenerator(rescale=1./255) . After defining the generators for training and validation images, the flow_from_directory method load images from the disk, applies rescaling, and resizes the images into the required dimensions. . batch_size = 32 # Read a batch of 64 images at each step . train_data_gen = train_generator.flow_from_directory(batch_size=batch_size, directory=train_dir, shuffle=True, target_size=(IMG_HEIGHT, IMG_WIDTH), class_mode=&#39;binary&#39;) . val_data_gen = val_generator.flow_from_directory(batch_size=batch_size, directory=validation_dir, target_size=(IMG_HEIGHT, IMG_WIDTH), class_mode=&#39;binary&#39;) . Usamos generators para mostrar algunas im&#225;genes y sus etiquetas . Next, we will extract a batch of images from the training generator, then plot several of them with matplotlib. The next function returns a batch from the dataset. The return value of next function is in form of (x_train, y_train) where x_train is the pixel values and y_train is the labels. . image_batch, labels_batch = next(train_data_gen) . # The shape will be (32, 150, 150, 3) # This means a list of 32 images, each of which is 150x150x3. # The 3 at the end refers to the R,G,B color channels. # A grayscale image would be (for example) 150x150x1 print(image_batch.shape) . # The shape (32,) means a list of 64 numbers # each of these will either be 0 or 1 print(labels_batch.shape) . # This function will plot images returned by the generator # in a grid with 1 row and 5 columns def plot_images(images): fig, axes = plt.subplots(1, 5, figsize=(10,10)) axes = axes.flatten() for img, ax in zip(images, axes): ax.imshow(img) ax.axis(&#39;off&#39;) plt.tight_layout() plt.show() . plot_images(image_batch[:5]) . Next, let&#39;s retrieve the labels. All images will be labeled either 0 or 1, since this is a binary classification problem. . # Here are the first 5 labels from the dataset # that correspond to the images above print(labels_batch[:5]) . # Here, we can see that &quot;0&quot; maps to cat, # and &quot;1&quot; maps to dog print(train_data_gen.class_indices) . Crear modelo . El modelo tiene 3 capas convolucionales con max pooling. Hay al final una capa completamente conectada con 256 unidades. la salida es 0 ó 1 con una función de activación sigmoid. Si cerca de 1, es un perro, si no, es un gato. . from tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPooling2D from tensorflow.keras.models import Sequential . model = Sequential([ Conv2D(32, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;, input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)), MaxPooling2D(), Conv2D(32, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;), MaxPooling2D(), Conv2D(64, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;), MaxPooling2D(), Flatten(), Dense(256, activation=&#39;relu&#39;), Dense(1, activation=&#39;sigmoid&#39;) ]) . Compilamos el modelo, y seleccionamos el optimizador Adam para el descenso de gradientes, y binary cross entropy para la función de pérdidas (cross entropy mide aproximadamente la distancia entre la predicción de la red y la que querríamos que tuviera). . model.compile(optimizer=&#39;adam&#39;, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) . Vemos un resumen con el método summary: . model.summary() . Notar que este modelo tiene 5M de parámetros (ó pesos). El modelo está listo para entrenar, usando las salidas de antes de ImagedataGenerator . Entrenar el modelo . Use the fit method to train the network. You will train the model for 15 epochs (an epoch is one &quot;sweep&quot; over the training set, where each image is used once to perform a round of gradient descent, and update the models parameters). This will take one to two minutes, so let&#39;s start it now: . epochs = 15 . history = model.fit( train_data_gen, epochs=epochs, validation_data=val_data_gen, ) . Inside model.fit, TensorFlow uses gradient descent to find useful values for all the weights in the model. When you create the model, the weights are initialized randomly, then gradually improved over time. The data generator is used to load batches of data off disk. Then, for each batch: . The model performs a forward pass (the images are classified by the network). | Then, the model performs a backward pass (the error is computed, then each weight is slightly adjusted using gradient descent to improve the accuracy on the next iteration). | . Gradient descent is an iterative process. The longer you train the model, the more accurate it will become on the training set. But, the more likely it is to overfit! Meaning, the model will begin to memorize the training images, rather than learn patterns that enable it generalize to new images not included in the training set. . We can see whether overfitting is present by comparing the accuracy on the training and validation data. | . If you look at the accuracy figures reported above, you should see that training accuracy is over 90%, while validation accuracy is only around 70%. . Comprobar overfitting . El precisión en el set de validación es importante: it helps you estimate how well our model is likely to work on new, unseen data in the future. To see how much overfitting is present (and when it occurs), we will create two plots, one for accuracy, and another for loss. Roughly, loss (or error) is the inverse of accuracy (lower is better). Unlike accuracy, loss takes the confidence of a prediction into account (a confidently wrong predicitions has a higher loss than one that is only slightly wrong). . acc = history.history[&#39;accuracy&#39;] val_acc = history.history[&#39;val_accuracy&#39;] loss = history.history[&#39;loss&#39;] val_loss = history.history[&#39;val_loss&#39;] epochs_range = range(epochs) plt.figure(figsize=(8, 8)) plt.subplot(1, 2, 1) plt.plot(epochs_range, acc, label=&#39;Training Accuracy&#39;) plt.plot(epochs_range, val_acc, label=&#39;Validation Accuracy&#39;) plt.legend(loc=&#39;lower right&#39;) plt.title(&#39;Training and Validation Accuracy&#39;) plt.subplot(1, 2, 2) plt.plot(epochs_range, loss, label=&#39;Training Loss&#39;) plt.plot(epochs_range, val_loss, label=&#39;Validation Loss&#39;) plt.legend(loc=&#39;upper right&#39;) plt.title(&#39;Training and Validation Loss&#39;) plt.show() . Overfitting occurs when the validation loss stops decreasing. In this case, that occurs around epoch 5 (give or take). Your results may be slightly different each time you run this code (since the weights are initialized randomly). . Why does overfitting happen? When there are only a &quot;small&quot; number of training examples, the model sometimes learns from noises or unwanted details, to an extent that it negatively impacts the performance of the model on new examples. It means that the model will have a difficult time &quot;generalizing&quot; on a new dataset (making accurate predictions on images that weren&#39;t included in the training set). . Optional: reducir overfitting . Instructions . In this exercise, you will use data augmentation and dropout to improve your model. Follow along by reading and running the code below. There are two TODOs for you to complete, and a solution is given below. . Data augmentation . Overfitting occurs when there are a &quot;small&quot; number of training examples. One way to fix this problem is to increase the size of the training set, by gathering more data (the larger and more diverse the dataset, the better!) . We can also use a technique called &quot;data augmentation&quot; to increase the size of the training set, by generating new examples from existing ones by applying random transformations (for example, rotation) that yield believable-looking images. . This is especially effective when working with images. For example, our training set may only contain images of cats that are right side up. If our validation set contains images of cats that are upside down, our model may have trouble classifying them correctly. To help teach it that cats can appear in any orientation, we will randomly rotate images from our training set during training. This helps expose the model to more aspects of the data, and can lead to better generalization. . Data augmentation is built into the ImageDataGenerator. You can specifiy different transformations, and it will take care of applying then during the training. . # Let&#39;s create new data generators, this time with # data augmentation enabled train_generator = ImageDataGenerator( rescale=1./255, rotation_range=45, width_shift_range=.15, height_shift_range=.15, horizontal_flip=True, zoom_range=0.5 ) . train_data_gen = train_generator.flow_from_directory(batch_size=32, directory=train_dir, shuffle=True, target_size=(IMG_HEIGHT, IMG_WIDTH), class_mode=&#39;binary&#39;) . The next cell will show how the same training image appears when used with five different types of data augmentation. . augmented_images = [train_data_gen[0][0][0] for i in range(5)] plot_images(augmented_images) . We only apply data augmentation to the training examples, so our validation generator looks the same as before. . val_generator = ImageDataGenerator(rescale=1./255) . val_data_gen = val_generator.flow_from_directory(batch_size=32, directory=validation_dir, target_size=(IMG_HEIGHT, IMG_WIDTH), class_mode=&#39;binary&#39;) . Dropout . Another technique to reduce overfitting is to introduce dropout to the network. Dropout is a form of regularization that makes it more difficult for the network to memorize rare details (instead, it is forced to learn more general patterns). . When you apply dropout to a layer it randomly drops out (set to zero) a number of activations during training. Dropout takes a fractional number as its input value, in the form such as 0.1, 0.2, 0.4, etc. This means dropping out 10%, 20% or 40% of the output units randomly from the applied layer. . When appling 0.1 dropout to a certain layer, it randomly deactivates 10% of the output units in each training epoch. . Create a new model using Dropout. You&#39;ll reuse the model definition from above, and add a Dropout layer. . from tensorflow.keras.layers import Dropout . # TODO: Your code here # Create a new CNN that takes advantage of Dropout. # 1) Reuse the model declared in tutorial above. # 2) Add a new line that says &quot;Dropout(0.2),&quot; immediately # before the line that says &quot;Flatten()&quot;. . Solution . #@title model = Sequential([ Conv2D(32, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;, input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)), MaxPooling2D(), Conv2D(32, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;), MaxPooling2D(), Conv2D(64, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;), MaxPooling2D(), Dropout(0.2), Flatten(), Dense(256, activation=&#39;relu&#39;), Dense(1, activation=&#39;sigmoid&#39;) ]) . After introducing dropout to the network, compile your model and view the layers summary. You should see a Dropout layer right before flatten. . model.compile(optimizer=&#39;adam&#39;, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) model.summary() . Train your new model . Add code to train your new model. Previously, we trained for 15 epochs. You will need to train this new modek for more epochs, as data augmentation and dropout make it more difficult for a CNN to memorize the training data (this is what we want!). . Here, you&#39;ll train this model for 25 epochs. This may take a few minutes, and you may need to train it for longer to reach peak accuracy. If you like, you can continue experimenting with that at home. . epochs = 25 . # TODO: your code here # Add code to call model.fit, using your new # data generators with image augmentation # For reference, see the &quot;Train the model&quot; # section above . Solution . #@title history = model.fit( train_data_gen, epochs=epochs, validation_data=val_data_gen, ) . Evaluate your new model . Finally, let&#39;s again create plots of accuracy and loss (we use these plots often in practice!) Now, compare the loss and accuracy curves for the training and validation data. Were you able to achieve a higher validation accuracy than before? Note that even this model will eventually overfit. To prevent that, we use a technique called early stopping (we stop training when the validation loss is no longer decreasing). . acc = history.history[&#39;accuracy&#39;] val_acc = history.history[&#39;val_accuracy&#39;] loss = history.history[&#39;loss&#39;] val_loss = history.history[&#39;val_loss&#39;] epochs_range = range(epochs) plt.figure(figsize=(8, 8)) plt.subplot(1, 2, 1) plt.plot(epochs_range, acc, label=&#39;Training Accuracy&#39;) plt.plot(epochs_range, val_acc, label=&#39;Validation Accuracy&#39;) plt.legend(loc=&#39;lower right&#39;) plt.title(&#39;Training and Validation Accuracy&#39;) plt.subplot(1, 2, 2) plt.plot(epochs_range, loss, label=&#39;Training Loss&#39;) plt.plot(epochs_range, val_loss, label=&#39;Validation Loss&#39;) plt.legend(loc=&#39;upper right&#39;) plt.title(&#39;Training and Validation Loss&#39;) plt.show() .",
            "url": "https://rafaelsf80.github.io/notebooks/tensorflow/2020/06/01/cats-dogs.html",
            "relUrl": "/tensorflow/2020/06/01/cats-dogs.html",
            "date": " • Jun 1, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://rafaelsf80.github.io/notebooks/tensorflow/2020/02/20/test.html",
            "relUrl": "/tensorflow/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Transfer Google Drive to GCS in Colab",
            "content": "from google.colab import drive . drive.mount(&#39;/content/drive&#39;) . project_id = &lt;YOUR_PROJECT_ID&gt; . !gcloud config set project $project_id . !gsutil ls . !gcloud auth login . !gsutil ls . !gsutil -m cp -r /content/drive/My Drive/a/06/* gs://BUCKET_NAME/06/ .",
            "url": "https://rafaelsf80.github.io/notebooks/googledrive/colab/googlecloud/2020/02/01/transfer-Drive2GCS.html",
            "relUrl": "/googledrive/colab/googlecloud/2020/02/01/transfer-Drive2GCS.html",
            "date": " • Feb 1, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://rafaelsf80.github.io/notebooks/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Site under construction . This is my Github page . This is my LinkedIn page .",
          "url": "https://rafaelsf80.github.io/notebooks/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://rafaelsf80.github.io/notebooks/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}