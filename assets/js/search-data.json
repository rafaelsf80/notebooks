{
  
    
        "post0": {
            "title": "Intro Redes neuronales",
            "content": "Importar TensorFlow . Instalamos TensorFlow. Se puede instalar con pipen Colab, pero el magic command es más rápido. También accesible en este enlace. . %tensorflow_version 2.x import tensorflow as tf print(&quot;You are using TensorFlow version&quot;, tf.__version__) if len(tf.config.list_physical_devices(&#39;GPU&#39;)) &gt; 0: print(&quot;You have a GPU enabled.&quot;) else: print(&quot;Enable a GPU before running this notebook.&quot;) . Colab tiene varias GPUS disponibles (se asigna una aleatoria, dependiendo de la disponibilidad). Para ver tipos de GPUs, se debe ejecutar !nvidia-smi en una celda. . # In this notebook, we&#39;ll use Keras: TensorFlow&#39;s user-friendly API to # define neural networks. Let&#39;s import Keras now. from tensorflow import keras import matplotlib.pyplot as plt . Descargar el dataset de MNIST . MNIST contiene 70,000 imágenes en blanco y negro en 10 categorías. La resolución es baja (28 x 28 pixels). Siempre es importante explorar un dataset antes de usarlo. . dataset = keras.datasets.mnist (train_images, train_labels), (test_images, test_labels) = dataset.load_data() . Hay 60,000 imágenes para entrenar: . print(train_images.shape) . Y 10,000 imágenes en el set de prueba: . print(test_images.shape) . Cada etiqueta es un número entero 0-9: . print(train_labels) . Preprocesar los datos . Normalizamos los valores de píxeles entre 0 y 1. Importante hacerlo tanto en el set de entrenamiento como el de prueba: . train_images = train_images / 255.0 test_images = test_images / 255.0 . Vemos 25 imágenes con sus etiquetas: . plt.figure(figsize=(10,10)) for i in range(25): plt.subplot(5,5,i+1) plt.xticks([]) plt.yticks([]) plt.grid(False) plt.imshow(train_images[i], cmap=plt.cm.binary) plt.xlabel(train_labels[i]) plt.show() . Crear las capas . Neural networks are made up of layers. Here, you&#39;ll define the layers, and assemble them into a model. We will start with a single Dense layer. . What does a layer do? . The basic building block of a neural network is the layer. Layers extract representations from the data fed into them. For example: . The first layer in a network might receives the pixel values as input. From these, it learns to detect edges (combinations of pixels). . | The next layer in the network receives edges as input, and may learn to detect lines (combinations of edges). . | If you added another layer, it might learn to detect shapes (combinations of edges). . | . The &quot;Deep&quot; in &quot;Deep Learning&quot; refers to the depth of the network. Deeper networks can learn increasingly abstract patterns. Roughly, the width of a layer (in terms of the number of neurons) refers to the number of patterns it can learn of each type. . Most of deep learning consists of chaining together simple layers. Most layers, such as tf.keras.layers.Dense, have parameters that are initialized randomly, then tuned (or learned) during training by gradient descent. . # A linear model model = keras.Sequential([ keras.layers.Flatten(input_shape=(28, 28)), keras.layers.Dense(10, activation=&#39;softmax&#39;) ]) . La primera capa, tf.keras.layers.Flatten, transforma el formato de las imágenes desde un array 2D (de 28 x 28 pixels) a uno unidimensional (de 28 * 28 = 784 pixels). Es como aplanar la imagen y poner los pixels en línea. Esta capa no tiene parámetros para aprender y es necesaria porque las capas densas necesitan arrays como entrada. . Después de aplanar la imagen, el modelo tiene una única capa densa. Es una capa densa completamente conectada. La capa densa tiene 10 unidades con una activación tipo softmax, que devuelve un array con 10 notas de probabilidad que suman 1. . Después de clasificar cada imagen, cada neurona contiene una nota (puntuación) con la probabilidad de que la imagen pertenezca a uno de las 10 clases. . Compilar el modelo . Before the model is ready for training, it needs a few more settings. These are added during the model&#39;s compile step: . Loss function — This measures how accurate the model is during training. You want to minimize this function to &quot;steer&quot; the model in the right direction. . Optimizer — This is how the model is updated based on the data it sees and its loss function. . Metrics — Used to monitor the training and testing steps. The following example uses accuracy, the fraction of the images that are correctly classified. . model.compile(optimizer=&#39;adam&#39;, loss=&#39;sparse_categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) . Entrenar el modelo . Training the neural network model requires the following steps: . Feed the training data to the model. In this example, the training data is in the train_images and train_labels arrays. . | The model learns to associate images and labels. . | You ask the model to make predictions about a test set—in this example, the test_images array. . | Verify that the predictions match the labels from the test_labels array. . | To begin training, call the model.fit method — so called because it &quot;fits&quot; the model to the training data: . EPOCHS=10 model.fit(train_images, train_labels, epochs=EPOCHS) . As the model trains, the loss and accuracy metrics are displayed. This model reaches an accuracy of about 0.90 (or 90%) on the training data. Accuracy may be slightly different each time you run this code, since the parameters inside the Dense layer are randomly initialized. . Precisi&#243;n . Next, compare how the model performs on the test dataset: . test_loss, test_acc = model.evaluate(test_images, test_labels) print(&#39; nTest accuracy:&#39;, test_acc) . It turns out that the accuracy on the test dataset is a little less than the accuracy on the training dataset. This gap between training accuracy and test accuracy represents overfitting. Overfitting is when a machine learning model performs worse on new, previously unseen inputs than on the training data. An overfitted model &quot;memorizes&quot; the training data—with less accuracy on testing data. . Realizar una predicci&#243;n . Con el modelo ya entrenado, vamos a realizar una predicción sobre imágenes nuevas . predictions = model.predict(test_images) . Here, the model has predicted the label for each image in the testing set. Let&#39;s take a look at the first prediction: . print(predictions[0]) . A prediction is an array of 10 numbers. They represent the model&#39;s &quot;confidence&quot; that the image corresponds to each of the 10 digits. You can see which label has the highest confidence value: . print(tf.argmax(predictions[0])) .",
            "url": "https://rafaelsf80.github.io/notebooks/jupyter/2020/06/01/mnist.html",
            "relUrl": "/jupyter/2020/06/01/mnist.html",
            "date": " • Jun 1, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Intro Redes convolucionales",
            "content": "Redes convolucionales: Cats and Dogs . Se entrenará una CNN que distingue entre imágenes de perros y gatos (clasificación binaria) . Descargar dataset . La descarga no se hace sobre WiFi, sino sobre Colab, con lo que debería ser rápida. . import os import tensorflow as tf . # El dataset está en Internet origin = &#39;https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip&#39; path_to_zip = tf.keras.utils.get_file(&#39;cats_and_dogs.zip&#39;, origin=origin, extract=True) path_to_folder = os.path.join(os.path.dirname(path_to_zip), &#39;cats_and_dogs_filtered&#39;) . Contenido del zip descomprimido: . cats_and_dogs_filtered |__ train |______ cats: [cat.0.jpg, cat.1.jpg, cat.2.jpg ....] |______ dogs: [dog.0.jpg, dog.1.jpg, dog.2.jpg ...] |__ validation |______ cats: [cat.2000.jpg, cat.2001.jpg, cat.2002.jpg ....] |______ dogs: [dog.2000.jpg, dog.2001.jpg, dog.2002.jpg ...] . El dataset está dividido en train y validation. Creamos variables que apunten a esos directorios . train_dir = os.path.join(path_to_folder, &#39;train&#39;) validation_dir = os.path.join(path_to_folder, &#39;validation&#39;) . train_cats_dir = os.path.join(train_dir, &#39;cats&#39;) train_dogs_dir = os.path.join(train_dir, &#39;dogs&#39;) validation_cats_dir = os.path.join(validation_dir, &#39;cats&#39;) validation_dogs_dir = os.path.join(validation_dir, &#39;dogs&#39;) . Contamos el número de imágenes . num_cats_tr = len(os.listdir(train_cats_dir)) num_dogs_tr = len(os.listdir(train_dogs_dir)) num_cats_val = len(os.listdir(validation_cats_dir)) num_dogs_val = len(os.listdir(validation_dogs_dir)) total_train = num_cats_tr + num_dogs_tr total_val = num_cats_val + num_dogs_val print(&#39;Total training cat images:&#39;, num_cats_tr) print(&#39;Total training dog images:&#39;, num_dogs_tr) print(&#39;Total validation cat images:&#39;, num_cats_val) print(&#39;Total validation dog images:&#39;, num_dogs_val) print(&#39;&#39;) print(&quot;Total training images:&quot;, total_train) print(&quot;Total validation images:&quot;, total_val) . Hay 3000 imágenes (2000 para entrenar y 1000 para validar). Y está balanceado (mismo número de imágenes de perros y gatos) . Nota: se pueden ejecutar comandos shell en colab (ejemplo, !ls $train_cats_dir). . !ls $train_cats_dir . Mostramos algunas imágenes. . import matplotlib.pyplot as plt . _ = plt.imshow(plt.imread(os.path.join(train_cats_dir, &quot;cat.0.jpg&quot;))) . _ = plt.imshow(plt.imread(os.path.join(train_cats_dir, &quot;cat.1.jpg&quot;))) . Las imágenes tienen distinto tamaño. Hay que igualarlo antes de introducirlas en la red neuronal. . Preprocesado de datos . Para preprocesarva, vamos a: . Leer imágenes de disco. | Decodificar contenido y convertirlo en RGB. | Convertir valores de enteros a coma flotante (float). | Reescalado a valores entre 0 y 1 (mejor para redes neuronales, esto previene posibles overflows al multiplicar por pesos). | . Todas las operacfiones anteriores las realiza la clase ImageDataGenerator del paquete tf.keras. Lee las imágenes y las almacena en arrays. . from tensorflow.keras.preprocessing.image import ImageDataGenerator . # Let&#39;s resize images to this size IMG_HEIGHT = 150 IMG_WIDTH = 150 . # Rescale the pixel values to range between 0 and 1 train_generator = ImageDataGenerator(rescale=1./255) val_generator = ImageDataGenerator(rescale=1./255) . After defining the generators for training and validation images, the flow_from_directory method load images from the disk, applies rescaling, and resizes the images into the required dimensions. . batch_size = 32 # Read a batch of 64 images at each step . train_data_gen = train_generator.flow_from_directory(batch_size=batch_size, directory=train_dir, shuffle=True, target_size=(IMG_HEIGHT, IMG_WIDTH), class_mode=&#39;binary&#39;) . val_data_gen = val_generator.flow_from_directory(batch_size=batch_size, directory=validation_dir, target_size=(IMG_HEIGHT, IMG_WIDTH), class_mode=&#39;binary&#39;) . Usamos generators para mostrar algunas im&#225;genes y sus etiquetas . Next, we will extract a batch of images from the training generator, then plot several of them with matplotlib. The next function returns a batch from the dataset. The return value of next function is in form of (x_train, y_train) where x_train is the pixel values and y_train is the labels. . image_batch, labels_batch = next(train_data_gen) . # The shape will be (32, 150, 150, 3) # This means a list of 32 images, each of which is 150x150x3. # The 3 at the end refers to the R,G,B color channels. # A grayscale image would be (for example) 150x150x1 print(image_batch.shape) . # The shape (32,) means a list of 64 numbers # each of these will either be 0 or 1 print(labels_batch.shape) . # This function will plot images returned by the generator # in a grid with 1 row and 5 columns def plot_images(images): fig, axes = plt.subplots(1, 5, figsize=(10,10)) axes = axes.flatten() for img, ax in zip(images, axes): ax.imshow(img) ax.axis(&#39;off&#39;) plt.tight_layout() plt.show() . plot_images(image_batch[:5]) . Next, let&#39;s retrieve the labels. All images will be labeled either 0 or 1, since this is a binary classification problem. . # Here are the first 5 labels from the dataset # that correspond to the images above print(labels_batch[:5]) . # Here, we can see that &quot;0&quot; maps to cat, # and &quot;1&quot; maps to dog print(train_data_gen.class_indices) . Crear modelo . El modelo tiene 3 capas convolucionales con max pooling. Hay al final una capa completamente conectada con 256 unidades. la salida es 0 ó 1 con una función de activación sigmoid. Si cerca de 1, es un perro, si no, es un gato. . from tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPooling2D from tensorflow.keras.models import Sequential . model = Sequential([ Conv2D(32, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;, input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)), MaxPooling2D(), Conv2D(32, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;), MaxPooling2D(), Conv2D(64, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;), MaxPooling2D(), Flatten(), Dense(256, activation=&#39;relu&#39;), Dense(1, activation=&#39;sigmoid&#39;) ]) . Compilamos el modelo, y seleccionamos el optimizador Adam para el descenso de gradientes, y binary cross entropy para la función de pérdidas (cross entropy mide aproximadamente la distancia entre la predicción de la red y la que querríamos que tuviera). . model.compile(optimizer=&#39;adam&#39;, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) . Vemos un resumen con el método summary: . model.summary() . Notar que este modelo tiene 5M de parámetros (ó pesos). El modelo está listo para entrenar, usando las salidas de antes de ImagedataGenerator . Entrenar el modelo . Use the fit method to train the network. You will train the model for 15 epochs (an epoch is one &quot;sweep&quot; over the training set, where each image is used once to perform a round of gradient descent, and update the models parameters). This will take one to two minutes, so let&#39;s start it now: . epochs = 15 . history = model.fit( train_data_gen, epochs=epochs, validation_data=val_data_gen, ) . Inside model.fit, TensorFlow uses gradient descent to find useful values for all the weights in the model. When you create the model, the weights are initialized randomly, then gradually improved over time. The data generator is used to load batches of data off disk. Then, for each batch: . The model performs a forward pass (the images are classified by the network). | Then, the model performs a backward pass (the error is computed, then each weight is slightly adjusted using gradient descent to improve the accuracy on the next iteration). | . Gradient descent is an iterative process. The longer you train the model, the more accurate it will become on the training set. But, the more likely it is to overfit! Meaning, the model will begin to memorize the training images, rather than learn patterns that enable it generalize to new images not included in the training set. . We can see whether overfitting is present by comparing the accuracy on the training and validation data. | . If you look at the accuracy figures reported above, you should see that training accuracy is over 90%, while validation accuracy is only around 70%. . Comprobar overfitting . El precisión en el set de validación es importante: it helps you estimate how well our model is likely to work on new, unseen data in the future. To see how much overfitting is present (and when it occurs), we will create two plots, one for accuracy, and another for loss. Roughly, loss (or error) is the inverse of accuracy (lower is better). Unlike accuracy, loss takes the confidence of a prediction into account (a confidently wrong predicitions has a higher loss than one that is only slightly wrong). . acc = history.history[&#39;accuracy&#39;] val_acc = history.history[&#39;val_accuracy&#39;] loss = history.history[&#39;loss&#39;] val_loss = history.history[&#39;val_loss&#39;] epochs_range = range(epochs) plt.figure(figsize=(8, 8)) plt.subplot(1, 2, 1) plt.plot(epochs_range, acc, label=&#39;Training Accuracy&#39;) plt.plot(epochs_range, val_acc, label=&#39;Validation Accuracy&#39;) plt.legend(loc=&#39;lower right&#39;) plt.title(&#39;Training and Validation Accuracy&#39;) plt.subplot(1, 2, 2) plt.plot(epochs_range, loss, label=&#39;Training Loss&#39;) plt.plot(epochs_range, val_loss, label=&#39;Validation Loss&#39;) plt.legend(loc=&#39;upper right&#39;) plt.title(&#39;Training and Validation Loss&#39;) plt.show() . Overfitting occurs when the validation loss stops decreasing. In this case, that occurs around epoch 5 (give or take). Your results may be slightly different each time you run this code (since the weights are initialized randomly). . Why does overfitting happen? When there are only a &quot;small&quot; number of training examples, the model sometimes learns from noises or unwanted details, to an extent that it negatively impacts the performance of the model on new examples. It means that the model will have a difficult time &quot;generalizing&quot; on a new dataset (making accurate predictions on images that weren&#39;t included in the training set). . Optional: reducir overfitting . Instructions . In this exercise, you will use data augmentation and dropout to improve your model. Follow along by reading and running the code below. There are two TODOs for you to complete, and a solution is given below. . Data augmentation . Overfitting occurs when there are a &quot;small&quot; number of training examples. One way to fix this problem is to increase the size of the training set, by gathering more data (the larger and more diverse the dataset, the better!) . We can also use a technique called &quot;data augmentation&quot; to increase the size of the training set, by generating new examples from existing ones by applying random transformations (for example, rotation) that yield believable-looking images. . This is especially effective when working with images. For example, our training set may only contain images of cats that are right side up. If our validation set contains images of cats that are upside down, our model may have trouble classifying them correctly. To help teach it that cats can appear in any orientation, we will randomly rotate images from our training set during training. This helps expose the model to more aspects of the data, and can lead to better generalization. . Data augmentation is built into the ImageDataGenerator. You can specifiy different transformations, and it will take care of applying then during the training. . # Let&#39;s create new data generators, this time with # data augmentation enabled train_generator = ImageDataGenerator( rescale=1./255, rotation_range=45, width_shift_range=.15, height_shift_range=.15, horizontal_flip=True, zoom_range=0.5 ) . train_data_gen = train_generator.flow_from_directory(batch_size=32, directory=train_dir, shuffle=True, target_size=(IMG_HEIGHT, IMG_WIDTH), class_mode=&#39;binary&#39;) . The next cell will show how the same training image appears when used with five different types of data augmentation. . augmented_images = [train_data_gen[0][0][0] for i in range(5)] plot_images(augmented_images) . We only apply data augmentation to the training examples, so our validation generator looks the same as before. . val_generator = ImageDataGenerator(rescale=1./255) . val_data_gen = val_generator.flow_from_directory(batch_size=32, directory=validation_dir, target_size=(IMG_HEIGHT, IMG_WIDTH), class_mode=&#39;binary&#39;) . Dropout . Another technique to reduce overfitting is to introduce dropout to the network. Dropout is a form of regularization that makes it more difficult for the network to memorize rare details (instead, it is forced to learn more general patterns). . When you apply dropout to a layer it randomly drops out (set to zero) a number of activations during training. Dropout takes a fractional number as its input value, in the form such as 0.1, 0.2, 0.4, etc. This means dropping out 10%, 20% or 40% of the output units randomly from the applied layer. . When appling 0.1 dropout to a certain layer, it randomly deactivates 10% of the output units in each training epoch. . Create a new model using Dropout. You&#39;ll reuse the model definition from above, and add a Dropout layer. . from tensorflow.keras.layers import Dropout . # TODO: Your code here # Create a new CNN that takes advantage of Dropout. # 1) Reuse the model declared in tutorial above. # 2) Add a new line that says &quot;Dropout(0.2),&quot; immediately # before the line that says &quot;Flatten()&quot;. . Solution . #@title model = Sequential([ Conv2D(32, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;, input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)), MaxPooling2D(), Conv2D(32, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;), MaxPooling2D(), Conv2D(64, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;), MaxPooling2D(), Dropout(0.2), Flatten(), Dense(256, activation=&#39;relu&#39;), Dense(1, activation=&#39;sigmoid&#39;) ]) . After introducing dropout to the network, compile your model and view the layers summary. You should see a Dropout layer right before flatten. . model.compile(optimizer=&#39;adam&#39;, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) model.summary() . Train your new model . Add code to train your new model. Previously, we trained for 15 epochs. You will need to train this new modek for more epochs, as data augmentation and dropout make it more difficult for a CNN to memorize the training data (this is what we want!). . Here, you&#39;ll train this model for 25 epochs. This may take a few minutes, and you may need to train it for longer to reach peak accuracy. If you like, you can continue experimenting with that at home. . epochs = 25 . # TODO: your code here # Add code to call model.fit, using your new # data generators with image augmentation # For reference, see the &quot;Train the model&quot; # section above . Solution . #@title history = model.fit( train_data_gen, epochs=epochs, validation_data=val_data_gen, ) . Evaluate your new model . Finally, let&#39;s again create plots of accuracy and loss (we use these plots often in practice!) Now, compare the loss and accuracy curves for the training and validation data. Were you able to achieve a higher validation accuracy than before? Note that even this model will eventually overfit. To prevent that, we use a technique called early stopping (we stop training when the validation loss is no longer decreasing). . acc = history.history[&#39;accuracy&#39;] val_acc = history.history[&#39;val_accuracy&#39;] loss = history.history[&#39;loss&#39;] val_loss = history.history[&#39;val_loss&#39;] epochs_range = range(epochs) plt.figure(figsize=(8, 8)) plt.subplot(1, 2, 1) plt.plot(epochs_range, acc, label=&#39;Training Accuracy&#39;) plt.plot(epochs_range, val_acc, label=&#39;Validation Accuracy&#39;) plt.legend(loc=&#39;lower right&#39;) plt.title(&#39;Training and Validation Accuracy&#39;) plt.subplot(1, 2, 2) plt.plot(epochs_range, loss, label=&#39;Training Loss&#39;) plt.plot(epochs_range, val_loss, label=&#39;Validation Loss&#39;) plt.legend(loc=&#39;upper right&#39;) plt.title(&#39;Training and Validation Loss&#39;) plt.show() .",
            "url": "https://rafaelsf80.github.io/notebooks/jupyter/2020/06/01/cats-dogs.html",
            "relUrl": "/jupyter/2020/06/01/cats-dogs.html",
            "date": " • Jun 1, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://rafaelsf80.github.io/notebooks/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Transfer Google Drive to GCS in Colab",
            "content": "from google.colab import drive . drive.mount(&#39;/content/drive&#39;) . project_id = &lt;YOUR_PROJECT_ID&gt; . !gcloud config set project $project_id . !gsutil ls . !gcloud auth login . !gsutil ls . !gsutil -m cp -r /content/drive/My Drive/a/06/* gs://BUCKET_NAME/06/ .",
            "url": "https://rafaelsf80.github.io/notebooks/drive/gcs/2020/02/01/transfer-Drive2GCS.html",
            "relUrl": "/drive/gcs/2020/02/01/transfer-Drive2GCS.html",
            "date": " • Feb 1, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://rafaelsf80.github.io/notebooks/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://rafaelsf80.github.io/notebooks/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://rafaelsf80.github.io/notebooks/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}