{
  
    
        "post0": {
            "title": "Cats and dogs notebook",
            "content": "Tutorial: Cats and Dogs . You&#39;ll train a CNN to classify images of cats and dogs using a real-world dataset you will download from the web. . Descargar dataset . Although you are downloading large files, you are doing so in Colab through Google Cloud Platform (instead of over your local WiFi connection). This means that downloads will usually be fast, regardless of your internet connection. . import os . # El dataset está en Internet origin = &#39;https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip&#39; path_to_zip = tf.keras.utils.get_file(&#39;cats_and_dogs.zip&#39;, origin=origin, extract=True) path_to_folder = os.path.join(os.path.dirname(path_to_zip), &#39;cats_and_dogs_filtered&#39;) . The unzipped dataset has the following directory structure: . cats_and_dogs_filtered |__ train |______ cats: [cat.0.jpg, cat.1.jpg, cat.2.jpg ....] |______ dogs: [dog.0.jpg, dog.1.jpg, dog.2.jpg ...] |__ validation |______ cats: [cat.2000.jpg, cat.2001.jpg, cat.2002.jpg ....] |______ dogs: [dog.2000.jpg, dog.2001.jpg, dog.2002.jpg ...] . El dataset está dividido en train y validation. Creamos variables que apunten a esos directorios . train_dir = os.path.join(path_to_folder, &#39;train&#39;) validation_dir = os.path.join(path_to_folder, &#39;validation&#39;) . train_cats_dir = os.path.join(train_dir, &#39;cats&#39;) train_dogs_dir = os.path.join(train_dir, &#39;dogs&#39;) validation_cats_dir = os.path.join(validation_dir, &#39;cats&#39;) validation_dogs_dir = os.path.join(validation_dir, &#39;dogs&#39;) . Contamos el número de imágenes . num_cats_tr = len(os.listdir(train_cats_dir)) num_dogs_tr = len(os.listdir(train_dogs_dir)) num_cats_val = len(os.listdir(validation_cats_dir)) num_dogs_val = len(os.listdir(validation_dogs_dir)) total_train = num_cats_tr + num_dogs_tr total_val = num_cats_val + num_dogs_val print(&#39;Total training cat images:&#39;, num_cats_tr) print(&#39;Total training dog images:&#39;, num_dogs_tr) print(&#39;Total validation cat images:&#39;, num_cats_val) print(&#39;Total validation dog images:&#39;, num_dogs_val) print(&#39;&#39;) print(&quot;Total training images:&quot;, total_train) print(&quot;Total validation images:&quot;, total_val) . Hay 3000 imágenes (2000 para entrenar y 1000 para validar). Y está balanceado (mismo número de imágenes de perros y gatos) . Nota: se pueden ejecutar comandos shell en colab (ejemplo, !ls $train_cats_dir). . !ls $train_cats_dir . Mostramos algunas imágenes. . import matplotlib.pyplot as plt . _ = plt.imshow(plt.imread(os.path.join(train_cats_dir, &quot;cat.0.jpg&quot;))) . _ = plt.imshow(plt.imread(os.path.join(train_cats_dir, &quot;cat.1.jpg&quot;))) . Las imágenes tienen distinto tamaño. Hay que igualarlo antes de introducirlas en la red neuronal. . Preprocesado de datos . Next, we will need a way to read these images off disk, and to preprocess them. Specifically, we will need to: . Read the image off disk. | Decode contents of these images and convert them into RGB arrays. | Convert the pixels values from integer to floating point types. | Rescale the pixel from values between 0 and 255 to values between 0 and 1 (neural networks work better with small input values - under the hood, each input is multiplied by a weight, large inputs could result in overflow). | . Fortunately, all of these tasks can be done with the ImageDataGenerator class provided by tf.keras. It can read images from disk and preprocess them into proper arrays. . from tensorflow.keras.preprocessing.image import ImageDataGenerator . # Let&#39;s resize images to this size IMG_HEIGHT = 150 IMG_WIDTH = 150 . # Rescale the pixel values to range between 0 and 1 train_generator = ImageDataGenerator(rescale=1./255) val_generator = ImageDataGenerator(rescale=1./255) . After defining the generators for training and validation images, the flow_from_directory method load images from the disk, applies rescaling, and resizes the images into the required dimensions. . batch_size = 32 # Read a batch of 64 images at each step . train_data_gen = train_generator.flow_from_directory(batch_size=batch_size, directory=train_dir, shuffle=True, target_size=(IMG_HEIGHT, IMG_WIDTH), class_mode=&#39;binary&#39;) . val_data_gen = val_generator.flow_from_directory(batch_size=batch_size, directory=validation_dir, target_size=(IMG_HEIGHT, IMG_WIDTH), class_mode=&#39;binary&#39;) . Usamos generators para mostrar algunas im&#225;genes y sus etiquetas . Next, we will extract a batch of images from the training generator, then plot several of them with matplotlib. The next function returns a batch from the dataset. The return value of next function is in form of (x_train, y_train) where x_train is the pixel values and y_train is the labels. . image_batch, labels_batch = next(train_data_gen) . # The shape will be (32, 150, 150, 3) # This means a list of 32 images, each of which is 150x150x3. # The 3 at the end refers to the R,G,B color channels. # A grayscale image would be (for example) 150x150x1 print(image_batch.shape) . # The shape (32,) means a list of 64 numbers # each of these will either be 0 or 1 print(labels_batch.shape) . # This function will plot images returned by the generator # in a grid with 1 row and 5 columns def plot_images(images): fig, axes = plt.subplots(1, 5, figsize=(10,10)) axes = axes.flatten() for img, ax in zip(images, axes): ax.imshow(img) ax.axis(&#39;off&#39;) plt.tight_layout() plt.show() . plot_images(image_batch[:5]) . Next, let&#39;s retrieve the labels. All images will be labeled either 0 or 1, since this is a binary classification problem. . # Here are the first 5 labels from the dataset # that correspond to the images above print(labels_batch[:5]) . # Here, we can see that &quot;0&quot; maps to cat, # and &quot;1&quot; maps to dog print(train_data_gen.class_indices) . Crear modelo . Your model will consist of three convolutional blocks followed by max pooling. There&#39;s a fully connected layer with 256 units on top. This model will output class probabilities (between 0 and 1) based on the sigmoid activation function. If the output is closer to 1, the image will be classified as a dog, otherwise a cat. . from tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPooling2D from tensorflow.keras.models import Sequential . model = Sequential([ Conv2D(32, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;, input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)), MaxPooling2D(), Conv2D(32, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;), MaxPooling2D(), Conv2D(64, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;), MaxPooling2D(), Flatten(), Dense(256, activation=&#39;relu&#39;), Dense(1, activation=&#39;sigmoid&#39;) ]) . Compile the model, and select the adam optimizer for gradient descent, and binary cross entropy for the loss function (roughly, cross entropy is a way to measure the distance between the prediction we wanted the network to make, and the prediction it made). . model.compile(optimizer=&#39;adam&#39;, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) . Let&#39;s look at a diagram of all the layers of the network using the summary method: . model.summary() . This model has about 5M parameters (or weights) to learn. Our model is ready to go, and next we can train it using the data generators we created earlier. . Entrenar el modelo . Use the fit method to train the network. You will train the model for 15 epochs (an epoch is one &quot;sweep&quot; over the training set, where each image is used once to perform a round of gradient descent, and update the models parameters). This will take one to two minutes, so let&#39;s start it now: . epochs = 15 . history = model.fit( train_data_gen, epochs=epochs, validation_data=val_data_gen, ) . Inside model.fit, TensorFlow uses gradient descent to find useful values for all the weights in the model. When you create the model, the weights are initialized randomly, then gradually improved over time. The data generator is used to load batches of data off disk. Then, for each batch: . The model performs a forward pass (the images are classified by the network). | Then, the model performs a backward pass (the error is computed, then each weight is slightly adjusted using gradient descent to improve the accuracy on the next iteration). | . Gradient descent is an iterative process. The longer you train the model, the more accurate it will become on the training set. But, the more likely it is to overfit! Meaning, the model will begin to memorize the training images, rather than learn patterns that enable it generalize to new images not included in the training set. . We can see whether overfitting is present by comparing the accuracy on the training and validation data. | . If you look at the accuracy figures reported above, you should see that training accuracy is over 90%, while validation accuracy is only around 70%. . Comprobar overfitting . Accuracy on the validation data is important: it helps you estimate how well our model is likely to work on new, unseen data in the future. To see how much overfitting is present (and when it occurs), we will create two plots, one for accuracy, and another for loss. Roughly, loss (or error) is the inverse of accuracy (lower is better). Unlike accuracy, loss takes the confidence of a prediction into account (a confidently wrong predicitions has a higher loss than one that is only slightly wrong). . acc = history.history[&#39;accuracy&#39;] val_acc = history.history[&#39;val_accuracy&#39;] loss = history.history[&#39;loss&#39;] val_loss = history.history[&#39;val_loss&#39;] epochs_range = range(epochs) plt.figure(figsize=(8, 8)) plt.subplot(1, 2, 1) plt.plot(epochs_range, acc, label=&#39;Training Accuracy&#39;) plt.plot(epochs_range, val_acc, label=&#39;Validation Accuracy&#39;) plt.legend(loc=&#39;lower right&#39;) plt.title(&#39;Training and Validation Accuracy&#39;) plt.subplot(1, 2, 2) plt.plot(epochs_range, loss, label=&#39;Training Loss&#39;) plt.plot(epochs_range, val_loss, label=&#39;Validation Loss&#39;) plt.legend(loc=&#39;upper right&#39;) plt.title(&#39;Training and Validation Loss&#39;) plt.show() . Overfitting occurs when the validation loss stops decreasing. In this case, that occurs around epoch 5 (give or take). Your results may be slightly different each time you run this code (since the weights are initialized randomly). . Why does overfitting happen? When there are only a &quot;small&quot; number of training examples, the model sometimes learns from noises or unwanted details, to an extent that it negatively impacts the performance of the model on new examples. It means that the model will have a difficult time &quot;generalizing&quot; on a new dataset (making accurate predictions on images that weren&#39;t included in the training set). . Ejercicio opcional: reducir overfitting . Instructions . In this exercise, you will use data augmentation and dropout to improve your model. Follow along by reading and running the code below. There are two TODOs for you to complete, and a solution is given below. . Data augmentation . Overfitting occurs when there are a &quot;small&quot; number of training examples. One way to fix this problem is to increase the size of the training set, by gathering more data (the larger and more diverse the dataset, the better!) . We can also use a technique called &quot;data augmentation&quot; to increase the size of the training set, by generating new examples from existing ones by applying random transformations (for example, rotation) that yield believable-looking images. . This is especially effective when working with images. For example, our training set may only contain images of cats that are right side up. If our validation set contains images of cats that are upside down, our model may have trouble classifying them correctly. To help teach it that cats can appear in any orientation, we will randomly rotate images from our training set during training. This helps expose the model to more aspects of the data, and can lead to better generalization. . Data augmentation is built into the ImageDataGenerator. You can specifiy different transformations, and it will take care of applying then during the training. . # Let&#39;s create new data generators, this time with # data augmentation enabled train_generator = ImageDataGenerator( rescale=1./255, rotation_range=45, width_shift_range=.15, height_shift_range=.15, horizontal_flip=True, zoom_range=0.5 ) . train_data_gen = train_generator.flow_from_directory(batch_size=32, directory=train_dir, shuffle=True, target_size=(IMG_HEIGHT, IMG_WIDTH), class_mode=&#39;binary&#39;) . The next cell will show how the same training image appears when used with five different types of data augmentation. . augmented_images = [train_data_gen[0][0][0] for i in range(5)] plot_images(augmented_images) . We only apply data augmentation to the training examples, so our validation generator looks the same as before. . val_generator = ImageDataGenerator(rescale=1./255) . val_data_gen = val_generator.flow_from_directory(batch_size=32, directory=validation_dir, target_size=(IMG_HEIGHT, IMG_WIDTH), class_mode=&#39;binary&#39;) . Dropout . Another technique to reduce overfitting is to introduce dropout to the network. Dropout is a form of regularization that makes it more difficult for the network to memorize rare details (instead, it is forced to learn more general patterns). . When you apply dropout to a layer it randomly drops out (set to zero) a number of activations during training. Dropout takes a fractional number as its input value, in the form such as 0.1, 0.2, 0.4, etc. This means dropping out 10%, 20% or 40% of the output units randomly from the applied layer. . When appling 0.1 dropout to a certain layer, it randomly deactivates 10% of the output units in each training epoch. . Create a new model using Dropout. You&#39;ll reuse the model definition from above, and add a Dropout layer. . from tensorflow.keras.layers import Dropout . # TODO: Your code here # Create a new CNN that takes advantage of Dropout. # 1) Reuse the model declared in tutorial above. # 2) Add a new line that says &quot;Dropout(0.2),&quot; immediately # before the line that says &quot;Flatten()&quot;. . Solution . #@title model = Sequential([ Conv2D(32, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;, input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)), MaxPooling2D(), Conv2D(32, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;), MaxPooling2D(), Conv2D(64, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;), MaxPooling2D(), Dropout(0.2), Flatten(), Dense(256, activation=&#39;relu&#39;), Dense(1, activation=&#39;sigmoid&#39;) ]) . After introducing dropout to the network, compile your model and view the layers summary. You should see a Dropout layer right before flatten. . model.compile(optimizer=&#39;adam&#39;, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) model.summary() . Train your new model . Add code to train your new model. Previously, we trained for 15 epochs. You will need to train this new modek for more epochs, as data augmentation and dropout make it more difficult for a CNN to memorize the training data (this is what we want!). . Here, you&#39;ll train this model for 25 epochs. This may take a few minutes, and you may need to train it for longer to reach peak accuracy. If you like, you can continue experimenting with that at home. . epochs = 25 . # TODO: your code here # Add code to call model.fit, using your new # data generators with image augmentation # For reference, see the &quot;Train the model&quot; # section above . Solution . #@title history = model.fit( train_data_gen, epochs=epochs, validation_data=val_data_gen, ) . Evaluate your new model . Finally, let&#39;s again create plots of accuracy and loss (we use these plots often in practice!) Now, compare the loss and accuracy curves for the training and validation data. Were you able to achieve a higher validation accuracy than before? Note that even this model will eventually overfit. To prevent that, we use a technique called early stopping (we stop training when the validation loss is no longer decreasing). . acc = history.history[&#39;accuracy&#39;] val_acc = history.history[&#39;val_accuracy&#39;] loss = history.history[&#39;loss&#39;] val_loss = history.history[&#39;val_loss&#39;] epochs_range = range(epochs) plt.figure(figsize=(8, 8)) plt.subplot(1, 2, 1) plt.plot(epochs_range, acc, label=&#39;Training Accuracy&#39;) plt.plot(epochs_range, val_acc, label=&#39;Validation Accuracy&#39;) plt.legend(loc=&#39;lower right&#39;) plt.title(&#39;Training and Validation Accuracy&#39;) plt.subplot(1, 2, 2) plt.plot(epochs_range, loss, label=&#39;Training Loss&#39;) plt.plot(epochs_range, val_loss, label=&#39;Validation Loss&#39;) plt.legend(loc=&#39;upper right&#39;) plt.title(&#39;Training and Validation Loss&#39;) plt.show() . Game break: Sketch-RNN . If you&#39;d like, now would be a good time to take a break from coding and try: https://magenta.tensorflow.org/assets/sketch_rnn_demo/index.html . Exercise: Flowers . In this exercise, you write a CNN and use it to classify five different types of flowers (sunflowers, tulips, etc). The dataset contains 1000 images in the training set, and 500 in the validation set. . You will download the dataset, read and preprocess the images using ImageDataGenerator, then create, train and evaluate a model. . A code outline is written for you, and there are several sections for you to complete, using the same pattern as the tutorial above. . Download the dataset . origin = &#39;https://storage.googleapis.com/tensorflow-blog/datasets/mini_flowers.zip&#39; path_to_zip = tf.keras.utils.get_file(&#39;mini_flowers.zip&#39;, origin=origin, extract=True) path_to_folder = os.path.join(os.path.dirname(path_to_zip)) train_dir = os.path.join(path_to_folder, &quot;train/&quot;) val_dir = os.path.join(path_to_folder, &quot;val/&quot;) . Read the images off disk . train_image_generator = ImageDataGenerator(rescale=1./255) . train_data_gen = train_image_generator.flow_from_directory(batch_size=32, directory=train_dir, shuffle=True, target_size=(IMG_HEIGHT, IMG_WIDTH), class_mode=&#39;categorical&#39;) . Plot images and their labels . image_batch, labels_batch = next(train_data_gen) . plt.figure(figsize=(10,10)) for i in range(25): plt.subplot(5,5,i+1) plt.xticks([]) plt.yticks([]) plt.grid(False) plt.imshow(image_batch[i]) plt.xlabel(str(labels_batch[i])) plt.show() . Understanding one-hot labels . Notice the labels are in one-hot format. Let&#39;s add some code to display the class names. . print(train_data_gen.class_indices) . class_names = {v:k for k,v in train_data_gen.class_indices.items()} . plt.figure(figsize=(10,10)) for i in range(25): plt.subplot(5,5,i+1) plt.xticks([]) plt.yticks([]) plt.grid(False) plt.imshow(image_batch[i]) plt.xlabel(class_names[tf.argmax(labels_batch[i]).numpy()]) plt.show() . Read the validation images . # Above, you created a ImageDataGenerator for the training set # Next, create one to read the validation images # For example: # validation_image_generator = ImageDataGenerator ... # val_data_gen = validation_image_generator.flow_from_directory ... . Create a CNN . Now, it&#39;s time to define your model. You can create a similar model to the CNN used in the tutorial above. . The only difference is that the final Dense layer of your model (which classifies the data based on the features provided by the convolutional base) must use softmax activation and have five output classes: . model.add(Dense(5, activation=&#39;softmax&#39;)) . This is because we now have five different types of flowers, instead of just cats and dogs. . # TODO: your code here # Define a CNN using code similar to the above # For example # model = Sequential() # model.add ... # ... # The last line of your model should be: # model.add(Dense(5, activation=&#39;softmax&#39;)) . After you have defined your model, compile it by uncommenting and running this code. Important: notice that the loss has changed to categorical_crossentropy. This is necessary because the labels are in one-hot format. Finally, although these loss functions sound complicated, there are only a handful for you to learn. . #model.compile(optimizer=&#39;adam&#39;, # loss=&#39;categorical_crossentropy&#39;, # metrics=[&#39;accuracy&#39;]) . Now train your model for 10 epochs using model.fit. If you like, you can try to create plots of the training and validation accuracy and loss. . # TODO: your code here # For example # model.fit ... . If all has gone well, your model should be about 90% accurate on the training data. . Solution . # Read the validation images validation_image_generator = ImageDataGenerator(rescale=1./255) val_data_gen = validation_image_generator.flow_from_directory(batch_size=32, directory=val_dir, shuffle=True, target_size=(IMG_HEIGHT, IMG_WIDTH), class_mode=&#39;categorical&#39;) . # Define a model model = Sequential() model.add(Conv2D(32, (3, 3), activation=&#39;relu&#39;, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))) model.add(MaxPooling2D()) model.add(Conv2D(32, (3, 3), activation=&#39;relu&#39;)) model.add(MaxPooling2D()) model.add(Conv2D(32, (3, 3), activation=&#39;relu&#39;)) model.add(MaxPooling2D()) model.add(Flatten()) model.add(Dense(128, activation=&#39;relu&#39;)) model.add(Dense(5, activation=&#39;softmax&#39;)) . # Train the model history = model.fit( train_data_gen, epochs=10, validation_data=val_data_gen, ) . An advanced example: DeepDream . If time remains, in this tutorial your instructor will walk you through a minimal version of DeepDream, an experiment to visualize some of the features a convolutional neural network has learned to detect. DeepDream is an advanced tutorial, and our goal is to introduce you to some of the fascinating (and unexpected) things you can explore with Deep Learning. . Normally, when training a model we use gradient descent to minimize classification loss. In a CNN, this means we adjust the weights in the filters. In DeepDream, we start with a large, pretrained CNN (and leave the filters fixed!) We then use gradient descent to modify the input image to increasingly activate the filters. For example, if there is a filter that recognizes a certain kind of texture, we can progressively modify the image to contain more and more examples of that texture. . import numpy as np from IPython.display import clear_output . Download and display an image . url = &#39;https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg&#39; . def download(url, target_size=None): name = url.split(&#39;/&#39;)[-1] image_path = tf.keras.utils.get_file(name, origin=url) return tf.keras.preprocessing.image.load_img(image_path, target_size) def show(img): plt.figure(figsize=(8,8)) plt.grid(False) plt.axis(&#39;off&#39;) plt.imshow(img) plt.show() original_img = download(url, target_size=[225, 375]) original_img = np.array(original_img) show(original_img) . Rescale the pixel values . def preprocess(img): &quot;&quot;&quot; Convert RGB values from [0, 255] to [-1, 1] &quot;&quot;&quot; img = tf.cast(img, tf.float32) img /= 128.0 img -= 1. return img def unprocess(img): &quot;&quot;&quot; Undo the preprocessing above &quot;&quot;&quot; img = 255 * (img + 1.0) / 2.0 return tf.cast(img, tf.uint8) . Import a large, pretrained CNN . This model has been trained on ImageNet, a dataset with about 1M images in about 1K classes . conv_base = tf.keras.applications.InceptionV3(weights=&#39;imagenet&#39;, include_top=False) . Choose layers to activate . Normally, when you train a neural network, you use gradient descent to adjust the weights to minimize loss, in order to accurately classify images. In DeepDream, the trick is to use gradient descent to adjust the image, in order to increasingly activate certain layers from the network. You can explore different layers and see how this affects the results. You can find all the layer names using model.summary(). . names = [&#39;mixed2&#39;, &#39;mixed3&#39;, &#39;mixed4&#39;, &#39;mixed5&#39;] layers = [conv_base.get_layer(name).output for name in names] model = tf.keras.Model(inputs=conv_base.input, outputs=layers) . Custom loss function . Normally, we would use cross-entropy loss (for classification), or mean squared error (for regression). Here, we&#39;ll write a loss function that describes how activated our layers were by the image. . def calc_loss(img): img_batch = tf.expand_dims(img, axis=0) layer_activations = model(img_batch) losses = [tf.math.reduce_mean(act) for act in layer_activations] return tf.reduce_sum(losses) . Use gradient ascent to progressively activate the layers . Normally, when training a model you use gradient descent to adjust the weights to reduce the loss. In DeepDream, you will use gradient ascent to maximize the activation of the layers you selected by modifying the image, while leaving the weights of the network fixed. . @tf.function def step(img, lr=0.001): with tf.GradientTape() as tape: loss = calc_loss(img) gradients = tape.gradient(loss, img) gradients /= tf.math.reduce_std(gradients) + 1e-8 # Because the gradients are in the same shape # as the image, we can directly add them to it! img.assign_add(gradients * lr) img.assign(tf.clip_by_value(img, -1, 1)) . img = tf.Variable(preprocess(original_img)) steps = 1000 for i in range(steps): step(img) if i % 200 == 0: clear_output(wait=True) print (&quot;Step {}&quot;.format(i)) show(unprocess(img.numpy())) clear_output(wait=True) show(unprocess(img.numpy())) . You can find a complete example on the website (which includes additional code to generate less noisy images), and you may also be interested in exploring a related technique Neural Style Transfer. . Stay in touch . Twitter https://twitter.com/tensorflow | Blog http://blog.tensorflow.org/ | YouTube https://www.youtube.com/tensorflow. | . Thank you! . Learning more . Book recommendations . Deep Learning with Python | Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow | .",
            "url": "https://rafaelsf80.github.io/notebooks/jupyter/2020/06/22/cat-dogs.html",
            "relUrl": "/jupyter/2020/06/22/cat-dogs.html",
            "date": " • Jun 22, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://rafaelsf80.github.io/notebooks/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://rafaelsf80.github.io/notebooks/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://rafaelsf80.github.io/notebooks/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://rafaelsf80.github.io/notebooks/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}