{
  
    
        "post0": {
            "title": "kNN classifier with CIFAR-10",
            "content": "1. Introducci&#243;n . Este ejemplo muestra cómo hacer una clasificación de imágenes sin usar una red convolucional. Se va a usar k Nearest Neighbours y una precisión básica de diferencia de pixels. . 2. Setup . Importamos las librerías: . import matplotlib.pyplot as plt import numpy as np import tensorflow as tf from tensorflow.keras import datasets, layers, models . 3. Carga de datos . Cargamos el dataset desde tensorflow.keras.datasets . (train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data() # Normalizamos valores de píxeles entre 0 y 1 train_images, test_images = train_images / 255.0, test_images / 255. . Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz 170500096/170498071 [==============================] - 21s 0us/step . Visualizaci&#243;n . Visualizamos los primeros 25 elementos del dataset . train_labels.shape . (50000, 1) . class_names = [&#39;airplane&#39;, &#39;automobile&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;, &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;] plt.figure(figsize=(10,10)) for i in range(25): plt.subplot(5,5,i+1) plt.xticks([]) plt.yticks([]) plt.grid(False) plt.imshow(train_images[i], cmap=plt.cm.binary) # The CIFAR labels happen to be arrays, # which is why you need the extra index plt.xlabel(class_names[train_labels[i][0]]) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2020-09-19T11:18:17.598068 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ 3. Preparar datos . El dataset de entrenamiento es de tamaño train_images.shape=(50000,32,32,3) y sus etiquetas train_labels.shape=(50000, 1). Aplanamos: . train_images_rows = train_images.reshape(train_images.shape[0], 32 * 32 * 3) # Xtr_rows becomes 50000 x 3072 test_images_rows = test_images.reshape(test_images.shape[0], 32 * 32 * 3) # Xte_rows becomes 10000 x 3072 . 4. Construimos el modelo . class NearestNeighbor(object): def __init__(self): pass def train(self, X, y): &quot;&quot;&quot; X is N x D where each row is an example. Y is 1-dimension of size N &quot;&quot;&quot; # the nearest neighbor classifier simply remembers all the training data self.Xtr = X self.ytr = y def predict(self, X): &quot;&quot;&quot; X is N x D where each row is an example we wish to predict label for &quot;&quot;&quot; num_test = X.shape[0] # lets make sure that the output type matches the input type Ypred = np.zeros(num_test, dtype = self.ytr.dtype) # loop over all test rows for i in range(num_test): # find the nearest training image to the i&#39;th test image # using the L1 distance (sum of absolute value differences) distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = 1) min_index = np.argmin(distances) # get the index with smallest distance Ypred[i] = self.ytr[min_index] # predict the label of the nearest example return Ypred . . 4. Entrenamiento . Lanzamos el entrenamiento usando k Nearest Neighbours (kNN) . nn = NearestNeighbor() # create a Nearest Neighbor classifier class nn.train(train_images_rows, train_labels) # train the classifier on the training images and labels Yte_predict = nn.predict(test_images_rows) # predict labels on the test images # and now print the classification accuracy, which is the average number # of examples that are correctly predicted (i.e. label matches) print(&#39;accuracy: %f&#39; % ( np.mean(Yte_predict == test_labels) )) . NameError Traceback (most recent call last) &lt;ipython-input-19-318c5ce4292a&gt; in &lt;module&gt; 1 nn = NearestNeighbor() # create a Nearest Neighbor classifier class -&gt; 2 nn.train(train_images_rows, train_labels) # train the classifier on the training images and labels 3 Yte_predict = nn.predict(test_images_rows) # predict labels on the test images 4 # and now print the classification accuracy, which is the average number 5 # of examples that are correctly predicted (i.e. label matches) NameError: name &#39;train_images_rows&#39; is not defined . 5. Evaluaci&#243;n . La evaluación se puede hacer con la distancia L1 ó L2. . gs://BUCKET_NAME/FOLDER_NAME1/FOLDER_NAME2/.../FILE_NAME. Alternatively you can create a BigQuery table and upload the data into the table. The URI for your table is bq://PROJECT_ID.DATASET_ID.TABLE_ID. . distances = np.sqrt(np.sum(np.square(self.Xtr - X[i,:]), axis = 1)) .",
            "url": "https://rafaelsf80.github.io/notebooks/computer%20vision/2020/09/06/cifar-knn.html",
            "relUrl": "/computer%20vision/2020/09/06/cifar-knn.html",
            "date": " • Sep 6, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "AutoML tables",
            "content": "1. Project set up . Follow the AutoML Tables documentation to . Create a Google Cloud Platform (GCP) project. | Enable billing. | Apply to whitelist your project. | Enable AutoML API. | Enable AutoML Talbes API. | Create a service account, grant required permissions, and download the service account private key. | . You also need to upload your data into Google Cloud Storage (GCS) or BigQuery. For example, to use GCS as your data source . Create a GCS bucket. | Upload the training and batch prediction files. | . Warning: Private keys must be kept secret. If you expose your private key it is recommended to revoke it immediately from the Google Cloud Console. . . 2. Initialize and authenticate . This section runs intialization and authentication. It creates an authenticated session which is required for running any of the following sections. . Install the client library . Run the following cell. Click on the &#39;Choose Files&#39; button and select the client library compressed file. The file is uploaded to your Colab and installed using pip. . #@title Install AutoML Tables client library { vertical-output: true } from __future__ import absolute_import from __future__ import division from __future__ import print_function from google.colab import files import tarfile # Upload the client library compressed_file_upload = files.upload() compressed_file_name = list(compressed_file_upload.keys())[0] # Decompress the client library with tarfile.open(compressed_file_name) as tar: tar.extractall(path=&#39;.&#39;) # Install the client library !pip install ./python/automl-v1beta1 . Authenticate using service account key . Run the following cell. Click on the &#39;Choose Files&#39; button and select the service account private key file. If your Service Account key file or folder is hidden, you can reveal it in a Mac by pressing the Command + Shift + . combo. . #@title Authenticate using service account key and create a client. { vertical-output: true } from google.cloud import automl_v1beta1 # Upload service account key keyfile_upload = files.upload() keyfile_name = list(keyfile_upload.keys())[0] # Authenticate and create an AutoML client. client = automl_v1beta1.AutoMlClient.from_service_account_file(keyfile_name) # Authenticate and create a prediction service client. prediction_client = automl_v1beta1.PredictionServiceClient.from_service_account_file(keyfile_name) . Test . Enter your GCP project ID. . #@title GCP project ID and location project_id = &#39;&lt;PROJECT_ID&gt;&#39; #@param {type:&#39;string&#39;} location = &#39;us-central1&#39; location_path = client.location_path(project_id, location) location_path . To test whether your project set up and authentication steps were successful, run the following cell to list your datasets. . #@title List datasets. { vertical-output: true } list_datasets_response = client.list_datasets(location_path) datasets = {dataset.display_name: dataset.name for dataset in list_datasets_response} datasets . You can also print the list of your models by running the following cell. . #@title List models. { vertical-output: true } list_models_response = client.list_models(location_path) models = {model.display_name: model.name for model in list_models_response} models . . 3. Import training data . Create dataset . Select a dataset display name and pass your table source information to create a new dataset. . #@title Create dataset { vertical-output: true, output-height: 200 } dataset_display_name = &#39;iris_dataset&#39; #@param {type: &#39;string&#39;} create_dataset_response = client.create_dataset( location_path, {&#39;display_name&#39;: dataset_display_name, &#39;tables_dataset_metadata&#39;: {}}) dataset_name = create_dataset_response.name create_dataset_response . Import data . You can import your data to AutoML Tables from GCS or BigQuery. For this tutorial, you can use the iris dataset as your training data. You can create a GCS bucket and upload the data into your bucket. The URI for your file is gs://BUCKET_NAME/FOLDER_NAME1/FOLDER_NAME2/.../FILE_NAME. Alternatively you can create a BigQuery table and upload the data into the table. The URI for your table is bq://PROJECT_ID.DATASET_ID.TABLE_ID. . Importing data may take a few minutes or hours depending on the size of your data. If your Colab times out, run the following command to retrieve your dataset. Replace dataset_name with its actual value obtained in the preceding cells. . dataset = client.get_dataset(dataset_name) . #@title ... if data source is GCS { vertical-output: true } dataset_gcs_input_uris = [&#39;gs://&lt;BUCKET_NAME&gt;/&lt;FILE_PATH&gt;&#39;,] #@param # Define input configuration. input_config = { &#39;gcs_source&#39;: { &#39;input_uris&#39;: dataset_gcs_input_uris } } . #@title ... if data source is BigQuery { vertical-output: true } dataset_bq_input_uri = &#39;bq://&lt;PROJECT_ID&gt;.&lt;DATASET_NAME&gt;.&lt;TABLE_NAME&gt;&#39; #@param {type: &#39;string&#39;} # Define input configuration. input_config = { &#39;bigquery_source&#39;: { &#39;input_uri&#39;: dataset_bq_input_uri } } . #@title Import data { vertical-output: true } import_data_response = client.import_data(dataset_name, input_config) print(&#39;Dataset import operation: {}&#39;.format(import_data_response.operation)) # Wait until import is done. import_data_result = import_data_response.result() import_data_result . Review the specs . Run the following command to see table specs such as row count. . #@title Table schema { vertical-output: true } import google.cloud.automl_v1beta1.proto.data_types_pb2 as data_types import matplotlib.pyplot as plt # List table specs list_table_specs_response = client.list_table_specs(dataset_name) table_specs = [s for s in list_table_specs_response] # List column specs table_spec_name = table_specs[0].name list_column_specs_response = client.list_column_specs(table_spec_name) column_specs = {s.display_name: s for s in list_column_specs_response} # Table schema pie chart. type_counts = {} for column_spec in column_specs.values(): type_name = data_types.TypeCode.Name(column_spec.data_type.type_code) type_counts[type_name] = type_counts.get(type_name, 0) + 1 plt.pie(x=type_counts.values(), labels=type_counts.keys(), autopct=&#39;%1.1f%%&#39;) plt.axis(&#39;equal&#39;) plt.show() . Run the following command to see column specs such inferred schema. . . 4. Update dataset: assign a label column and enable nullable columns . AutoML Tables automatically detects your data column type. For example, for the Iris dataset it detects species to be categorical and petal_length, petal_width, sepal_length, and sepal_width to be numerical. Depending on the type of your label column, AutoML Tables chooses to run a classification or regression model. If your label column contains only numerical values, but they represent categories, change your label column type to categorical by updating your schema. . Update a column: set to nullable . #@title Update dataset { vertical-output: true } update_column_spec_dict = { &#39;name&#39;: column_specs[&#39;sepal_length&#39;].name, &#39;data_type&#39;: { &#39;type_code&#39;: &#39;FLOAT64&#39;, &#39;nullable&#39;: True } } update_column_response = client.update_column_spec(update_column_spec_dict) update_column_response . Tip: You can use &#39;type_code&#39;: &#39;CATEGORY&#39; in the preceding update_column_spec_dict to convert the column data type from FLOAT64 toCATEGORY`. . Update dataset: assign a label . #@title Update dataset { vertical-output: true } label_column_name = &#39;species&#39; #@param {type: &#39;string&#39;} label_column_spec = column_specs[label_column_name] label_column_id = label_column_spec.name.rsplit(&#39;/&#39;, 1)[-1] print(&#39;Label column ID: {}&#39;.format(label_column_id)) # Define the values of the fields to be updated. update_dataset_dict = { &#39;name&#39;: dataset_name, &#39;tables_dataset_metadata&#39;: { &#39;target_column_spec_id&#39;: label_column_id } } update_dataset_response = client.update_dataset(update_dataset_dict) update_dataset_response . . 5. Creating a model . Train a model . Specify the duration of the training. For example, &#39;train_budget_milli_node_hours&#39;: 1000 runs the training for one hour. If your Colab times out, use client.list_models(location_path) to check whether your model has been created. Then use model name to continue to the next steps. Run the following command to retrieve your model. Replace model_name with its actual value. . model = client.get_model(model_name) . #@title Create model { vertical-output: true } model_display_name = &#39;iris_model&#39; #@param {type:&#39;string&#39;} model_dict = { &#39;display_name&#39;: model_display_name, &#39;dataset_id&#39;: dataset_name.rsplit(&#39;/&#39;, 1)[-1], &#39;tables_model_metadata&#39;: {&#39;train_budget_milli_node_hours&#39;: 1000} } create_model_response = client.create_model(location_path, model_dict) print(&#39;Dataset import operation: {}&#39;.format(create_model_response.operation)) # Wait until model training is done. create_model_result = create_model_response.result() model_name = create_model_result.name create_model_result . . 6. Make a prediction . There are two different prediction modes: online and batch. The following cell shows you how to make an online prediction. . #@title Make an online prediction { vertical-output: true } sepal_length = 5.8 #@param {type:&#39;slider&#39;, min:4, max:8, step:0.1} sepal_width = 3.1 #@param {type:&#39;slider&#39;, min:2, max:5, step:0.1} petal_length = 3.8 #@param {type:&#39;slider&#39;, min:1, max:7, step:0.1} petal_width = 1.2 #@param {type:&#39;slider&#39;, min:0, max:3, step:0.1} payload = { &#39;row&#39;: { &#39;values&#39;: [ {&#39;number_value&#39;: sepal_length}, {&#39;number_value&#39;: sepal_width}, {&#39;number_value&#39;: petal_length}, {&#39;number_value&#39;: petal_width} ] } } # Make a prediction. prediction_client.predict(model_name, payload) . . 7. Batch prediction . Initialize prediction . Your data source for batch prediction can be GCS or BigQuery. For this tutorial, you can use iris_batch_prediction_input.csv as input source. Create a GCS bucket and upload the file into your bucket. Some of the lines in the batch prediction input file are intentionally left missing some values. The AutoML Tables logs the errors in the errors.csv file. . NOTE: The client library has a bug. If the following cell returns a TypeError: Could not convert Any to BatchPredictResult error, ignore it. The batch prediction output file(s) will be updated to the GCS bucket that you set in the preceding cells. . #@title Start batch prediction { vertical-output: true, output-height: 200 } batch_predict_gcs_input_uris = [&#39;gs://automl-tables-data/iris_batch_prediction_input.csv&#39;,] #@param batch_predict_gcs_output_uri_prefix = &#39;gs://automl-tables-pred&#39; #@param {type:&#39;string&#39;} # Define input source. batch_prediction_input_source = { &#39;gcs_source&#39;: { &#39;input_uris&#39;: batch_predict_gcs_input_uris } } # Define output target. batch_prediction_output_target = { &#39;gcs_destination&#39;: { &#39;output_uri_prefix&#39;: batch_predict_gcs_output_uri_prefix } } batch_predict_response = prediction_client.batch_predict( model_name, batch_prediction_input_source, batch_prediction_output_target) print(&#39;Batch prediction operation: {}&#39;.format(batch_predict_response.operation)) # Wait until batch prediction is done. batch_predict_result = batch_predict_response.result() batch_predict_response.metadata . batch_predict_response.metadata .",
            "url": "https://rafaelsf80.github.io/notebooks/structured%20data/2020/09/05/automl-tables.html",
            "relUrl": "/structured%20data/2020/09/05/automl-tables.html",
            "date": " • Sep 5, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Extracción de campos de formularios con Google Document AI",
            "content": "Introducci&#243;n . Este ejemplo muestra cómo analizar un formulario (en formato pdf) y extraer ciertas entidades, como nombre, dirección, teléfono, .... como lista en python y como cajas con coordenadas X, Y para resaltarlos en el propio pdf. . Contenido original en inglés y adaptación de aqui . Instalaci&#243;n . !pip3 install google-cloud-documentai !pip3 install wand !pip3 install pillo #!apt-get update #!apt-get install poppler-utils # for converting pdf to jpg. We&#39;ll use this for displaying the pdf later #!apt-get install libmagickwand-dev . Collecting google-cloud-documentai Downloading google_cloud_documentai-0.2.0-py2.py3-none-any.whl (46 kB) |████████████████████████████████| 46 kB 5.2 MB/s Collecting google-api-core[grpc]&lt;2.0.0dev,&gt;=1.17.0 Downloading google_api_core-1.22.1-py2.py3-none-any.whl (91 kB) |████████████████████████████████| 91 kB 7.3 MB/s Collecting proto-plus&gt;=0.4.0 Downloading proto-plus-1.8.1.tar.gz (23 kB) Collecting google-auth&lt;2.0dev,&gt;=1.19.1 Downloading google_auth-1.21.0-py2.py3-none-any.whl (92 kB) |████████████████████████████████| 92 kB 3.4 MB/s Requirement already satisfied: six&gt;=1.10.0 in /Users/rafaelsanchez/Library/Python/3.7/lib/python/site-packages (from google-api-core[grpc]&lt;2.0.0dev,&gt;=1.17.0-&gt;google-cloud-documentai) (1.12.0) Requirement already satisfied: pytz in /usr/local/lib/python3.7/site-packages (from google-api-core[grpc]&lt;2.0.0dev,&gt;=1.17.0-&gt;google-cloud-documentai) (2019.3) Requirement already satisfied: setuptools&gt;=34.0.0 in /usr/local/lib/python3.7/site-packages (from google-api-core[grpc]&lt;2.0.0dev,&gt;=1.17.0-&gt;google-cloud-documentai) (46.0.0) Collecting protobuf&gt;=3.12.0 Downloading protobuf-3.13.0-cp37-cp37m-macosx_10_9_x86_64.whl (1.3 MB) |████████████████████████████████| 1.3 MB 11.2 MB/s Requirement already satisfied: googleapis-common-protos&lt;2.0dev,&gt;=1.6.0 in /usr/local/lib/python3.7/site-packages (from google-api-core[grpc]&lt;2.0.0dev,&gt;=1.17.0-&gt;google-cloud-documentai) (1.51.0) Requirement already satisfied: requests&lt;3.0.0dev,&gt;=2.18.0 in /Users/rafaelsanchez/Library/Python/3.7/lib/python/site-packages (from google-api-core[grpc]&lt;2.0.0dev,&gt;=1.17.0-&gt;google-cloud-documentai) (2.22.0) Collecting grpcio&lt;2.0dev,&gt;=1.29.0; extra == &#34;grpc&#34; Using cached grpcio-1.31.0-cp37-cp37m-macosx_10_9_x86_64.whl (3.0 MB) Requirement already satisfied: cachetools&lt;5.0,&gt;=2.0.0 in /usr/local/lib/python3.7/site-packages (from google-auth&lt;2.0dev,&gt;=1.19.1-&gt;google-api-core[grpc]&lt;2.0.0dev,&gt;=1.17.0-&gt;google-cloud-documentai) (4.0.0) Requirement already satisfied: rsa&lt;5,&gt;=3.1.4; python_version &gt;= &#34;3.5&#34; in /usr/local/lib/python3.7/site-packages (from google-auth&lt;2.0dev,&gt;=1.19.1-&gt;google-api-core[grpc]&lt;2.0.0dev,&gt;=1.17.0-&gt;google-cloud-documentai) (4.0) Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /usr/local/lib/python3.7/site-packages (from google-auth&lt;2.0dev,&gt;=1.19.1-&gt;google-api-core[grpc]&lt;2.0.0dev,&gt;=1.17.0-&gt;google-cloud-documentai) (0.2.8) Requirement already satisfied: certifi&gt;=2017.4.17 in /Users/rafaelsanchez/Library/Python/3.7/lib/python/site-packages (from requests&lt;3.0.0dev,&gt;=2.18.0-&gt;google-api-core[grpc]&lt;2.0.0dev,&gt;=1.17.0-&gt;google-cloud-documentai) (2019.9.11) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /Users/rafaelsanchez/Library/Python/3.7/lib/python/site-packages (from requests&lt;3.0.0dev,&gt;=2.18.0-&gt;google-api-core[grpc]&lt;2.0.0dev,&gt;=1.17.0-&gt;google-cloud-documentai) (1.24.3) Requirement already satisfied: idna&lt;2.9,&gt;=2.5 in /Users/rafaelsanchez/Library/Python/3.7/lib/python/site-packages (from requests&lt;3.0.0dev,&gt;=2.18.0-&gt;google-api-core[grpc]&lt;2.0.0dev,&gt;=1.17.0-&gt;google-cloud-documentai) (2.8) Requirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in /Users/rafaelsanchez/Library/Python/3.7/lib/python/site-packages (from requests&lt;3.0.0dev,&gt;=2.18.0-&gt;google-api-core[grpc]&lt;2.0.0dev,&gt;=1.17.0-&gt;google-cloud-documentai) (3.0.4) Requirement already satisfied: pyasn1&gt;=0.1.3 in /usr/local/lib/python3.7/site-packages (from rsa&lt;5,&gt;=3.1.4; python_version &gt;= &#34;3.5&#34;-&gt;google-auth&lt;2.0dev,&gt;=1.19.1-&gt;google-api-core[grpc]&lt;2.0.0dev,&gt;=1.17.0-&gt;google-cloud-documentai) (0.4.8) Building wheels for collected packages: proto-plus Building wheel for proto-plus (setup.py) ... done Created wheel for proto-plus: filename=proto_plus-1.8.1-py3-none-any.whl size=35874 sha256=862bcf025c31212792b26c2e8306a61f0357ba8e64c5efc76a45b93cb356ad8b Stored in directory: /Users/rafaelsanchez/Library/Caches/pip/wheels/4f/2d/91/ed8cbdb4921f9c8af5b22353d213a700d9e1e43c9fae3aa92a Successfully built proto-plus ERROR: tensorflow 2.1.0 has requirement gast==0.2.2, but you&#39;ll have gast 0.3.3 which is incompatible. Installing collected packages: google-auth, protobuf, grpcio, google-api-core, proto-plus, google-cloud-documentai Attempting uninstall: google-auth Found existing installation: google-auth 1.13.1 Uninstalling google-auth-1.13.1: Successfully uninstalled google-auth-1.13.1 Attempting uninstall: protobuf Found existing installation: protobuf 3.11.3 Uninstalling protobuf-3.11.3: Successfully uninstalled protobuf-3.11.3 Attempting uninstall: grpcio Found existing installation: grpcio 1.28.1 Uninstalling grpcio-1.28.1: Successfully uninstalled grpcio-1.28.1 Attempting uninstall: google-api-core Found existing installation: google-api-core 1.16.0 Uninstalling google-api-core-1.16.0: Successfully uninstalled google-api-core-1.16.0 ERROR: Could not install packages due to an EnvironmentError: [Errno 13] Permission denied: &#39;/usr/local/bin/fixup_keywords.py&#39; Consider using the `--user` option or check the permissions. Collecting wand Downloading Wand-0.6.2-py2.py3-none-any.whl (130 kB) |████████████████████████████████| 130 kB 7.0 MB/s Installing collected packages: wand Successfully installed wand-0.6.2 ERROR: Could not find a version that satisfies the requirement pillo (from versions: none) ERROR: No matching distribution found for pillo . Now you&#39;ll need to restart your notebook to load the new libraries before you can continue . from google.cloud import documentai_v1beta2 as documentai from wand.image import Image as WImage from PIL import Image, ImageDraw import os . OSError Traceback (most recent call last) /usr/local/lib/python3.7/site-packages/wand/api.py in &lt;module&gt; 150 try: --&gt; 151 libraries = load_library() 152 except (OSError, IOError): /usr/local/lib/python3.7/site-packages/wand/api.py in load_library() 139 return libwand, libmagick --&gt; 140 raise IOError(&#39;cannot find library; tried paths: &#39; + repr(tried_paths)) 141 OSError: cannot find library; tried paths: [] During handling of the above exception, another exception occurred: ImportError Traceback (most recent call last) &lt;ipython-input-4-d2a75236ebad&gt; in &lt;module&gt; 1 from google.cloud import documentai_v1beta2 as documentai -&gt; 2 from wand.image import Image as WImage 3 from PIL import Image, ImageDraw 4 import os /usr/local/lib/python3.7/site-packages/wand/image.py in &lt;module&gt; 16 import weakref 17 &gt; 18 from . import assertions 19 from .api import libc, libmagick, library 20 from .color import Color /usr/local/lib/python3.7/site-packages/wand/assertions.py in &lt;module&gt; 153 154 # Lazy load recursive import --&gt; 155 from .color import Color # noqa: E402 /usr/local/lib/python3.7/site-packages/wand/color.py in &lt;module&gt; 8 import numbers 9 &gt; 10 from .api import library 11 from .cdefs.structures import MagickPixelPacket, PixelInfo 12 from .compat import binary, text /usr/local/lib/python3.7/site-packages/wand/api.py in &lt;module&gt; 175 raise ImportError(&#39;MagickWand shared library not found. n&#39; 176 &#39;You probably had not installed ImageMagick library. n&#39; --&gt; 177 &#39;Try to install: n &#39; + msg) 178 179 #: (:class:`ctypes.CDLL`) The MagickWand library. ImportError: MagickWand shared library not found. You probably had not installed ImageMagick library. Try to install: brew install freetype imagemagick . Configurar proyecto Google Cloud . # En caso de usar Colab, ejecutar lo siguiente from google.colab import auth auth.authenticate_user() . #@title Set Project Id PROJECT_ID = &#39;lending-ai&#39; #@param {type: &quot;string&quot;} PDF_URI = &quot;gs://cloud-samples-data/documentai/form.pdf&quot; #@param {type: &quot;string&quot;} SERVICE_ACCOUNT_NAME=&quot;dai-5729jdfkasf&quot; #@param {type: &quot;string&quot;} . Crear cuenta de servicio y descargar la clave: . !gcloud config set project &#39;{PROJECT_ID}&#39; # Uncomment the following line to create a new service account #!gcloud iam service-accounts create {SERVICE_ACCOUNT_NAME} !gcloud iam service-accounts keys create ./key.json --iam-account {SERVICE_ACCOUNT_NAME}@{PROJECT_ID}.iam.gserviceaccount.com os.environ[&quot;GOOGLE_APPLICATION_CREDENTIALS&quot;] = &quot;./key.json&quot; . Updated property [core/project]. created key [0e4877587f9cd4a19ddbdf48b31cfa3f85b5c336] of type [json] as [./key.json] for [dai-5729jdfkasf@lending-ai.iam.gserviceaccount.com] . Habilitar dos APIs: la de Document AI y la de Invoice AI . !gcloud services enable documentai.googleapis.com !gcloud services enable invoice.googleapis.com . # Descargar el archivo !gsutil cp $PDF_URI ./doc.pdf . CommandException: Wrong number of arguments for &#34;cp&#34; command. . Llamada al Document AI API . def parse_form(project_id=PROJECT_ID, input_uri=PDF_URI): &quot;&quot;&quot;Parse a form using the Document AI API&quot;&quot;&quot; # Create a new Document AI client client = documentai.DocumentUnderstandingServiceClient() # Specify which cloud in GCS you&#39;d like to analyze gcs_source = documentai.types.GcsSource(uri=input_uri) # mime_type can be application/pdf, image/tiff, # and image/gif, or application/json input_config = documentai.types.InputConfig( gcs_source=gcs_source, mime_type=&#39;application/pdf&#39;) # Optional: Improve form parsing results by providing # key-value pair hints. # For each key hint, key is text that is likely to appear in the # document as a form field name (i.e. &quot;DOB&quot;). # Value types are optional, but can be one or more of: # ADDRESS, LOCATION, ORGANIZATION, PERSON, PHONE_NUMBER, ID, # NUMBER, EMAIL, PRICE, TERMS, DATE, NAME key_value_pair_hints = [ documentai.types.KeyValuePairHint(key=&#39;Emergency Contact&#39;, value_types=[&#39;NAME&#39;]), documentai.types.KeyValuePairHint( key=&#39;Referred By&#39;) ] # Setting enabled=True enables form extraction form_extraction_params = documentai.types.FormExtractionParams( enabled=True, key_value_pair_hints=key_value_pair_hints) # Location can be &#39;us&#39; or &#39;eu&#39; parent = &#39;projects/{}/locations/us&#39;.format(project_id) request = documentai.types.ProcessDocumentRequest( parent=parent, input_config=input_config, form_extraction_params=form_extraction_params) document = client.process_document(request=request) return document . doc = parse_form(PROJECT_ID) . Respuesta del API: . # This document consists of one page len(doc.pages) . 1 . # It&#39;s got 17 form fields len(doc.pages[0].form_fields) . 17 . # This is what a form field looks like doc.pages[0].form_fields[0] . field_name { text_anchor { text_segments { start_index: 325 end_index: 327 } } confidence: 0.9999823570251465 bounding_poly { normalized_vertices { x: 0.5433105230331421 y: 0.25519150495529175 } normalized_vertices { x: 0.6368563771247864 y: 0.25519150495529175 } normalized_vertices { x: 0.6368563771247864 y: 0.2925703823566437 } normalized_vertices { x: 0.5433105230331421 y: 0.2925703823566437 } } orientation: 1 } field_value { text_anchor { text_segments { start_index: 327 end_index: 343 } } confidence: 0.9999823570251465 bounding_poly { normalized_vertices { x: 0.6395664215087891 y: 0.25380709767341614 } normalized_vertices { x: 0.8802167773246765 y: 0.25380709767341614 } normalized_vertices { x: 0.8802167773246765 y: 0.2925703823566437 } normalized_vertices { x: 0.6395664215087891 y: 0.2925703823566437 } } orientation: 1 } . Funci&#243;n para parsear la respuesta del API . def get_text(document, el): &quot;&quot;&quot;Doc AI identifies form fields by their offsets in document text. This function converts offsets to text snippets. Parameters: doc (documentai.proto): Proto returned from docai api el (documentai.entity): Single entity from the doc Returns: array of {&quot;x&quot;: float, &quot;y&quot;: float} bounding box of the entity &quot;&quot;&quot; response = &#39;&#39; # If a text segment spans several lines, it will # be stored in different text segments. for segment in el.text_anchor.text_segments: start_index = segment.start_index end_index = segment.end_index response += document.text[start_index:end_index] return response . # Form fields are given as character offsets in text: # text_anchor { # text_segments { # start_index: 325 # end_index: 327 # } # } # To convert text offsets to actual words, we&#39;ll use the helper function get_text for form_field in doc.pages[0].form_fields: field_name = get_text(doc, form_field.field_name).strip() field_value = get_text(doc, form_field.field_value).strip() print(f&quot;{field_name} t{field_value}&quot;) . #: _(906) 917-3486 Emergency Contact: Eva Walker Marital Status: Single Gender: _F Occupation: Software Engineer Referred By: None Date: 9/14/19 DOB: 09/04/1986 Address: 24 Barney Lane City: Tonaco Name: Sally Walker State: NJ Email: Sally, waller@cmail.com_Phone Zip: 07082 Emergency Contact Phone: (906)334-89766 Are you currently taking any medication? (If yes, please describe) you currently taking any medication? (If yes, please describe): Vyvanse (25mg) daily for attention Describe your medical concerns (symptoms, diagnoses, etc): Ranny nose, mucas in thoat, weakness, aches, chills, tired . Impresi&#243;n de resultados . Descarga del archivo y conversi&#243;n a jpg . This way, we can draw on it here in this Jupyter Notebook . !pdfimages -j doc.pdf doc . Imprimir entidades y dibujar las cajas . im = Image.open(&#39;doc-000.jpg&#39;) draw = ImageDraw.Draw(im) for form_field in doc.pages[0].form_fields: # Draw the bounding box around the form_fields # Forst get the co-ords of the field name vertices = [] for vertex in form_field.field_name.bounding_poly.normalized_vertices: vertices.append({&#39;x&#39;: vertex.x * im.size[0], &#39;y&#39;: vertex.y * im.size[1]}) draw.polygon([ vertices[0][&#39;x&#39;], vertices[0][&#39;y&#39;], vertices[1][&#39;x&#39;], vertices[1][&#39;y&#39;], vertices[2][&#39;x&#39;], vertices[2][&#39;y&#39;], vertices[3][&#39;x&#39;], vertices[3][&#39;y&#39;]], outline=&#39;red&#39;) vertices = [] for vertex in form_field.field_value.bounding_poly.normalized_vertices: vertices.append({&#39;x&#39;: vertex.x * im.size[0], &#39;y&#39;: vertex.y * im.size[1]}) draw.polygon([ vertices[0][&#39;x&#39;], vertices[0][&#39;y&#39;], vertices[1][&#39;x&#39;], vertices[1][&#39;y&#39;], vertices[2][&#39;x&#39;], vertices[2][&#39;y&#39;], vertices[3][&#39;x&#39;], vertices[3][&#39;y&#39;]], outline=&#39;blue&#39;) . im .",
            "url": "https://rafaelsf80.github.io/notebooks/structured%20data/google%20cloud/2020/09/01/bounding-box-dai.html",
            "relUrl": "/structured%20data/google%20cloud/2020/09/01/bounding-box-dai.html",
            "date": " • Sep 1, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Series temporales: temperaturas en Melbourne",
            "content": "Introducci&#243;n . Se analizará la serie temporal de temperaturas mínimas en Melbourne con el dataset disponible en MachineLearningMastery.com de Jason Brownlee. . import tensorflow as tf print(tf.__version__) . import numpy as np import matplotlib.pyplot as plt def plot_series(time, series, format=&quot;-&quot;, start=0, end=None): plt.plot(time[start:end], series[start:end], format) plt.xlabel(&quot;Time&quot;) plt.ylabel(&quot;Value&quot;) plt.grid(True) . !wget --no-check-certificate https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv -O /tmp/daily-min-temperatures.csv . import csv time_step = [] temps = [] with open(&#39;/tmp/daily-min-temperatures.csv&#39;) as csvfile: time_steps= series = np.array(temps) time = np.array(time_step) plt.figure(figsize=(10, 6)) plot_series(time, series) . split_time = 2500 time_train = # YOUR CODE HERE x_train = # YOUR CODE HERE time_valid = # YOUR CODE HERE x_valid = # YOUR CODE HERE window_size = 30 batch_size = 32 shuffle_buffer_size = 1000 . def windowed_dataset(series, window_size, batch_size, shuffle_buffer): # YOUR CODE HERE . def model_forecast(model, series, window_size): # YOUR CODE HERE . tf.keras.backend.clear_session() tf.random.set_seed(51) np.random.seed(51) window_size = 64 batch_size = 256 train_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size) print(train_set) print(x_train.shape) model = tf.keras.models.Sequential([ # YOUR CODE HERE ]) lr_schedule = tf.keras.callbacks.LearningRateScheduler( lambda epoch: 1e-8 * 10**(epoch / 20)) optimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9) model.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer, metrics=[&quot;mae&quot;]) history = model.fit(train_set, epochs=100, callbacks=[lr_schedule]) . WARNING: Logging before flag parsing goes to stderr. W0719 05:10:05.389573 140234944071552 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/util/random_seed.py:58: add_dispatch_support.&lt;locals&gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where . &lt;PrefetchDataset shapes: ((None, None, 1), (None, None, 1)), types: (tf.float64, tf.float64)&gt; (2500,) Epoch 1/100 10/10 [==============================] - 6s 624ms/step - loss: 31.1549 - mae: 31.6551 Epoch 2/100 10/10 [==============================] - 4s 364ms/step - loss: 30.5696 - mae: 31.0771 Epoch 3/100 10/10 [==============================] - 4s 358ms/step - loss: 29.6691 - mae: 30.1811 Epoch 4/100 10/10 [==============================] - 4s 366ms/step - loss: 28.5431 - mae: 29.0596 Epoch 5/100 10/10 [==============================] - 4s 361ms/step - loss: 27.1744 - mae: 27.6976 Epoch 6/100 10/10 [==============================] - 4s 367ms/step - loss: 25.4676 - mae: 26.0015 Epoch 7/100 10/10 [==============================] - 4s 375ms/step - loss: 23.2987 - mae: 23.8487 Epoch 8/100 10/10 [==============================] - 4s 369ms/step - loss: 20.5506 - mae: 21.1192 Epoch 9/100 10/10 [==============================] - 4s 372ms/step - loss: 17.2408 - mae: 17.8223 Epoch 10/100 10/10 [==============================] - 4s 365ms/step - loss: 13.5549 - mae: 14.1350 Epoch 11/100 10/10 [==============================] - 4s 370ms/step - loss: 10.0753 - mae: 10.6367 Epoch 12/100 10/10 [==============================] - 4s 365ms/step - loss: 7.5638 - mae: 8.0985 Epoch 13/100 10/10 [==============================] - 4s 363ms/step - loss: 6.2445 - mae: 6.7619 Epoch 14/100 10/10 [==============================] - 4s 374ms/step - loss: 5.6749 - mae: 6.1854 Epoch 15/100 10/10 [==============================] - 4s 363ms/step - loss: 5.3090 - mae: 5.8168 Epoch 16/100 10/10 [==============================] - 4s 361ms/step - loss: 4.9122 - mae: 5.4169 Epoch 17/100 10/10 [==============================] - 4s 356ms/step - loss: 4.5318 - mae: 5.0305 Epoch 18/100 10/10 [==============================] - 4s 367ms/step - loss: 4.2115 - mae: 4.7068 Epoch 19/100 10/10 [==============================] - 4s 374ms/step - loss: 3.9429 - mae: 4.4360 Epoch 20/100 10/10 [==============================] - 4s 358ms/step - loss: 3.7309 - mae: 4.2194 Epoch 21/100 10/10 [==============================] - 4s 363ms/step - loss: 3.5706 - mae: 4.0551 Epoch 22/100 10/10 [==============================] - 4s 366ms/step - loss: 3.4527 - mae: 3.9344 Epoch 23/100 10/10 [==============================] - 4s 362ms/step - loss: 3.3617 - mae: 3.8423 Epoch 24/100 10/10 [==============================] - 4s 358ms/step - loss: 3.2876 - mae: 3.7666 Epoch 25/100 10/10 [==============================] - 3s 344ms/step - loss: 3.2224 - mae: 3.6997 Epoch 26/100 10/10 [==============================] - 4s 360ms/step - loss: 3.1596 - mae: 3.6359 Epoch 27/100 10/10 [==============================] - 4s 360ms/step - loss: 3.0964 - mae: 3.5717 Epoch 28/100 10/10 [==============================] - 4s 362ms/step - loss: 3.0322 - mae: 3.5064 Epoch 29/100 10/10 [==============================] - 4s 352ms/step - loss: 2.9662 - mae: 3.4392 Epoch 30/100 10/10 [==============================] - 4s 359ms/step - loss: 2.9004 - mae: 3.3720 Epoch 31/100 10/10 [==============================] - 4s 361ms/step - loss: 2.8376 - mae: 3.3081 Epoch 32/100 10/10 [==============================] - 4s 375ms/step - loss: 2.7775 - mae: 3.2475 Epoch 33/100 10/10 [==============================] - 4s 360ms/step - loss: 2.7202 - mae: 3.1899 Epoch 34/100 10/10 [==============================] - 4s 367ms/step - loss: 2.6662 - mae: 3.1360 Epoch 35/100 10/10 [==============================] - 4s 379ms/step - loss: 2.6152 - mae: 3.0848 Epoch 36/100 10/10 [==============================] - 4s 357ms/step - loss: 2.5663 - mae: 3.0353 Epoch 37/100 10/10 [==============================] - 4s 366ms/step - loss: 2.5192 - mae: 2.9872 Epoch 38/100 10/10 [==============================] - 4s 367ms/step - loss: 2.4735 - mae: 2.9408 Epoch 39/100 10/10 [==============================] - 4s 372ms/step - loss: 2.4296 - mae: 2.8964 Epoch 40/100 10/10 [==============================] - 4s 371ms/step - loss: 2.3873 - mae: 2.8534 Epoch 41/100 10/10 [==============================] - 4s 361ms/step - loss: 2.3463 - mae: 2.8119 Epoch 42/100 10/10 [==============================] - 4s 354ms/step - loss: 2.3060 - mae: 2.7707 Epoch 43/100 10/10 [==============================] - 4s 375ms/step - loss: 2.2663 - mae: 2.7301 Epoch 44/100 10/10 [==============================] - 4s 359ms/step - loss: 2.2269 - mae: 2.6899 Epoch 45/100 10/10 [==============================] - 4s 366ms/step - loss: 2.1898 - mae: 2.6519 Epoch 46/100 10/10 [==============================] - 4s 367ms/step - loss: 2.1563 - mae: 2.6181 Epoch 47/100 10/10 [==============================] - 4s 363ms/step - loss: 2.1248 - mae: 2.5861 Epoch 48/100 10/10 [==============================] - 4s 367ms/step - loss: 2.0958 - mae: 2.5568 Epoch 49/100 10/10 [==============================] - 4s 363ms/step - loss: 2.0688 - mae: 2.5296 Epoch 50/100 10/10 [==============================] - 4s 370ms/step - loss: 2.0442 - mae: 2.5045 Epoch 51/100 10/10 [==============================] - 4s 371ms/step - loss: 2.0220 - mae: 2.4818 Epoch 52/100 10/10 [==============================] - 4s 362ms/step - loss: 2.0018 - mae: 2.4611 Epoch 53/100 10/10 [==============================] - 4s 361ms/step - loss: 1.9801 - mae: 2.4393 Epoch 54/100 10/10 [==============================] - 4s 369ms/step - loss: 1.9586 - mae: 2.4171 Epoch 55/100 10/10 [==============================] - 4s 358ms/step - loss: 1.9390 - mae: 2.3972 Epoch 56/100 10/10 [==============================] - 4s 366ms/step - loss: 1.9186 - mae: 2.3763 Epoch 57/100 10/10 [==============================] - 4s 366ms/step - loss: 1.8975 - mae: 2.3550 Epoch 58/100 10/10 [==============================] - 4s 359ms/step - loss: 1.8743 - mae: 2.3320 Epoch 59/100 10/10 [==============================] - 4s 363ms/step - loss: 1.8738 - mae: 2.3310 Epoch 60/100 10/10 [==============================] - 4s 354ms/step - loss: 2.1527 - mae: 2.6201 Epoch 61/100 10/10 [==============================] - 4s 357ms/step - loss: 2.6764 - mae: 3.1247 Epoch 62/100 10/10 [==============================] - 4s 360ms/step - loss: 2.9935 - mae: 3.4806 Epoch 63/100 10/10 [==============================] - 4s 360ms/step - loss: 3.5219 - mae: 3.9875 Epoch 64/100 10/10 [==============================] - 4s 361ms/step - loss: 3.8284 - mae: 4.2965 Epoch 65/100 10/10 [==============================] - 4s 357ms/step - loss: 4.1265 - mae: 4.5856 Epoch 66/100 10/10 [==============================] - 4s 359ms/step - loss: 4.3062 - mae: 4.7664 Epoch 67/100 10/10 [==============================] - 4s 358ms/step - loss: 4.3039 - mae: 4.6838 Epoch 68/100 10/10 [==============================] - 4s 363ms/step - loss: 4.8341 - mae: 5.2257 Epoch 69/100 10/10 [==============================] - 4s 370ms/step - loss: 10.3352 - mae: 10.7649 Epoch 70/100 10/10 [==============================] - 4s 367ms/step - loss: 5.4020 - mae: 5.8328 Epoch 71/100 10/10 [==============================] - 4s 363ms/step - loss: 5.9798 - mae: 6.4835 Epoch 72/100 10/10 [==============================] - 4s 356ms/step - loss: 5.4958 - mae: 6.0146 Epoch 73/100 10/10 [==============================] - 4s 362ms/step - loss: 4.4955 - mae: 5.0201 Epoch 74/100 10/10 [==============================] - 4s 370ms/step - loss: 4.4764 - mae: 5.0171 Epoch 75/100 10/10 [==============================] - 4s 380ms/step - loss: 4.2825 - mae: 4.7287 Epoch 76/100 10/10 [==============================] - 4s 361ms/step - loss: 4.2044 - mae: 4.6470 Epoch 77/100 10/10 [==============================] - 4s 370ms/step - loss: 4.4160 - mae: 4.9125 Epoch 78/100 10/10 [==============================] - 4s 361ms/step - loss: 4.3770 - mae: 4.8831 Epoch 79/100 10/10 [==============================] - 4s 361ms/step - loss: 5.0487 - mae: 5.5839 Epoch 80/100 10/10 [==============================] - 4s 390ms/step - loss: 10.0358 - mae: 10.8604 Epoch 81/100 10/10 [==============================] - 4s 370ms/step - loss: 3.1176 - mae: 3.6060 Epoch 82/100 10/10 [==============================] - 4s 371ms/step - loss: 3.0097 - mae: 3.4891 Epoch 83/100 10/10 [==============================] - 4s 361ms/step - loss: 2.7912 - mae: 3.2609 Epoch 84/100 10/10 [==============================] - 4s 357ms/step - loss: 4.3135 - mae: 4.7803 Epoch 85/100 10/10 [==============================] - 4s 357ms/step - loss: 5.3703 - mae: 5.8508 Epoch 86/100 10/10 [==============================] - 4s 360ms/step - loss: 6.5221 - mae: 7.0175 Epoch 87/100 10/10 [==============================] - 4s 361ms/step - loss: 7.1154 - mae: 7.7249 Epoch 88/100 10/10 [==============================] - 4s 368ms/step - loss: 8.9975 - mae: 9.4580 Epoch 89/100 10/10 [==============================] - 4s 360ms/step - loss: 9.8069 - mae: 10.1397 Epoch 90/100 10/10 [==============================] - 4s 354ms/step - loss: 11.1364 - mae: 11.6797 Epoch 91/100 10/10 [==============================] - 4s 356ms/step - loss: 12.5922 - mae: 13.2602 Epoch 92/100 10/10 [==============================] - 4s 357ms/step - loss: 14.2512 - mae: 14.8289 Epoch 93/100 10/10 [==============================] - 4s 358ms/step - loss: 13.0192 - mae: 13.8809 Epoch 94/100 10/10 [==============================] - 4s 356ms/step - loss: 61.8923 - mae: 63.5737 Epoch 95/100 10/10 [==============================] - 4s 366ms/step - loss: 30.5821 - mae: 31.4688 Epoch 96/100 10/10 [==============================] - 4s 357ms/step - loss: 45.9889 - mae: 46.9740 Epoch 97/100 10/10 [==============================] - 4s 355ms/step - loss: 51.7050 - mae: 51.8100 Epoch 98/100 10/10 [==============================] - 4s 361ms/step - loss: 59.1678 - mae: 57.9463 Epoch 99/100 10/10 [==============================] - 4s 373ms/step - loss: 66.1686 - mae: 68.4903 Epoch 100/100 10/10 [==============================] - 4s 360ms/step - loss: 74.1415 - mae: 72.0624 . plt.semilogx(history.history[&quot;lr&quot;], history.history[&quot;loss&quot;]) plt.axis([1e-8, 1e-4, 0, 60]) . [1e-08, 0.0001, 0, 60] . tf.keras.backend.clear_session() tf.random.set_seed(51) np.random.seed(51) train_set = windowed_dataset(x_train, window_size=60, batch_size=100, shuffle_buffer=shuffle_buffer_size) model = tf.keras.models.Sequential([ # YOUR CODE HERE ]) optimizer = tf.keras.optimizers.SGD(lr=# YOUR CODE HERE, momentum=0.9) model.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer, metrics=[&quot;mae&quot;]) history = model.fit(train_set,epochs=# YOUR CODE HERE) # EXPECTED OUTPUT SHOULD SEE AN MAE OF &lt;2 WITHIN ABOUT 30 EPOCHS . Epoch 1/150 25/25 [==============================] - 6s 243ms/step - loss: 9.9624 - mae: 10.5789 Epoch 2/150 25/25 [==============================] - 3s 136ms/step - loss: 2.5390 - mae: 3.0130 Epoch 3/150 25/25 [==============================] - 3s 131ms/step - loss: 1.9265 - mae: 2.3815 Epoch 4/150 25/25 [==============================] - 3s 137ms/step - loss: 1.8597 - mae: 2.3125 Epoch 5/150 25/25 [==============================] - 3s 139ms/step - loss: 1.8181 - mae: 2.2696 Epoch 6/150 25/25 [==============================] - 3s 140ms/step - loss: 1.7882 - mae: 2.2385 Epoch 7/150 25/25 [==============================] - 4s 141ms/step - loss: 1.7618 - mae: 2.2112 Epoch 8/150 25/25 [==============================] - 3s 135ms/step - loss: 1.7382 - mae: 2.1870 Epoch 9/150 25/25 [==============================] - 3s 136ms/step - loss: 1.7167 - mae: 2.1650 Epoch 10/150 25/25 [==============================] - 3s 136ms/step - loss: 1.6976 - mae: 2.1454 Epoch 11/150 25/25 [==============================] - 3s 138ms/step - loss: 1.6808 - mae: 2.1281 Epoch 12/150 25/25 [==============================] - 3s 138ms/step - loss: 1.6661 - mae: 2.1128 Epoch 13/150 25/25 [==============================] - 4s 142ms/step - loss: 1.6531 - mae: 2.0993 Epoch 14/150 25/25 [==============================] - 4s 142ms/step - loss: 1.6417 - mae: 2.0872 Epoch 15/150 25/25 [==============================] - 3s 135ms/step - loss: 1.6315 - mae: 2.0764 Epoch 16/150 25/25 [==============================] - 3s 128ms/step - loss: 1.6223 - mae: 2.0667 Epoch 17/150 25/25 [==============================] - 3s 132ms/step - loss: 1.6141 - mae: 2.0579 Epoch 18/150 25/25 [==============================] - 3s 130ms/step - loss: 1.6067 - mae: 2.0500 Epoch 19/150 25/25 [==============================] - 3s 133ms/step - loss: 1.6000 - mae: 2.0429 Epoch 20/150 25/25 [==============================] - 3s 138ms/step - loss: 1.5939 - mae: 2.0364 Epoch 21/150 25/25 [==============================] - 3s 137ms/step - loss: 1.5883 - mae: 2.0306 Epoch 22/150 25/25 [==============================] - 3s 136ms/step - loss: 1.5833 - mae: 2.0254 Epoch 23/150 25/25 [==============================] - 4s 141ms/step - loss: 1.5787 - mae: 2.0207 Epoch 24/150 25/25 [==============================] - 3s 140ms/step - loss: 1.5745 - mae: 2.0163 Epoch 25/150 25/25 [==============================] - 4s 141ms/step - loss: 1.5707 - mae: 2.0124 Epoch 26/150 25/25 [==============================] - 3s 132ms/step - loss: 1.5672 - mae: 2.0089 Epoch 27/150 25/25 [==============================] - 3s 133ms/step - loss: 1.5640 - mae: 2.0056 Epoch 28/150 25/25 [==============================] - 3s 135ms/step - loss: 1.5610 - mae: 2.0026 Epoch 29/150 25/25 [==============================] - 3s 136ms/step - loss: 1.5583 - mae: 1.9998 Epoch 30/150 25/25 [==============================] - 3s 133ms/step - loss: 1.5558 - mae: 1.9973 Epoch 31/150 25/25 [==============================] - 3s 132ms/step - loss: 1.5534 - mae: 1.9949 Epoch 32/150 25/25 [==============================] - 3s 139ms/step - loss: 1.5512 - mae: 1.9927 Epoch 33/150 25/25 [==============================] - 3s 139ms/step - loss: 1.5491 - mae: 1.9906 Epoch 34/150 25/25 [==============================] - 3s 138ms/step - loss: 1.5472 - mae: 1.9886 Epoch 35/150 25/25 [==============================] - 4s 140ms/step - loss: 1.5453 - mae: 1.9868 Epoch 36/150 25/25 [==============================] - 4s 141ms/step - loss: 1.5436 - mae: 1.9850 Epoch 37/150 25/25 [==============================] - 3s 132ms/step - loss: 1.5419 - mae: 1.9833 Epoch 38/150 25/25 [==============================] - 3s 130ms/step - loss: 1.5404 - mae: 1.9817 Epoch 39/150 25/25 [==============================] - 3s 130ms/step - loss: 1.5389 - mae: 1.9803 Epoch 40/150 25/25 [==============================] - 3s 133ms/step - loss: 1.5376 - mae: 1.9789 Epoch 41/150 25/25 [==============================] - 3s 132ms/step - loss: 1.5363 - mae: 1.9776 Epoch 42/150 25/25 [==============================] - 3s 129ms/step - loss: 1.5350 - mae: 1.9764 Epoch 43/150 25/25 [==============================] - 3s 132ms/step - loss: 1.5338 - mae: 1.9752 Epoch 44/150 25/25 [==============================] - 4s 140ms/step - loss: 1.5327 - mae: 1.9741 Epoch 45/150 25/25 [==============================] - 4s 141ms/step - loss: 1.5316 - mae: 1.9730 Epoch 46/150 25/25 [==============================] - 3s 133ms/step - loss: 1.5305 - mae: 1.9719 Epoch 47/150 25/25 [==============================] - 3s 135ms/step - loss: 1.5295 - mae: 1.9708 Epoch 48/150 25/25 [==============================] - 3s 138ms/step - loss: 1.5286 - mae: 1.9698 Epoch 49/150 25/25 [==============================] - 3s 136ms/step - loss: 1.5276 - mae: 1.9689 Epoch 50/150 25/25 [==============================] - 3s 136ms/step - loss: 1.5267 - mae: 1.9680 Epoch 51/150 25/25 [==============================] - 3s 139ms/step - loss: 1.5259 - mae: 1.9672 Epoch 52/150 25/25 [==============================] - 3s 134ms/step - loss: 1.5251 - mae: 1.9664 Epoch 53/150 25/25 [==============================] - 3s 135ms/step - loss: 1.5243 - mae: 1.9656 Epoch 54/150 25/25 [==============================] - 3s 137ms/step - loss: 1.5235 - mae: 1.9648 Epoch 55/150 25/25 [==============================] - 3s 137ms/step - loss: 1.5228 - mae: 1.9640 Epoch 56/150 25/25 [==============================] - 3s 136ms/step - loss: 1.5221 - mae: 1.9633 Epoch 57/150 25/25 [==============================] - 3s 136ms/step - loss: 1.5213 - mae: 1.9626 Epoch 58/150 25/25 [==============================] - 3s 137ms/step - loss: 1.5206 - mae: 1.9619 Epoch 59/150 25/25 [==============================] - 3s 135ms/step - loss: 1.5199 - mae: 1.9611 Epoch 60/150 25/25 [==============================] - 3s 133ms/step - loss: 1.5193 - mae: 1.9604 Epoch 61/150 25/25 [==============================] - 3s 133ms/step - loss: 1.5186 - mae: 1.9597 Epoch 62/150 25/25 [==============================] - 3s 131ms/step - loss: 1.5179 - mae: 1.9589 Epoch 63/150 25/25 [==============================] - 3s 131ms/step - loss: 1.5173 - mae: 1.9582 Epoch 64/150 25/25 [==============================] - 3s 135ms/step - loss: 1.5166 - mae: 1.9576 Epoch 65/150 25/25 [==============================] - 3s 133ms/step - loss: 1.5161 - mae: 1.9570 Epoch 66/150 25/25 [==============================] - 3s 138ms/step - loss: 1.5155 - mae: 1.9564 Epoch 67/150 25/25 [==============================] - 3s 140ms/step - loss: 1.5149 - mae: 1.9557 Epoch 68/150 25/25 [==============================] - 4s 141ms/step - loss: 1.5142 - mae: 1.9550 Epoch 69/150 25/25 [==============================] - 3s 133ms/step - loss: 1.5136 - mae: 1.9543 Epoch 70/150 25/25 [==============================] - 3s 130ms/step - loss: 1.5129 - mae: 1.9536 Epoch 71/150 25/25 [==============================] - 3s 132ms/step - loss: 1.5122 - mae: 1.9528 Epoch 72/150 25/25 [==============================] - 3s 132ms/step - loss: 1.5115 - mae: 1.9521 Epoch 73/150 25/25 [==============================] - 3s 133ms/step - loss: 1.5108 - mae: 1.9514 Epoch 74/150 25/25 [==============================] - 3s 135ms/step - loss: 1.5102 - mae: 1.9508 Epoch 75/150 25/25 [==============================] - 3s 133ms/step - loss: 1.5097 - mae: 1.9503 Epoch 76/150 25/25 [==============================] - 3s 134ms/step - loss: 1.5091 - mae: 1.9497 Epoch 77/150 25/25 [==============================] - 3s 137ms/step - loss: 1.5084 - mae: 1.9491 Epoch 78/150 25/25 [==============================] - 3s 128ms/step - loss: 1.5078 - mae: 1.9485 Epoch 79/150 25/25 [==============================] - 3s 129ms/step - loss: 1.5072 - mae: 1.9478 Epoch 80/150 25/25 [==============================] - 3s 134ms/step - loss: 1.5065 - mae: 1.9471 Epoch 81/150 25/25 [==============================] - 3s 136ms/step - loss: 1.5059 - mae: 1.9465 Epoch 82/150 25/25 [==============================] - 3s 134ms/step - loss: 1.5053 - mae: 1.9459 Epoch 83/150 25/25 [==============================] - 3s 132ms/step - loss: 1.5048 - mae: 1.9454 Epoch 84/150 25/25 [==============================] - 3s 134ms/step - loss: 1.5043 - mae: 1.9448 Epoch 85/150 25/25 [==============================] - 3s 132ms/step - loss: 1.5038 - mae: 1.9443 Epoch 86/150 25/25 [==============================] - 3s 131ms/step - loss: 1.5033 - mae: 1.9438 Epoch 87/150 25/25 [==============================] - 3s 132ms/step - loss: 1.5028 - mae: 1.9433 Epoch 88/150 25/25 [==============================] - 3s 130ms/step - loss: 1.5023 - mae: 1.9428 Epoch 89/150 25/25 [==============================] - 3s 134ms/step - loss: 1.5017 - mae: 1.9422 Epoch 90/150 25/25 [==============================] - 3s 132ms/step - loss: 1.5012 - mae: 1.9416 Epoch 91/150 25/25 [==============================] - 3s 131ms/step - loss: 1.5004 - mae: 1.9408 Epoch 92/150 25/25 [==============================] - 3s 132ms/step - loss: 1.4992 - mae: 1.9396 Epoch 93/150 25/25 [==============================] - 3s 132ms/step - loss: 1.4982 - mae: 1.9386 Epoch 94/150 25/25 [==============================] - 3s 136ms/step - loss: 1.4975 - mae: 1.9378 Epoch 95/150 25/25 [==============================] - 3s 134ms/step - loss: 1.4968 - mae: 1.9371 Epoch 96/150 25/25 [==============================] - 3s 136ms/step - loss: 1.4962 - mae: 1.9364 Epoch 97/150 25/25 [==============================] - 3s 137ms/step - loss: 1.4955 - mae: 1.9358 Epoch 98/150 25/25 [==============================] - 3s 140ms/step - loss: 1.4949 - mae: 1.9351 Epoch 99/150 25/25 [==============================] - 3s 134ms/step - loss: 1.4942 - mae: 1.9344 Epoch 100/150 25/25 [==============================] - 3s 134ms/step - loss: 1.4934 - mae: 1.9336 Epoch 101/150 25/25 [==============================] - 3s 135ms/step - loss: 1.4927 - mae: 1.9328 Epoch 102/150 25/25 [==============================] - 3s 132ms/step - loss: 1.4919 - mae: 1.9319 Epoch 103/150 25/25 [==============================] - 3s 131ms/step - loss: 1.4913 - mae: 1.9313 Epoch 104/150 25/25 [==============================] - 3s 132ms/step - loss: 1.4908 - mae: 1.9307 Epoch 105/150 25/25 [==============================] - 3s 133ms/step - loss: 1.4903 - mae: 1.9302 Epoch 106/150 25/25 [==============================] - 3s 131ms/step - loss: 1.4899 - mae: 1.9298 Epoch 107/150 25/25 [==============================] - 3s 130ms/step - loss: 1.4895 - mae: 1.9293 Epoch 108/150 25/25 [==============================] - 3s 130ms/step - loss: 1.4892 - mae: 1.9289 Epoch 109/150 25/25 [==============================] - 3s 132ms/step - loss: 1.4888 - mae: 1.9285 Epoch 110/150 25/25 [==============================] - 3s 131ms/step - loss: 1.4884 - mae: 1.9282 Epoch 111/150 25/25 [==============================] - 3s 133ms/step - loss: 1.4881 - mae: 1.9278 Epoch 112/150 25/25 [==============================] - 3s 130ms/step - loss: 1.4878 - mae: 1.9275 Epoch 113/150 25/25 [==============================] - 3s 131ms/step - loss: 1.4874 - mae: 1.9271 Epoch 114/150 25/25 [==============================] - 3s 136ms/step - loss: 1.4871 - mae: 1.9268 Epoch 115/150 25/25 [==============================] - 3s 130ms/step - loss: 1.4868 - mae: 1.9265 Epoch 116/150 25/25 [==============================] - 3s 132ms/step - loss: 1.4865 - mae: 1.9262 Epoch 117/150 25/25 [==============================] - 3s 134ms/step - loss: 1.4862 - mae: 1.9258 Epoch 118/150 25/25 [==============================] - 3s 132ms/step - loss: 1.4859 - mae: 1.9255 Epoch 119/150 25/25 [==============================] - 3s 130ms/step - loss: 1.4856 - mae: 1.9252 Epoch 120/150 25/25 [==============================] - 3s 134ms/step - loss: 1.4853 - mae: 1.9249 Epoch 121/150 25/25 [==============================] - 3s 134ms/step - loss: 1.4851 - mae: 1.9247 Epoch 122/150 25/25 [==============================] - 3s 135ms/step - loss: 1.4848 - mae: 1.9243 Epoch 123/150 25/25 [==============================] - 3s 131ms/step - loss: 1.4845 - mae: 1.9240 Epoch 124/150 25/25 [==============================] - 3s 134ms/step - loss: 1.4842 - mae: 1.9238 Epoch 125/150 25/25 [==============================] - 3s 133ms/step - loss: 1.4840 - mae: 1.9235 Epoch 126/150 25/25 [==============================] - 3s 131ms/step - loss: 1.4838 - mae: 1.9232 Epoch 127/150 25/25 [==============================] - 3s 129ms/step - loss: 1.4835 - mae: 1.9230 Epoch 128/150 25/25 [==============================] - 3s 132ms/step - loss: 1.4833 - mae: 1.9228 Epoch 129/150 25/25 [==============================] - 3s 131ms/step - loss: 1.4831 - mae: 1.9225 Epoch 130/150 25/25 [==============================] - 3s 129ms/step - loss: 1.4828 - mae: 1.9222 Epoch 131/150 25/25 [==============================] - 3s 128ms/step - loss: 1.4826 - mae: 1.9220 Epoch 132/150 25/25 [==============================] - 3s 127ms/step - loss: 1.4823 - mae: 1.9217 Epoch 133/150 25/25 [==============================] - 3s 129ms/step - loss: 1.4821 - mae: 1.9214 Epoch 134/150 25/25 [==============================] - 3s 129ms/step - loss: 1.4818 - mae: 1.9212 Epoch 135/150 25/25 [==============================] - 3s 132ms/step - loss: 1.4815 - mae: 1.9209 Epoch 136/150 25/25 [==============================] - 3s 131ms/step - loss: 1.4813 - mae: 1.9206 Epoch 137/150 25/25 [==============================] - 3s 127ms/step - loss: 1.4811 - mae: 1.9204 Epoch 138/150 25/25 [==============================] - 3s 131ms/step - loss: 1.4808 - mae: 1.9201 Epoch 139/150 25/25 [==============================] - 3s 130ms/step - loss: 1.4806 - mae: 1.9199 Epoch 140/150 25/25 [==============================] - 3s 130ms/step - loss: 1.4804 - mae: 1.9197 Epoch 141/150 25/25 [==============================] - 3s 132ms/step - loss: 1.4802 - mae: 1.9194 Epoch 142/150 25/25 [==============================] - 3s 129ms/step - loss: 1.4799 - mae: 1.9192 Epoch 143/150 25/25 [==============================] - 3s 134ms/step - loss: 1.4797 - mae: 1.9189 Epoch 144/150 25/25 [==============================] - 3s 135ms/step - loss: 1.4795 - mae: 1.9187 Epoch 145/150 25/25 [==============================] - 3s 131ms/step - loss: 1.4794 - mae: 1.9185 Epoch 146/150 25/25 [==============================] - 3s 131ms/step - loss: 1.4791 - mae: 1.9183 Epoch 147/150 25/25 [==============================] - 3s 130ms/step - loss: 1.4789 - mae: 1.9180 Epoch 148/150 25/25 [==============================] - 3s 136ms/step - loss: 1.4787 - mae: 1.9178 Epoch 149/150 25/25 [==============================] - 3s 137ms/step - loss: 1.4784 - mae: 1.9175 Epoch 150/150 25/25 [==============================] - 4s 143ms/step - loss: 1.4782 - mae: 1.9173 . rnn_forecast = model_forecast(model, series[..., np.newaxis], window_size) rnn_forecast = rnn_forecast[split_time - window_size:-1, -1, 0] . plt.figure(figsize=(10, 6)) plot_series(time_valid, x_valid) plot_series(time_valid, rnn_forecast) # EXPECTED OUTPUT. PLOT SHOULD SHOW PROJECTIONS FOLLOWING ORIGINAL DATA CLOSELY . tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy() # EXPECTED OUTPUT MAE &lt; 2 -- I GOT 1.789626 . 1.780626 . print(rnn_forecast) # EXPECTED OUTPUT -- ARRAY OF VALUES IN THE LOW TEENS . [11.636601 10.97607 12.159701 ... 13.589686 13.726407 14.940471] .",
            "url": "https://rafaelsf80.github.io/notebooks/timeseries/2020/08/04/time-series-temperatures.html",
            "relUrl": "/timeseries/2020/08/04/time-series-temperatures.html",
            "date": " • Aug 4, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Series temporales: datos sintéticos",
            "content": "import tensorflow as tf import numpy as np import matplotlib.pyplot as plt print(tf.__version__) . def plot_series(time, series, format=&quot;-&quot;, start=0, end=None): plt.plot(time[start:end], series[start:end], format) plt.xlabel(&quot;Time&quot;) plt.ylabel(&quot;Value&quot;) plt.grid(True) def trend(time, slope=0): return slope * time def seasonal_pattern(season_time): return np.where(season_time &lt; 0.4, np.cos(season_time * 2 * np.pi), 1 / np.exp(3 * season_time)) def seasonality(time, period, amplitude=1, phase=0): season_time = ((time + phase) % period) / period return amplitude * seasonal_pattern(season_time) def noise(time, noise_level=1, seed=None): rnd = np.random.RandomState(seed) return rnd.randn(len(time)) * noise_level time = np.arange(4 * 365 + 1, dtype=&quot;float32&quot;) baseline = 10 amplitude = 40 slope = 0.05 noise_level = 5 # Crear la serie series = baseline + trend(time, slope) + seasonality(time, period=365, amplitude=amplitude) # Añadir ruido blanco series += noise(time, noise_level, seed=42) split_time = 1000 time_train = time[:split_time] x_train = series[:split_time] time_valid = time[split_time:] x_valid = series[split_time:] window_size = 20 batch_size = 32 shuffle_buffer_size = 1000 . # Helper function para crear los inputs def windowed_dataset(series, window_size, batch_size, shuffle_buffer): series = tf.expand_dims(series, axis=-1) ds = tf.data.Dataset.from_tensor_slices(series) ds = ds.window(window_size + 1, shift=1, drop_remainder=True) ds = ds.flat_map(lambda w: w.batch(window_size + 1)) ds = ds.shuffle(shuffle_buffer) ds = ds.map(lambda w: (w[:-1], w[1:])) return ds.batch(batch_size).prefetch(1) . # Helper function para predecir con una serie de entrada def model_forecast(model, series, window_size): ds = tf.data.Dataset.from_tensor_slices(series) ds = ds.window(window_size, shift=1, drop_remainder=True) ds = ds.flat_map(lambda w: w.batch(window_size)) ds = ds.batch(32).prefetch(1) forecast = model.predict(ds) return forecast . tf.keras.backend.clear_session() tf.random.set_seed(51) np.random.seed(51) window_size = 30 train_set = windowed_dataset(x_train, window_size, batch_size=128, shuffle_buffer=shuffle_buffer_size) model = tf.keras.models.Sequential([ tf.keras.layers.Conv1D(filters=32, kernel_size=5, strides=1, padding=&quot;causal&quot;, activation=&quot;relu&quot;, input_shape=[None, 1]), tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)), tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)), tf.keras.layers.Dense(1), tf.keras.layers.Lambda(lambda x: x * 200) ]) lr_schedule = tf.keras.callbacks.LearningRateScheduler( lambda epoch: 1e-8 * 10**(epoch / 20)) optimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9) model.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer, metrics=[&quot;mae&quot;]) history = model.fit(train_set, epochs=100, callbacks=[lr_schedule]) . plt.semilogx(history.history[&quot;lr&quot;], history.history[&quot;loss&quot;]) plt.axis([1e-8, 1e-4, 0, 30]) . tf.keras.backend.clear_session() tf.random.set_seed(51) np.random.seed(51) #batch_size = 16 dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size) model = tf.keras.models.Sequential([ tf.keras.layers.Conv1D(filters=32, kernel_size=3, strides=1, padding=&quot;causal&quot;, activation=&quot;relu&quot;, input_shape=[None, 1]), tf.keras.layers.LSTM(32, return_sequences=True), tf.keras.layers.LSTM(32, return_sequences=True), tf.keras.layers.Dense(1), tf.keras.layers.Lambda(lambda x: x * 200) ]) optimizer = tf.keras.optimizers.SGD(lr=1e-5, momentum=0.9) model.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer, metrics=[&quot;mae&quot;]) history = model.fit(dataset,epochs=500) . rnn_forecast = model_forecast(model, series[..., np.newaxis], window_size) rnn_forecast = rnn_forecast[split_time - window_size:-1, -1, 0] . plt.figure(figsize=(10, 6)) plot_series(time_valid, x_valid) plot_series(time_valid, rnn_forecast) . tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy() . import matplotlib.image as mpimg import matplotlib.pyplot as plt #-- # Retrieve a list of list results on training and test data # sets for each training epoch #-- mae=history.history[&#39;mae&#39;] loss=history.history[&#39;loss&#39;] epochs=range(len(loss)) # Get number of epochs # # Plot MAE and Loss # plt.plot(epochs, mae, &#39;r&#39;) plt.plot(epochs, loss, &#39;b&#39;) plt.title(&#39;MAE and Loss&#39;) plt.xlabel(&quot;Epochs&quot;) plt.ylabel(&quot;Accuracy&quot;) plt.legend([&quot;MAE&quot;, &quot;Loss&quot;]) plt.figure() epochs_zoom = epochs[200:] mae_zoom = mae[200:] loss_zoom = loss[200:] # # Plot Zoomed MAE and Loss # plt.plot(epochs_zoom, mae_zoom, &#39;r&#39;) plt.plot(epochs_zoom, loss_zoom, &#39;b&#39;) plt.title(&#39;MAE and Loss&#39;) plt.xlabel(&quot;Epochs&quot;) plt.ylabel(&quot;Accuracy&quot;) plt.legend([&quot;MAE&quot;, &quot;Loss&quot;]) plt.figure() .",
            "url": "https://rafaelsf80.github.io/notebooks/timeseries/2020/08/04/time-series-synthetic.html",
            "relUrl": "/timeseries/2020/08/04/time-series-synthetic.html",
            "date": " • Aug 4, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Series temporales: manchas solares",
            "content": "Introducci&#243;n . Se resolverá la competición de Kaggle sobre manchas solares. . import tensorflow as tf import numpy as np import matplotlib.pyplot as plt print(tf.__version__) # If not, !pip3 install tensorflow==2.2.0 # Does not work with Colab. Manually download Sunspots.csv and place it in /tmp import os os.system(&#39;kaggle datasets download -d robervalt/sunspots&#39;) import numpy as np import matplotlib.pyplot as plt def plot_series(time, series, format=&quot;-&quot;, start=0, end=None): plt.plot(time[start:end], series[start:end], format) plt.xlabel(&quot;Time&quot;) plt.ylabel(&quot;Value&quot;) plt.grid(True) import csv time_step = [] sunspots = [] with open(&#39;/tmp/Sunspots.csv&#39;) as csvfile: reader = csv.reader(csvfile, delimiter=&#39;,&#39;) next(reader) for row in reader: sunspots.append(float(row[2])) time_step.append(int(row[0])) series = np.array(sunspots) time = np.array(time_step) plt.figure(figsize=(10, 6)) plot_series(time, series) . 2.3.0 . series = np.array(sunspots) time = np.array(time_step) plt.figure(figsize=(10, 6)) plot_series(time, series) . split_time = 3000 time_train = time[:split_time] x_train = series[:split_time] time_valid = time[split_time:] x_valid = series[split_time:] window_size = 30 batch_size = 32 shuffle_buffer_size = 1000 def windowed_dataset(series, window_size, batch_size, shuffle_buffer): series = tf.expand_dims(series, axis=-1) ds = tf.data.Dataset.from_tensor_slices(series) ds = ds.window(window_size + 1, shift=1, drop_remainder=True) ds = ds.flat_map(lambda w: w.batch(window_size + 1)) ds = ds.shuffle(shuffle_buffer) ds = ds.map(lambda w: (w[:-1], w[1:])) return ds.batch(batch_size).prefetch(1) def model_forecast(model, series, window_size): ds = tf.data.Dataset.from_tensor_slices(series) ds = ds.window(window_size, shift=1, drop_remainder=True) ds = ds.flat_map(lambda w: w.batch(window_size)) ds = ds.batch(32).prefetch(1) forecast = model.predict(ds) return forecast tf.keras.backend.clear_session() tf.random.set_seed(51) np.random.seed(51) window_size = 64 batch_size = 256 train_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size) print(train_set) print(x_train.shape) . &lt;PrefetchDataset shapes: ((None, None, 1), (None, None, 1)), types: (tf.float64, tf.float64)&gt; (3000,) . model = tf.keras.models.Sequential([ tf.keras.layers.Conv1D(filters=32, kernel_size=5, strides=1, padding=&quot;causal&quot;, activation=&quot;relu&quot;, input_shape=[None, 1]), tf.keras.layers.LSTM(64, return_sequences=True), tf.keras.layers.LSTM(64, return_sequences=True), tf.keras.layers.Dense(30, activation=&quot;relu&quot;), tf.keras.layers.Dense(10, activation=&quot;relu&quot;), tf.keras.layers.Dense(1), tf.keras.layers.Lambda(lambda x: x * 400) ]) lr_schedule = tf.keras.callbacks.LearningRateScheduler( lambda epoch: 1e-8 * 10**(epoch / 20)) optimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9) model.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer, metrics=[&quot;mae&quot;]) history = model.fit(train_set, epochs=100, callbacks=[lr_schedule]) . Epoch 1/100 12/12 [==============================] - 0s 22ms/step - loss: 79.8340 - mae: 80.3314 Epoch 2/100 12/12 [==============================] - 0s 25ms/step - loss: 78.0944 - mae: 78.5918 Epoch 3/100 12/12 [==============================] - 0s 26ms/step - loss: 75.4519 - mae: 75.9497 Epoch 4/100 12/12 [==============================] - 0s 25ms/step - loss: 72.2679 - mae: 72.7658 Epoch 5/100 12/12 [==============================] - 0s 25ms/step - loss: 68.7693 - mae: 69.2672 Epoch 6/100 12/12 [==============================] - 0s 30ms/step - loss: 65.1128 - mae: 65.6099 Epoch 7/100 12/12 [==============================] - 0s 27ms/step - loss: 61.5272 - mae: 62.0241 Epoch 8/100 12/12 [==============================] - 0s 27ms/step - loss: 58.1406 - mae: 58.6369 Epoch 9/100 12/12 [==============================] - 0s 29ms/step - loss: 55.0732 - mae: 55.5697 Epoch 10/100 12/12 [==============================] - 0s 35ms/step - loss: 52.3436 - mae: 52.8400 Epoch 11/100 12/12 [==============================] - 0s 25ms/step - loss: 49.9148 - mae: 50.4113 Epoch 12/100 12/12 [==============================] - 0s 28ms/step - loss: 47.8592 - mae: 48.3560 Epoch 13/100 12/12 [==============================] - 0s 27ms/step - loss: 46.0553 - mae: 46.5517 Epoch 14/100 12/12 [==============================] - 0s 27ms/step - loss: 44.5444 - mae: 45.0406 Epoch 15/100 12/12 [==============================] - 0s 26ms/step - loss: 43.3078 - mae: 43.8045 Epoch 16/100 12/12 [==============================] - 0s 27ms/step - loss: 42.2855 - mae: 42.7826 Epoch 17/100 12/12 [==============================] - 0s 29ms/step - loss: 41.3797 - mae: 41.8767 Epoch 18/100 12/12 [==============================] - 0s 37ms/step - loss: 40.5481 - mae: 41.0453 Epoch 19/100 12/12 [==============================] - 0s 25ms/step - loss: 39.7690 - mae: 40.2659 Epoch 20/100 12/12 [==============================] - 0s 27ms/step - loss: 39.0213 - mae: 39.5181 Epoch 21/100 12/12 [==============================] - 0s 26ms/step - loss: 38.2735 - mae: 38.7699 Epoch 22/100 12/12 [==============================] - 0s 26ms/step - loss: 37.4213 - mae: 37.9177 Epoch 23/100 12/12 [==============================] - 0s 25ms/step - loss: 36.3873 - mae: 36.8839 Epoch 24/100 12/12 [==============================] - 0s 26ms/step - loss: 35.3029 - mae: 35.7992 Epoch 25/100 12/12 [==============================] - 0s 26ms/step - loss: 34.0842 - mae: 34.5800 Epoch 26/100 12/12 [==============================] - 0s 36ms/step - loss: 32.9492 - mae: 33.4449 Epoch 27/100 12/12 [==============================] - 0s 30ms/step - loss: 31.8640 - mae: 32.3595 Epoch 28/100 12/12 [==============================] - 0s 36ms/step - loss: 31.1214 - mae: 31.6171 Epoch 29/100 12/12 [==============================] - 0s 26ms/step - loss: 30.3468 - mae: 30.8424 Epoch 30/100 12/12 [==============================] - 0s 26ms/step - loss: 29.6211 - mae: 30.1165 Epoch 31/100 12/12 [==============================] - 0s 26ms/step - loss: 29.0370 - mae: 29.5323 Epoch 32/100 12/12 [==============================] - 0s 31ms/step - loss: 28.5865 - mae: 29.0818 Epoch 33/100 12/12 [==============================] - 0s 26ms/step - loss: 28.1712 - mae: 28.6663 Epoch 34/100 12/12 [==============================] - 0s 35ms/step - loss: 27.9727 - mae: 28.4673 Epoch 35/100 12/12 [==============================] - 0s 33ms/step - loss: 27.7913 - mae: 28.2859 Epoch 36/100 12/12 [==============================] - 0s 34ms/step - loss: 27.3850 - mae: 27.8797 Epoch 37/100 12/12 [==============================] - 0s 25ms/step - loss: 26.9803 - mae: 27.4748 Epoch 38/100 12/12 [==============================] - 0s 26ms/step - loss: 26.7822 - mae: 27.2767 Epoch 39/100 12/12 [==============================] - 0s 35ms/step - loss: 26.2987 - mae: 26.7932 Epoch 40/100 12/12 [==============================] - 0s 35ms/step - loss: 25.7735 - mae: 26.2680 Epoch 41/100 12/12 [==============================] - 0s 26ms/step - loss: 25.4864 - mae: 25.9808 Epoch 42/100 12/12 [==============================] - 0s 27ms/step - loss: 25.1964 - mae: 25.6908 Epoch 43/100 12/12 [==============================] - 0s 25ms/step - loss: 24.8792 - mae: 25.3733 Epoch 44/100 12/12 [==============================] - 0s 26ms/step - loss: 24.2325 - mae: 24.7270 Epoch 45/100 12/12 [==============================] - 0s 25ms/step - loss: 23.8610 - mae: 24.3553 Epoch 46/100 12/12 [==============================] - 0s 30ms/step - loss: 23.8753 - mae: 24.3694 Epoch 47/100 12/12 [==============================] - 0s 29ms/step - loss: 23.2602 - mae: 23.7541 Epoch 48/100 12/12 [==============================] - 0s 24ms/step - loss: 22.8506 - mae: 23.3443 Epoch 49/100 12/12 [==============================] - 0s 25ms/step - loss: 22.4956 - mae: 22.9894 Epoch 50/100 12/12 [==============================] - 0s 26ms/step - loss: 22.0830 - mae: 22.5768 Epoch 51/100 12/12 [==============================] - 0s 31ms/step - loss: 22.2996 - mae: 22.7936 Epoch 52/100 12/12 [==============================] - 0s 25ms/step - loss: 22.0087 - mae: 22.5026 Epoch 53/100 12/12 [==============================] - 0s 36ms/step - loss: 22.0203 - mae: 22.5139 Epoch 54/100 12/12 [==============================] - 0s 33ms/step - loss: 21.2748 - mae: 21.7688 Epoch 55/100 12/12 [==============================] - 0s 35ms/step - loss: 21.1535 - mae: 21.6472 Epoch 56/100 12/12 [==============================] - 0s 34ms/step - loss: 20.5946 - mae: 21.0881 Epoch 57/100 12/12 [==============================] - 0s 35ms/step - loss: 20.8638 - mae: 21.3574 Epoch 58/100 12/12 [==============================] - 0s 38ms/step - loss: 20.6624 - mae: 21.1558 Epoch 59/100 12/12 [==============================] - 0s 33ms/step - loss: 20.4109 - mae: 20.9043 Epoch 60/100 12/12 [==============================] - 0s 31ms/step - loss: 20.0422 - mae: 20.5353 Epoch 61/100 12/12 [==============================] - 0s 31ms/step - loss: 19.9312 - mae: 20.4242 Epoch 62/100 12/12 [==============================] - 0s 37ms/step - loss: 20.7496 - mae: 21.2431 Epoch 63/100 12/12 [==============================] - 0s 26ms/step - loss: 19.8060 - mae: 20.2991 Epoch 64/100 12/12 [==============================] - 0s 25ms/step - loss: 20.7451 - mae: 21.2383 Epoch 65/100 12/12 [==============================] - 0s 26ms/step - loss: 19.6167 - mae: 20.1099 Epoch 66/100 12/12 [==============================] - 0s 35ms/step - loss: 20.5485 - mae: 21.0418 Epoch 67/100 12/12 [==============================] - 0s 29ms/step - loss: 22.2171 - mae: 22.7108 Epoch 68/100 12/12 [==============================] - 0s 25ms/step - loss: 19.4993 - mae: 19.9921 Epoch 69/100 12/12 [==============================] - 0s 27ms/step - loss: 22.1643 - mae: 22.6578 Epoch 70/100 12/12 [==============================] - 0s 25ms/step - loss: 21.3564 - mae: 21.8497 Epoch 71/100 12/12 [==============================] - 0s 28ms/step - loss: 20.4340 - mae: 20.9270 Epoch 72/100 12/12 [==============================] - 0s 35ms/step - loss: 21.4140 - mae: 21.9070 Epoch 73/100 12/12 [==============================] - 0s 29ms/step - loss: 20.9080 - mae: 21.4009 Epoch 74/100 12/12 [==============================] - 0s 25ms/step - loss: 21.9128 - mae: 22.4061 Epoch 75/100 12/12 [==============================] - 0s 27ms/step - loss: 19.8545 - mae: 20.3472 Epoch 76/100 12/12 [==============================] - 0s 35ms/step - loss: 23.4880 - mae: 23.9823 Epoch 77/100 12/12 [==============================] - 0s 31ms/step - loss: 21.4208 - mae: 21.9141 Epoch 78/100 12/12 [==============================] - 0s 26ms/step - loss: 21.8130 - mae: 22.3065 Epoch 79/100 12/12 [==============================] - 0s 28ms/step - loss: 22.6996 - mae: 23.1936 Epoch 80/100 12/12 [==============================] - 0s 26ms/step - loss: 23.4137 - mae: 23.9077 Epoch 81/100 12/12 [==============================] - 0s 26ms/step - loss: 36.0214 - mae: 36.5176 Epoch 82/100 12/12 [==============================] - 0s 26ms/step - loss: 37.9856 - mae: 38.4822 Epoch 83/100 12/12 [==============================] - 0s 26ms/step - loss: 46.1187 - mae: 46.6162 Epoch 84/100 12/12 [==============================] - 0s 28ms/step - loss: 36.3539 - mae: 36.8504 Epoch 85/100 12/12 [==============================] - 0s 27ms/step - loss: 38.0205 - mae: 38.5174 Epoch 86/100 12/12 [==============================] - 0s 35ms/step - loss: 34.9671 - mae: 35.4639 Epoch 87/100 12/12 [==============================] - 0s 26ms/step - loss: 35.3582 - mae: 35.8547 Epoch 88/100 12/12 [==============================] - 0s 34ms/step - loss: 36.7537 - mae: 37.2507 Epoch 89/100 12/12 [==============================] - 0s 31ms/step - loss: 33.7122 - mae: 34.2083 Epoch 90/100 12/12 [==============================] - 0s 26ms/step - loss: 60.4209 - mae: 60.9192 Epoch 91/100 12/12 [==============================] - 0s 26ms/step - loss: 52.6610 - mae: 53.1589 Epoch 92/100 12/12 [==============================] - 0s 25ms/step - loss: 49.7735 - mae: 50.2717 Epoch 93/100 12/12 [==============================] - 0s 25ms/step - loss: 54.1865 - mae: 54.6842 Epoch 94/100 12/12 [==============================] - 0s 26ms/step - loss: 58.2520 - mae: 58.7497 Epoch 95/100 12/12 [==============================] - 0s 25ms/step - loss: 56.2516 - mae: 56.7495 Epoch 96/100 12/12 [==============================] - 0s 26ms/step - loss: 50.3765 - mae: 50.8749 Epoch 97/100 12/12 [==============================] - 0s 30ms/step - loss: 60.5635 - mae: 61.0616 Epoch 98/100 12/12 [==============================] - 0s 27ms/step - loss: 56.9803 - mae: 57.4789 Epoch 99/100 12/12 [==============================] - 0s 26ms/step - loss: 55.6900 - mae: 56.1883 Epoch 100/100 12/12 [==============================] - 0s 33ms/step - loss: 55.0430 - mae: 55.5413 . plt.semilogx(history.history[&quot;lr&quot;], history.history[&quot;loss&quot;]) plt.axis([1e-8, 1e-4, 0, 60]) . (1e-08, 0.0001, 0.0, 60.0) . tf.keras.backend.clear_session() tf.random.set_seed(51) np.random.seed(51) train_set = windowed_dataset(x_train, window_size=60, batch_size=100, shuffle_buffer=shuffle_buffer_size) model = tf.keras.models.Sequential([ tf.keras.layers.Conv1D(filters=60, kernel_size=5, strides=1, padding=&quot;causal&quot;, activation=&quot;relu&quot;, input_shape=[None, 1]), tf.keras.layers.LSTM(60, return_sequences=True), tf.keras.layers.LSTM(60, return_sequences=True), tf.keras.layers.Dense(30, activation=&quot;relu&quot;), tf.keras.layers.Dense(10, activation=&quot;relu&quot;), tf.keras.layers.Dense(1), tf.keras.layers.Lambda(lambda x: x * 400) ]) optimizer = tf.keras.optimizers.SGD(lr=1e-5, momentum=0.9) model.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer, metrics=[&quot;mae&quot;]) history = model.fit(train_set,epochs=500) rnn_forecast = model_forecast(model, series[..., np.newaxis], window_size) rnn_forecast = rnn_forecast[split_time - window_size:-1, -1, 0] plt.figure(figsize=(10, 6)) plot_series(time_valid, x_valid) plot_series(time_valid, rnn_forecast) tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy() import matplotlib.image as mpimg import matplotlib.pyplot as plt #-- # Retrieve a list of list results on training and test data # sets for each training epoch #-- loss=history.history[&#39;loss&#39;] epochs=range(len(loss)) # Get number of epochs # # Plot training and validation loss per epoch # plt.plot(epochs, loss, &#39;r&#39;) plt.title(&#39;Training loss&#39;) plt.xlabel(&quot;Epochs&quot;) plt.ylabel(&quot;Loss&quot;) plt.legend([&quot;Loss&quot;]) plt.figure() zoomed_loss = loss[200:] zoomed_epochs = range(200,500) # # Plot training and validation loss per epoch # plt.plot(zoomed_epochs, zoomed_loss, &#39;r&#39;) plt.title(&#39;Training loss&#39;) plt.xlabel(&quot;Epochs&quot;) plt.ylabel(&quot;Loss&quot;) plt.legend([&quot;Loss&quot;]) plt.figure() . Epoch 1/500 30/30 [==============================] - 0s 14ms/step - loss: 38.9198 - mae: 39.4161 Epoch 2/500 30/30 [==============================] - 0s 14ms/step - loss: 25.7735 - mae: 26.2680 Epoch 3/500 30/30 [==============================] - 0s 13ms/step - loss: 22.0767 - mae: 22.5705 Epoch 4/500 30/30 [==============================] - 0s 14ms/step - loss: 20.4675 - mae: 20.9605 Epoch 5/500 30/30 [==============================] - 0s 15ms/step - loss: 19.7721 - mae: 20.2645 Epoch 6/500 30/30 [==============================] - 0s 14ms/step - loss: 19.3017 - mae: 19.7939 Epoch 7/500 30/30 [==============================] - 0s 15ms/step - loss: 18.7228 - mae: 19.2149 Epoch 8/500 30/30 [==============================] - 0s 14ms/step - loss: 18.1242 - mae: 18.6159 Epoch 9/500 30/30 [==============================] - 0s 14ms/step - loss: 18.0721 - mae: 18.5636 Epoch 10/500 30/30 [==============================] - 0s 14ms/step - loss: 18.1061 - mae: 18.5977 Epoch 11/500 30/30 [==============================] - 1s 17ms/step - loss: 17.8688 - mae: 18.3600 Epoch 12/500 30/30 [==============================] - 0s 16ms/step - loss: 17.9055 - mae: 18.3960 Epoch 13/500 30/30 [==============================] - 0s 14ms/step - loss: 17.8534 - mae: 18.3441 Epoch 14/500 30/30 [==============================] - 0s 14ms/step - loss: 17.7943 - mae: 18.2851 Epoch 15/500 30/30 [==============================] - 0s 14ms/step - loss: 17.5729 - mae: 18.0639 Epoch 16/500 30/30 [==============================] - 0s 15ms/step - loss: 17.5833 - mae: 18.0738 Epoch 17/500 30/30 [==============================] - 0s 14ms/step - loss: 17.5072 - mae: 17.9979 Epoch 18/500 30/30 [==============================] - 0s 14ms/step - loss: 17.3512 - mae: 17.8416 Epoch 19/500 30/30 [==============================] - 0s 14ms/step - loss: 17.6318 - mae: 18.1227 Epoch 20/500 30/30 [==============================] - 0s 14ms/step - loss: 17.3520 - mae: 17.8426 Epoch 21/500 30/30 [==============================] - 0s 14ms/step - loss: 17.6473 - mae: 18.1381 Epoch 22/500 30/30 [==============================] - 1s 17ms/step - loss: 17.3582 - mae: 17.8488 Epoch 23/500 30/30 [==============================] - 0s 16ms/step - loss: 17.1359 - mae: 17.6265 Epoch 24/500 30/30 [==============================] - 1s 17ms/step - loss: 17.2582 - mae: 17.7486 Epoch 25/500 30/30 [==============================] - 0s 15ms/step - loss: 17.1194 - mae: 17.6098 Epoch 26/500 30/30 [==============================] - 0s 14ms/step - loss: 17.2270 - mae: 17.7175 Epoch 27/500 30/30 [==============================] - 0s 14ms/step - loss: 17.0160 - mae: 17.5065 Epoch 28/500 30/30 [==============================] - 0s 14ms/step - loss: 17.3924 - mae: 17.8832 Epoch 29/500 30/30 [==============================] - 0s 14ms/step - loss: 17.1180 - mae: 17.6085 Epoch 30/500 30/30 [==============================] - 0s 14ms/step - loss: 16.9777 - mae: 17.4680 Epoch 31/500 30/30 [==============================] - 0s 14ms/step - loss: 16.9133 - mae: 17.4035 Epoch 32/500 30/30 [==============================] - 0s 14ms/step - loss: 17.0628 - mae: 17.5530 Epoch 33/500 30/30 [==============================] - 0s 14ms/step - loss: 17.1168 - mae: 17.6070 Epoch 34/500 30/30 [==============================] - 0s 14ms/step - loss: 16.9634 - mae: 17.4536 Epoch 35/500 30/30 [==============================] - 0s 14ms/step - loss: 17.0348 - mae: 17.5251 Epoch 36/500 30/30 [==============================] - 0s 14ms/step - loss: 16.8702 - mae: 17.3601 Epoch 37/500 30/30 [==============================] - 0s 16ms/step - loss: 17.1608 - mae: 17.6511 Epoch 38/500 30/30 [==============================] - 1s 17ms/step - loss: 17.1744 - mae: 17.6645 Epoch 39/500 30/30 [==============================] - 0s 15ms/step - loss: 17.0314 - mae: 17.5214 Epoch 40/500 30/30 [==============================] - 0s 14ms/step - loss: 16.9324 - mae: 17.4222 Epoch 41/500 30/30 [==============================] - 0s 14ms/step - loss: 16.8663 - mae: 17.3561 Epoch 42/500 30/30 [==============================] - 0s 14ms/step - loss: 16.8897 - mae: 17.3795 Epoch 43/500 30/30 [==============================] - 0s 14ms/step - loss: 16.8672 - mae: 17.3569 Epoch 44/500 30/30 [==============================] - 0s 16ms/step - loss: 17.2954 - mae: 17.7853 Epoch 45/500 30/30 [==============================] - 0s 14ms/step - loss: 16.7839 - mae: 17.2739 Epoch 46/500 30/30 [==============================] - 0s 14ms/step - loss: 16.8649 - mae: 17.3543 Epoch 47/500 30/30 [==============================] - 0s 14ms/step - loss: 16.8154 - mae: 17.3053 Epoch 48/500 30/30 [==============================] - 0s 14ms/step - loss: 16.8541 - mae: 17.3438 Epoch 49/500 30/30 [==============================] - 0s 15ms/step - loss: 16.9587 - mae: 17.4486 Epoch 50/500 30/30 [==============================] - 1s 19ms/step - loss: 16.7717 - mae: 17.2614 Epoch 51/500 30/30 [==============================] - 0s 15ms/step - loss: 16.7268 - mae: 17.2164 Epoch 52/500 30/30 [==============================] - 0s 14ms/step - loss: 16.9362 - mae: 17.4257 Epoch 53/500 30/30 [==============================] - 0s 14ms/step - loss: 16.7150 - mae: 17.2048 Epoch 54/500 30/30 [==============================] - 0s 14ms/step - loss: 16.7991 - mae: 17.2883 Epoch 55/500 30/30 [==============================] - 0s 15ms/step - loss: 16.7678 - mae: 17.2573 Epoch 56/500 30/30 [==============================] - 0s 14ms/step - loss: 16.6731 - mae: 17.1628 Epoch 57/500 30/30 [==============================] - 0s 15ms/step - loss: 16.6024 - mae: 17.0919 Epoch 58/500 30/30 [==============================] - 0s 15ms/step - loss: 16.5811 - mae: 17.0704 Epoch 59/500 30/30 [==============================] - 0s 17ms/step - loss: 16.7307 - mae: 17.2201 Epoch 60/500 30/30 [==============================] - 0s 14ms/step - loss: 16.7199 - mae: 17.2093 Epoch 61/500 30/30 [==============================] - 0s 16ms/step - loss: 16.6808 - mae: 17.1704 Epoch 62/500 30/30 [==============================] - 1s 17ms/step - loss: 16.6156 - mae: 17.1049 Epoch 63/500 30/30 [==============================] - 0s 16ms/step - loss: 16.6709 - mae: 17.1601 Epoch 64/500 30/30 [==============================] - 1s 17ms/step - loss: 16.5756 - mae: 17.0649 Epoch 65/500 30/30 [==============================] - 0s 14ms/step - loss: 16.5314 - mae: 17.0202 Epoch 66/500 30/30 [==============================] - 0s 14ms/step - loss: 16.5359 - mae: 17.0251 Epoch 67/500 30/30 [==============================] - 0s 15ms/step - loss: 16.5913 - mae: 17.0804 Epoch 68/500 30/30 [==============================] - 0s 15ms/step - loss: 16.6478 - mae: 17.1372 Epoch 69/500 30/30 [==============================] - 0s 14ms/step - loss: 16.5077 - mae: 16.9968 Epoch 70/500 30/30 [==============================] - 0s 14ms/step - loss: 16.6393 - mae: 17.1285 Epoch 71/500 30/30 [==============================] - 0s 14ms/step - loss: 16.5727 - mae: 17.0621 Epoch 72/500 30/30 [==============================] - 0s 16ms/step - loss: 16.4809 - mae: 16.9702 Epoch 73/500 30/30 [==============================] - 0s 16ms/step - loss: 16.5381 - mae: 17.0273 Epoch 74/500 30/30 [==============================] - 0s 14ms/step - loss: 16.6619 - mae: 17.1511 Epoch 75/500 30/30 [==============================] - 0s 16ms/step - loss: 16.4973 - mae: 16.9863 Epoch 76/500 30/30 [==============================] - 0s 15ms/step - loss: 16.4755 - mae: 16.9643 Epoch 77/500 30/30 [==============================] - 0s 14ms/step - loss: 16.4491 - mae: 16.9380 Epoch 78/500 30/30 [==============================] - 0s 14ms/step - loss: 16.4661 - mae: 16.9550 Epoch 79/500 30/30 [==============================] - 0s 15ms/step - loss: 16.4554 - mae: 16.9441 Epoch 80/500 30/30 [==============================] - 0s 14ms/step - loss: 16.6619 - mae: 17.1510 Epoch 81/500 30/30 [==============================] - 1s 17ms/step - loss: 16.4701 - mae: 16.9590 Epoch 82/500 30/30 [==============================] - 1s 17ms/step - loss: 16.4889 - mae: 16.9778 Epoch 83/500 30/30 [==============================] - 1s 17ms/step - loss: 16.3861 - mae: 16.8747 Epoch 84/500 30/30 [==============================] - 0s 14ms/step - loss: 16.4110 - mae: 16.8994 Epoch 85/500 30/30 [==============================] - 0s 15ms/step - loss: 16.4080 - mae: 16.8968 Epoch 86/500 30/30 [==============================] - 0s 14ms/step - loss: 16.4727 - mae: 16.9617 Epoch 87/500 30/30 [==============================] - 0s 17ms/step - loss: 16.7928 - mae: 17.2820 Epoch 88/500 30/30 [==============================] - 0s 16ms/step - loss: 16.4995 - mae: 16.9887 Epoch 89/500 30/30 [==============================] - 0s 14ms/step - loss: 16.3739 - mae: 16.8624 Epoch 90/500 30/30 [==============================] - 0s 14ms/step - loss: 16.4305 - mae: 16.9193 Epoch 91/500 30/30 [==============================] - 0s 14ms/step - loss: 16.4828 - mae: 16.9718 Epoch 92/500 30/30 [==============================] - 0s 15ms/step - loss: 16.4846 - mae: 16.9732 Epoch 93/500 30/30 [==============================] - 0s 14ms/step - loss: 16.4738 - mae: 16.9629 Epoch 94/500 30/30 [==============================] - 0s 16ms/step - loss: 16.6700 - mae: 17.1590 Epoch 95/500 30/30 [==============================] - 0s 17ms/step - loss: 16.5721 - mae: 17.0614 Epoch 96/500 30/30 [==============================] - 0s 14ms/step - loss: 16.3160 - mae: 16.8046 Epoch 97/500 30/30 [==============================] - 0s 16ms/step - loss: 16.3488 - mae: 16.8376 Epoch 98/500 30/30 [==============================] - 1s 17ms/step - loss: 16.5817 - mae: 17.0709 Epoch 99/500 30/30 [==============================] - 1s 18ms/step - loss: 16.3842 - mae: 16.8730 Epoch 100/500 30/30 [==============================] - 0s 16ms/step - loss: 16.3685 - mae: 16.8573 Epoch 101/500 30/30 [==============================] - 1s 18ms/step - loss: 16.3120 - mae: 16.8007 Epoch 102/500 30/30 [==============================] - 0s 15ms/step - loss: 16.6005 - mae: 17.0894 Epoch 103/500 30/30 [==============================] - 0s 14ms/step - loss: 16.2952 - mae: 16.7837 Epoch 104/500 30/30 [==============================] - 0s 15ms/step - loss: 16.4471 - mae: 16.9363 Epoch 105/500 30/30 [==============================] - 0s 15ms/step - loss: 16.3051 - mae: 16.7936 Epoch 106/500 30/30 [==============================] - 0s 13ms/step - loss: 16.2802 - mae: 16.7686 Epoch 107/500 30/30 [==============================] - 0s 14ms/step - loss: 16.2607 - mae: 16.7493 Epoch 108/500 30/30 [==============================] - 0s 14ms/step - loss: 16.2879 - mae: 16.7763 Epoch 109/500 30/30 [==============================] - 0s 14ms/step - loss: 16.4059 - mae: 16.8947 Epoch 110/500 30/30 [==============================] - 0s 14ms/step - loss: 16.3604 - mae: 16.8490 Epoch 111/500 30/30 [==============================] - 0s 15ms/step - loss: 16.2226 - mae: 16.7111 Epoch 112/500 30/30 [==============================] - 0s 14ms/step - loss: 16.2326 - mae: 16.7211 Epoch 113/500 30/30 [==============================] - 0s 14ms/step - loss: 16.3609 - mae: 16.8502 Epoch 114/500 30/30 [==============================] - 0s 14ms/step - loss: 16.2546 - mae: 16.7434 Epoch 115/500 30/30 [==============================] - 0s 14ms/step - loss: 16.2950 - mae: 16.7835 Epoch 116/500 30/30 [==============================] - 0s 14ms/step - loss: 16.3529 - mae: 16.8417 Epoch 117/500 30/30 [==============================] - 0s 14ms/step - loss: 16.2277 - mae: 16.7164 Epoch 118/500 30/30 [==============================] - 0s 14ms/step - loss: 16.2188 - mae: 16.7074 Epoch 119/500 30/30 [==============================] - 0s 14ms/step - loss: 16.1949 - mae: 16.6836 Epoch 120/500 30/30 [==============================] - 0s 15ms/step - loss: 16.2557 - mae: 16.7442 Epoch 121/500 30/30 [==============================] - 0s 14ms/step - loss: 16.2540 - mae: 16.7428 Epoch 122/500 30/30 [==============================] - 0s 14ms/step - loss: 16.1959 - mae: 16.6844 Epoch 123/500 30/30 [==============================] - 0s 14ms/step - loss: 16.2733 - mae: 16.7620 Epoch 124/500 30/30 [==============================] - 0s 14ms/step - loss: 16.2494 - mae: 16.7381 Epoch 125/500 30/30 [==============================] - 0s 14ms/step - loss: 16.2215 - mae: 16.7106 Epoch 126/500 30/30 [==============================] - 0s 15ms/step - loss: 16.1868 - mae: 16.6756 Epoch 127/500 30/30 [==============================] - 0s 15ms/step - loss: 16.2651 - mae: 16.7540 Epoch 128/500 30/30 [==============================] - 0s 14ms/step - loss: 16.2318 - mae: 16.7204 Epoch 129/500 30/30 [==============================] - 0s 14ms/step - loss: 16.2032 - mae: 16.6919 Epoch 130/500 30/30 [==============================] - 0s 16ms/step - loss: 16.1210 - mae: 16.6095 Epoch 131/500 30/30 [==============================] - 1s 17ms/step - loss: 16.1581 - mae: 16.6466 Epoch 132/500 30/30 [==============================] - 0s 15ms/step - loss: 16.1799 - mae: 16.6685 Epoch 133/500 30/30 [==============================] - 1s 17ms/step - loss: 16.1099 - mae: 16.5982 Epoch 134/500 30/30 [==============================] - 0s 15ms/step - loss: 16.1523 - mae: 16.6412 Epoch 135/500 30/30 [==============================] - 0s 14ms/step - loss: 16.2892 - mae: 16.7783 Epoch 136/500 30/30 [==============================] - 1s 18ms/step - loss: 16.1824 - mae: 16.6713 Epoch 137/500 30/30 [==============================] - 0s 14ms/step - loss: 16.2042 - mae: 16.6929 Epoch 138/500 30/30 [==============================] - 0s 14ms/step - loss: 16.1300 - mae: 16.6188 Epoch 139/500 30/30 [==============================] - 0s 14ms/step - loss: 16.0768 - mae: 16.5655 Epoch 140/500 30/30 [==============================] - 0s 14ms/step - loss: 16.0720 - mae: 16.5608 Epoch 141/500 30/30 [==============================] - 0s 15ms/step - loss: 16.0639 - mae: 16.5527 Epoch 142/500 30/30 [==============================] - 0s 14ms/step - loss: 16.2686 - mae: 16.7576 Epoch 143/500 30/30 [==============================] - 0s 15ms/step - loss: 16.5888 - mae: 17.0781 Epoch 144/500 30/30 [==============================] - 0s 16ms/step - loss: 16.1501 - mae: 16.6389 Epoch 145/500 30/30 [==============================] - 0s 14ms/step - loss: 16.0998 - mae: 16.5884 Epoch 146/500 30/30 [==============================] - 0s 13ms/step - loss: 16.0652 - mae: 16.5535 Epoch 147/500 30/30 [==============================] - 0s 16ms/step - loss: 16.0644 - mae: 16.5532 Epoch 148/500 30/30 [==============================] - 1s 18ms/step - loss: 16.0389 - mae: 16.5276 Epoch 149/500 30/30 [==============================] - 0s 14ms/step - loss: 16.1162 - mae: 16.6048 Epoch 150/500 30/30 [==============================] - 0s 16ms/step - loss: 16.0238 - mae: 16.5122 Epoch 151/500 30/30 [==============================] - 0s 14ms/step - loss: 16.3710 - mae: 16.8603 Epoch 152/500 30/30 [==============================] - 0s 15ms/step - loss: 16.2191 - mae: 16.7078 Epoch 153/500 30/30 [==============================] - 0s 15ms/step - loss: 15.9968 - mae: 16.4854 Epoch 154/500 30/30 [==============================] - 0s 14ms/step - loss: 16.0367 - mae: 16.5250 Epoch 155/500 30/30 [==============================] - 0s 15ms/step - loss: 15.9479 - mae: 16.4360 Epoch 156/500 30/30 [==============================] - 0s 14ms/step - loss: 16.1149 - mae: 16.6034 Epoch 157/500 30/30 [==============================] - 0s 14ms/step - loss: 16.0225 - mae: 16.5109 Epoch 158/500 30/30 [==============================] - 1s 17ms/step - loss: 15.9680 - mae: 16.4562 Epoch 159/500 30/30 [==============================] - 0s 17ms/step - loss: 16.4934 - mae: 16.9826 Epoch 160/500 30/30 [==============================] - 0s 14ms/step - loss: 15.9927 - mae: 16.4808 Epoch 161/500 30/30 [==============================] - 0s 15ms/step - loss: 15.9507 - mae: 16.4391 Epoch 162/500 30/30 [==============================] - 1s 17ms/step - loss: 16.0650 - mae: 16.5536 Epoch 163/500 30/30 [==============================] - 0s 16ms/step - loss: 16.0183 - mae: 16.5065 Epoch 164/500 30/30 [==============================] - 0s 16ms/step - loss: 15.9695 - mae: 16.4579 Epoch 165/500 30/30 [==============================] - 0s 14ms/step - loss: 16.0168 - mae: 16.5052 Epoch 166/500 30/30 [==============================] - 0s 14ms/step - loss: 15.9412 - mae: 16.4294 Epoch 167/500 30/30 [==============================] - 0s 14ms/step - loss: 16.0221 - mae: 16.5106 Epoch 168/500 30/30 [==============================] - 0s 14ms/step - loss: 16.0135 - mae: 16.5020 Epoch 169/500 30/30 [==============================] - 0s 14ms/step - loss: 15.8922 - mae: 16.3804 Epoch 170/500 30/30 [==============================] - 0s 14ms/step - loss: 15.9840 - mae: 16.4722 Epoch 171/500 30/30 [==============================] - 0s 14ms/step - loss: 15.9054 - mae: 16.3934 Epoch 172/500 30/30 [==============================] - 0s 15ms/step - loss: 15.9427 - mae: 16.4310 Epoch 173/500 30/30 [==============================] - 0s 14ms/step - loss: 15.9119 - mae: 16.4002 Epoch 174/500 30/30 [==============================] - 0s 14ms/step - loss: 16.1226 - mae: 16.6110 Epoch 175/500 30/30 [==============================] - 0s 14ms/step - loss: 16.1785 - mae: 16.6673 Epoch 176/500 30/30 [==============================] - 0s 13ms/step - loss: 15.9484 - mae: 16.4367 Epoch 177/500 30/30 [==============================] - 0s 14ms/step - loss: 15.9025 - mae: 16.3905 Epoch 178/500 30/30 [==============================] - 0s 14ms/step - loss: 15.9317 - mae: 16.4202 Epoch 179/500 30/30 [==============================] - 0s 14ms/step - loss: 15.9629 - mae: 16.4511 Epoch 180/500 30/30 [==============================] - 0s 14ms/step - loss: 15.9972 - mae: 16.4859 Epoch 181/500 30/30 [==============================] - 0s 14ms/step - loss: 16.0321 - mae: 16.5207 Epoch 182/500 30/30 [==============================] - 0s 15ms/step - loss: 16.1572 - mae: 16.6460 Epoch 183/500 30/30 [==============================] - 0s 14ms/step - loss: 15.8769 - mae: 16.3651 Epoch 184/500 30/30 [==============================] - 0s 15ms/step - loss: 15.8820 - mae: 16.3701 Epoch 185/500 30/30 [==============================] - 0s 14ms/step - loss: 16.3207 - mae: 16.8093 Epoch 186/500 30/30 [==============================] - 0s 14ms/step - loss: 15.8750 - mae: 16.3633 Epoch 187/500 30/30 [==============================] - 0s 14ms/step - loss: 16.0028 - mae: 16.4915 Epoch 188/500 30/30 [==============================] - 0s 14ms/step - loss: 16.0252 - mae: 16.5137 Epoch 189/500 30/30 [==============================] - 0s 14ms/step - loss: 15.8437 - mae: 16.3319 Epoch 190/500 30/30 [==============================] - 0s 14ms/step - loss: 15.8724 - mae: 16.3606 Epoch 191/500 30/30 [==============================] - 0s 14ms/step - loss: 15.8171 - mae: 16.3053 Epoch 192/500 30/30 [==============================] - 0s 16ms/step - loss: 15.9633 - mae: 16.4516 Epoch 193/500 30/30 [==============================] - 1s 18ms/step - loss: 15.8101 - mae: 16.2980 Epoch 194/500 30/30 [==============================] - 1s 18ms/step - loss: 15.7552 - mae: 16.2428 Epoch 195/500 30/30 [==============================] - 0s 15ms/step - loss: 15.7855 - mae: 16.2734 Epoch 196/500 30/30 [==============================] - 0s 15ms/step - loss: 15.9667 - mae: 16.4550 Epoch 197/500 30/30 [==============================] - 0s 15ms/step - loss: 15.7721 - mae: 16.2598 Epoch 198/500 30/30 [==============================] - 0s 16ms/step - loss: 15.7905 - mae: 16.2785 Epoch 199/500 30/30 [==============================] - 1s 17ms/step - loss: 15.7814 - mae: 16.2695 Epoch 200/500 30/30 [==============================] - 0s 16ms/step - loss: 15.8151 - mae: 16.3030 Epoch 201/500 30/30 [==============================] - 0s 15ms/step - loss: 16.0109 - mae: 16.4994 Epoch 202/500 30/30 [==============================] - 0s 13ms/step - loss: 16.2435 - mae: 16.7325 Epoch 203/500 30/30 [==============================] - 0s 15ms/step - loss: 15.9296 - mae: 16.4180 Epoch 204/500 30/30 [==============================] - 0s 15ms/step - loss: 15.7759 - mae: 16.2640 Epoch 205/500 30/30 [==============================] - 0s 15ms/step - loss: 15.8343 - mae: 16.3225 Epoch 206/500 30/30 [==============================] - 0s 15ms/step - loss: 15.7869 - mae: 16.2749 Epoch 207/500 30/30 [==============================] - 0s 15ms/step - loss: 15.8904 - mae: 16.3786 Epoch 208/500 30/30 [==============================] - 0s 16ms/step - loss: 15.7712 - mae: 16.2591 Epoch 209/500 30/30 [==============================] - 1s 20ms/step - loss: 15.7325 - mae: 16.2202 Epoch 210/500 30/30 [==============================] - 0s 16ms/step - loss: 16.0256 - mae: 16.5142 Epoch 211/500 30/30 [==============================] - 0s 14ms/step - loss: 16.0629 - mae: 16.5517 Epoch 212/500 30/30 [==============================] - 0s 14ms/step - loss: 15.7444 - mae: 16.2323 Epoch 213/500 30/30 [==============================] - 0s 14ms/step - loss: 15.8732 - mae: 16.3613 Epoch 214/500 30/30 [==============================] - 0s 14ms/step - loss: 15.7050 - mae: 16.1929 Epoch 215/500 30/30 [==============================] - 0s 16ms/step - loss: 15.7031 - mae: 16.1909 Epoch 216/500 30/30 [==============================] - 1s 18ms/step - loss: 15.8182 - mae: 16.3062 Epoch 217/500 30/30 [==============================] - 0s 15ms/step - loss: 15.6561 - mae: 16.1438 Epoch 218/500 30/30 [==============================] - 0s 16ms/step - loss: 15.6673 - mae: 16.1550 Epoch 219/500 30/30 [==============================] - 1s 19ms/step - loss: 15.7449 - mae: 16.2332 Epoch 220/500 30/30 [==============================] - 0s 14ms/step - loss: 15.7299 - mae: 16.2177 Epoch 221/500 30/30 [==============================] - 0s 16ms/step - loss: 15.8053 - mae: 16.2936 Epoch 222/500 30/30 [==============================] - 0s 15ms/step - loss: 15.7894 - mae: 16.2776 Epoch 223/500 30/30 [==============================] - 0s 15ms/step - loss: 15.8301 - mae: 16.3182 Epoch 224/500 30/30 [==============================] - 0s 14ms/step - loss: 15.7778 - mae: 16.2661 Epoch 225/500 30/30 [==============================] - 0s 15ms/step - loss: 15.6241 - mae: 16.1117 Epoch 226/500 30/30 [==============================] - 0s 14ms/step - loss: 15.6527 - mae: 16.1405 Epoch 227/500 30/30 [==============================] - 1s 17ms/step - loss: 15.6845 - mae: 16.1722 Epoch 228/500 30/30 [==============================] - 0s 15ms/step - loss: 15.7902 - mae: 16.2785 Epoch 229/500 30/30 [==============================] - 0s 15ms/step - loss: 15.8082 - mae: 16.2962 Epoch 230/500 30/30 [==============================] - 0s 15ms/step - loss: 15.6261 - mae: 16.1139 Epoch 231/500 30/30 [==============================] - 0s 14ms/step - loss: 15.6192 - mae: 16.1068 Epoch 232/500 30/30 [==============================] - 0s 16ms/step - loss: 15.6034 - mae: 16.0912 Epoch 233/500 30/30 [==============================] - 0s 16ms/step - loss: 15.5755 - mae: 16.0633 Epoch 234/500 30/30 [==============================] - 0s 14ms/step - loss: 15.7108 - mae: 16.1984 Epoch 235/500 30/30 [==============================] - 0s 15ms/step - loss: 15.6383 - mae: 16.1264 Epoch 236/500 30/30 [==============================] - 0s 14ms/step - loss: 15.7013 - mae: 16.1889 Epoch 237/500 30/30 [==============================] - 0s 15ms/step - loss: 15.9201 - mae: 16.4087 Epoch 238/500 30/30 [==============================] - 0s 14ms/step - loss: 15.6194 - mae: 16.1070 Epoch 239/500 30/30 [==============================] - 0s 16ms/step - loss: 16.1093 - mae: 16.5979 Epoch 240/500 30/30 [==============================] - 0s 15ms/step - loss: 16.0429 - mae: 16.5318 Epoch 241/500 30/30 [==============================] - 1s 17ms/step - loss: 15.5977 - mae: 16.0856 Epoch 242/500 30/30 [==============================] - 0s 14ms/step - loss: 15.6842 - mae: 16.1722 Epoch 243/500 30/30 [==============================] - 0s 14ms/step - loss: 15.6201 - mae: 16.1081 Epoch 244/500 30/30 [==============================] - 0s 14ms/step - loss: 15.5879 - mae: 16.0757 Epoch 245/500 30/30 [==============================] - 0s 14ms/step - loss: 15.6471 - mae: 16.1352 Epoch 246/500 30/30 [==============================] - 0s 13ms/step - loss: 15.5125 - mae: 15.9998 Epoch 247/500 30/30 [==============================] - 0s 14ms/step - loss: 15.5122 - mae: 15.9999 Epoch 248/500 30/30 [==============================] - 0s 14ms/step - loss: 15.6258 - mae: 16.1138 Epoch 249/500 30/30 [==============================] - 0s 14ms/step - loss: 16.3115 - mae: 16.8002 Epoch 250/500 30/30 [==============================] - 0s 14ms/step - loss: 15.6440 - mae: 16.1321 Epoch 251/500 30/30 [==============================] - 0s 14ms/step - loss: 15.5283 - mae: 16.0154 Epoch 252/500 30/30 [==============================] - 0s 14ms/step - loss: 15.7157 - mae: 16.2039 Epoch 253/500 30/30 [==============================] - 0s 14ms/step - loss: 15.5189 - mae: 16.0065 Epoch 254/500 30/30 [==============================] - 0s 16ms/step - loss: 15.5357 - mae: 16.0235 Epoch 255/500 30/30 [==============================] - 1s 17ms/step - loss: 15.6168 - mae: 16.1043 Epoch 256/500 30/30 [==============================] - 1s 17ms/step - loss: 15.5067 - mae: 15.9946 Epoch 257/500 30/30 [==============================] - 1s 17ms/step - loss: 15.4329 - mae: 15.9205 Epoch 258/500 30/30 [==============================] - 0s 15ms/step - loss: 15.4566 - mae: 15.9441 Epoch 259/500 30/30 [==============================] - 0s 14ms/step - loss: 15.4590 - mae: 15.9463 Epoch 260/500 30/30 [==============================] - 0s 14ms/step - loss: 15.4482 - mae: 15.9357 Epoch 261/500 30/30 [==============================] - 0s 13ms/step - loss: 15.4836 - mae: 15.9712 Epoch 262/500 30/30 [==============================] - 0s 14ms/step - loss: 15.4226 - mae: 15.9103 Epoch 263/500 30/30 [==============================] - 0s 14ms/step - loss: 15.5171 - mae: 16.0046 Epoch 264/500 30/30 [==============================] - 0s 14ms/step - loss: 15.4175 - mae: 15.9046 Epoch 265/500 30/30 [==============================] - 0s 14ms/step - loss: 15.4536 - mae: 15.9416 Epoch 266/500 30/30 [==============================] - 0s 14ms/step - loss: 15.5049 - mae: 15.9927 Epoch 267/500 30/30 [==============================] - 0s 14ms/step - loss: 15.4413 - mae: 15.9287 Epoch 268/500 30/30 [==============================] - 0s 16ms/step - loss: 15.4964 - mae: 15.9838 Epoch 269/500 30/30 [==============================] - 0s 14ms/step - loss: 15.3675 - mae: 15.8547 Epoch 270/500 30/30 [==============================] - 0s 14ms/step - loss: 15.4797 - mae: 15.9677 Epoch 271/500 30/30 [==============================] - 0s 16ms/step - loss: 15.4602 - mae: 15.9476 Epoch 272/500 30/30 [==============================] - 0s 15ms/step - loss: 15.3877 - mae: 15.8752 Epoch 273/500 30/30 [==============================] - 1s 18ms/step - loss: 15.3989 - mae: 15.8862 Epoch 274/500 30/30 [==============================] - 1s 17ms/step - loss: 15.3990 - mae: 15.8861 Epoch 275/500 30/30 [==============================] - 0s 15ms/step - loss: 15.3618 - mae: 15.8492 Epoch 276/500 30/30 [==============================] - 0s 14ms/step - loss: 15.4279 - mae: 15.9153 Epoch 277/500 30/30 [==============================] - 0s 14ms/step - loss: 15.3635 - mae: 15.8505 Epoch 278/500 30/30 [==============================] - 0s 14ms/step - loss: 15.5433 - mae: 16.0306 Epoch 279/500 30/30 [==============================] - 0s 14ms/step - loss: 15.3391 - mae: 15.8264 Epoch 280/500 30/30 [==============================] - 0s 14ms/step - loss: 15.4045 - mae: 15.8917 Epoch 281/500 30/30 [==============================] - 0s 14ms/step - loss: 15.3967 - mae: 15.8840 Epoch 282/500 30/30 [==============================] - 0s 14ms/step - loss: 15.4836 - mae: 15.9712 Epoch 283/500 30/30 [==============================] - 0s 14ms/step - loss: 15.3408 - mae: 15.8279 Epoch 284/500 30/30 [==============================] - 0s 15ms/step - loss: 15.3785 - mae: 15.8656 Epoch 285/500 30/30 [==============================] - 1s 17ms/step - loss: 15.3433 - mae: 15.8301 Epoch 286/500 30/30 [==============================] - 0s 15ms/step - loss: 15.5490 - mae: 16.0366 Epoch 287/500 30/30 [==============================] - 0s 14ms/step - loss: 15.3053 - mae: 15.7924 Epoch 288/500 30/30 [==============================] - 0s 14ms/step - loss: 15.3722 - mae: 15.8593 Epoch 289/500 30/30 [==============================] - 0s 15ms/step - loss: 15.2896 - mae: 15.7763 Epoch 290/500 30/30 [==============================] - 1s 17ms/step - loss: 15.3352 - mae: 15.8223 Epoch 291/500 30/30 [==============================] - 1s 17ms/step - loss: 15.4112 - mae: 15.8982 Epoch 292/500 30/30 [==============================] - 0s 16ms/step - loss: 15.2988 - mae: 15.7858 Epoch 293/500 30/30 [==============================] - 0s 14ms/step - loss: 15.3388 - mae: 15.8259 Epoch 294/500 30/30 [==============================] - 0s 14ms/step - loss: 15.3062 - mae: 15.7929 Epoch 295/500 30/30 [==============================] - 0s 14ms/step - loss: 15.4105 - mae: 15.8977 Epoch 296/500 30/30 [==============================] - 0s 16ms/step - loss: 15.2703 - mae: 15.7570 Epoch 297/500 30/30 [==============================] - 1s 17ms/step - loss: 15.4421 - mae: 15.9292 Epoch 298/500 30/30 [==============================] - 1s 17ms/step - loss: 15.9984 - mae: 16.4868 Epoch 299/500 30/30 [==============================] - 0s 14ms/step - loss: 15.3209 - mae: 15.8076 Epoch 300/500 30/30 [==============================] - 1s 18ms/step - loss: 15.2969 - mae: 15.7837 Epoch 301/500 30/30 [==============================] - 0s 15ms/step - loss: 15.3053 - mae: 15.7922 Epoch 302/500 30/30 [==============================] - 0s 17ms/step - loss: 15.3504 - mae: 15.8375 Epoch 303/500 30/30 [==============================] - 0s 14ms/step - loss: 15.2900 - mae: 15.7768 Epoch 304/500 30/30 [==============================] - 0s 14ms/step - loss: 15.3542 - mae: 15.8414 Epoch 305/500 30/30 [==============================] - 0s 14ms/step - loss: 15.1981 - mae: 15.6845 Epoch 306/500 30/30 [==============================] - 0s 14ms/step - loss: 15.2415 - mae: 15.7282 Epoch 307/500 30/30 [==============================] - 0s 14ms/step - loss: 15.4262 - mae: 15.9137 Epoch 308/500 30/30 [==============================] - 0s 15ms/step - loss: 15.2325 - mae: 15.7189 Epoch 309/500 30/30 [==============================] - 0s 14ms/step - loss: 15.5058 - mae: 15.9931 Epoch 310/500 30/30 [==============================] - 1s 18ms/step - loss: 15.3972 - mae: 15.8843 Epoch 311/500 30/30 [==============================] - 0s 15ms/step - loss: 15.3948 - mae: 15.8822 Epoch 312/500 30/30 [==============================] - 1s 17ms/step - loss: 15.3015 - mae: 15.7884 Epoch 313/500 30/30 [==============================] - 0s 15ms/step - loss: 15.3304 - mae: 15.8171 Epoch 314/500 30/30 [==============================] - 0s 15ms/step - loss: 15.2263 - mae: 15.7129 Epoch 315/500 30/30 [==============================] - 0s 14ms/step - loss: 15.1832 - mae: 15.6693 Epoch 316/500 30/30 [==============================] - 0s 14ms/step - loss: 15.1867 - mae: 15.6732 Epoch 317/500 30/30 [==============================] - 0s 14ms/step - loss: 15.1514 - mae: 15.6378 Epoch 318/500 30/30 [==============================] - 0s 16ms/step - loss: 15.3113 - mae: 15.7983 Epoch 319/500 30/30 [==============================] - 1s 17ms/step - loss: 15.6725 - mae: 16.1602 Epoch 320/500 30/30 [==============================] - 0s 14ms/step - loss: 15.1930 - mae: 15.6793 Epoch 321/500 30/30 [==============================] - 0s 15ms/step - loss: 15.1704 - mae: 15.6564 Epoch 322/500 30/30 [==============================] - 0s 15ms/step - loss: 15.5024 - mae: 15.9902 Epoch 323/500 30/30 [==============================] - 0s 15ms/step - loss: 15.2420 - mae: 15.7288 Epoch 324/500 30/30 [==============================] - 0s 13ms/step - loss: 15.1451 - mae: 15.6314 Epoch 325/500 30/30 [==============================] - 0s 14ms/step - loss: 15.3194 - mae: 15.8064 Epoch 326/500 30/30 [==============================] - 0s 14ms/step - loss: 15.1986 - mae: 15.6848 Epoch 327/500 30/30 [==============================] - 0s 14ms/step - loss: 15.1682 - mae: 15.6546 Epoch 328/500 30/30 [==============================] - 0s 14ms/step - loss: 15.1894 - mae: 15.6759 Epoch 329/500 30/30 [==============================] - 0s 14ms/step - loss: 15.2311 - mae: 15.7175 Epoch 330/500 30/30 [==============================] - 0s 14ms/step - loss: 15.2767 - mae: 15.7634 Epoch 331/500 30/30 [==============================] - 0s 13ms/step - loss: 15.1838 - mae: 15.6702 Epoch 332/500 30/30 [==============================] - 0s 14ms/step - loss: 15.3262 - mae: 15.8128 Epoch 333/500 30/30 [==============================] - 0s 14ms/step - loss: 15.1611 - mae: 15.6471 Epoch 334/500 30/30 [==============================] - 0s 16ms/step - loss: 15.1194 - mae: 15.6057 Epoch 335/500 30/30 [==============================] - 1s 18ms/step - loss: 15.1417 - mae: 15.6280 Epoch 336/500 30/30 [==============================] - 1s 17ms/step - loss: 15.1291 - mae: 15.6155 Epoch 337/500 30/30 [==============================] - 1s 18ms/step - loss: 15.1139 - mae: 15.6003 Epoch 338/500 30/30 [==============================] - 1s 18ms/step - loss: 15.1558 - mae: 15.6418 Epoch 339/500 30/30 [==============================] - 0s 14ms/step - loss: 15.0973 - mae: 15.5834 Epoch 340/500 30/30 [==============================] - 0s 16ms/step - loss: 15.0798 - mae: 15.5663 Epoch 341/500 30/30 [==============================] - 1s 18ms/step - loss: 15.1344 - mae: 15.6208 Epoch 342/500 30/30 [==============================] - 0s 16ms/step - loss: 15.0556 - mae: 15.5419 Epoch 343/500 30/30 [==============================] - 1s 17ms/step - loss: 15.1649 - mae: 15.6512 Epoch 344/500 30/30 [==============================] - 0s 14ms/step - loss: 15.3677 - mae: 15.8548 Epoch 345/500 30/30 [==============================] - 0s 15ms/step - loss: 15.4797 - mae: 15.9666 Epoch 346/500 30/30 [==============================] - 0s 14ms/step - loss: 15.0599 - mae: 15.5458 Epoch 347/500 30/30 [==============================] - 0s 14ms/step - loss: 15.1366 - mae: 15.6227 Epoch 348/500 30/30 [==============================] - 0s 14ms/step - loss: 15.2901 - mae: 15.7771 Epoch 349/500 30/30 [==============================] - 0s 14ms/step - loss: 15.0421 - mae: 15.5281 Epoch 350/500 30/30 [==============================] - 0s 14ms/step - loss: 15.0406 - mae: 15.5271 Epoch 351/500 30/30 [==============================] - 0s 13ms/step - loss: 15.0782 - mae: 15.5643 Epoch 352/500 30/30 [==============================] - 0s 14ms/step - loss: 15.0080 - mae: 15.4939 Epoch 353/500 30/30 [==============================] - 0s 14ms/step - loss: 15.0709 - mae: 15.5567 Epoch 354/500 30/30 [==============================] - 0s 14ms/step - loss: 15.0508 - mae: 15.5368 Epoch 355/500 30/30 [==============================] - 0s 14ms/step - loss: 15.0239 - mae: 15.5098 Epoch 356/500 30/30 [==============================] - 0s 15ms/step - loss: 14.9594 - mae: 15.4451 Epoch 357/500 30/30 [==============================] - 0s 14ms/step - loss: 15.0580 - mae: 15.5441 Epoch 358/500 30/30 [==============================] - 0s 14ms/step - loss: 15.2814 - mae: 15.7687 Epoch 359/500 30/30 [==============================] - 0s 13ms/step - loss: 15.0490 - mae: 15.5351 Epoch 360/500 30/30 [==============================] - 0s 15ms/step - loss: 15.0498 - mae: 15.5365 Epoch 361/500 30/30 [==============================] - 0s 14ms/step - loss: 15.0915 - mae: 15.5779 Epoch 362/500 30/30 [==============================] - 0s 15ms/step - loss: 15.2306 - mae: 15.7175 Epoch 363/500 30/30 [==============================] - 0s 14ms/step - loss: 15.3104 - mae: 15.7972 Epoch 364/500 30/30 [==============================] - 0s 14ms/step - loss: 15.0009 - mae: 15.4870 Epoch 365/500 30/30 [==============================] - 0s 14ms/step - loss: 14.9399 - mae: 15.4251 Epoch 366/500 30/30 [==============================] - 0s 14ms/step - loss: 14.9447 - mae: 15.4307 Epoch 367/500 30/30 [==============================] - 0s 16ms/step - loss: 15.1911 - mae: 15.6779 Epoch 368/500 30/30 [==============================] - 0s 14ms/step - loss: 15.0051 - mae: 15.4914 Epoch 369/500 30/30 [==============================] - 0s 15ms/step - loss: 15.0552 - mae: 15.5416 Epoch 370/500 30/30 [==============================] - 0s 14ms/step - loss: 15.0436 - mae: 15.5295 Epoch 371/500 30/30 [==============================] - 0s 14ms/step - loss: 15.0175 - mae: 15.5037 Epoch 372/500 30/30 [==============================] - 0s 15ms/step - loss: 14.9911 - mae: 15.4772 Epoch 373/500 30/30 [==============================] - 0s 15ms/step - loss: 14.9347 - mae: 15.4203 Epoch 374/500 30/30 [==============================] - 0s 14ms/step - loss: 15.0221 - mae: 15.5081 Epoch 375/500 30/30 [==============================] - 1s 18ms/step - loss: 14.9755 - mae: 15.4615 Epoch 376/500 30/30 [==============================] - 1s 17ms/step - loss: 14.9219 - mae: 15.4074 Epoch 377/500 30/30 [==============================] - 1s 17ms/step - loss: 14.9700 - mae: 15.4562 Epoch 378/500 30/30 [==============================] - 0s 14ms/step - loss: 15.1166 - mae: 15.6031 Epoch 379/500 30/30 [==============================] - 0s 14ms/step - loss: 15.1503 - mae: 15.6369 Epoch 380/500 30/30 [==============================] - 0s 14ms/step - loss: 14.8959 - mae: 15.3815 Epoch 381/500 30/30 [==============================] - 0s 15ms/step - loss: 14.9333 - mae: 15.4189 Epoch 382/500 30/30 [==============================] - 0s 14ms/step - loss: 15.2071 - mae: 15.6942 Epoch 383/500 30/30 [==============================] - 0s 14ms/step - loss: 14.8798 - mae: 15.3658 Epoch 384/500 30/30 [==============================] - 0s 14ms/step - loss: 14.9031 - mae: 15.3885 Epoch 385/500 30/30 [==============================] - 1s 18ms/step - loss: 14.9718 - mae: 15.4579 Epoch 386/500 30/30 [==============================] - 0s 14ms/step - loss: 15.0478 - mae: 15.5342 Epoch 387/500 30/30 [==============================] - 0s 14ms/step - loss: 14.9848 - mae: 15.4712 Epoch 388/500 30/30 [==============================] - 0s 15ms/step - loss: 14.9270 - mae: 15.4128 Epoch 389/500 30/30 [==============================] - 0s 17ms/step - loss: 14.8695 - mae: 15.3551 Epoch 390/500 30/30 [==============================] - 1s 18ms/step - loss: 15.4171 - mae: 15.9042 Epoch 391/500 30/30 [==============================] - 0s 15ms/step - loss: 15.0925 - mae: 15.5791 Epoch 392/500 30/30 [==============================] - 0s 14ms/step - loss: 14.9192 - mae: 15.4048 Epoch 393/500 30/30 [==============================] - 0s 14ms/step - loss: 15.5875 - mae: 16.0752 Epoch 394/500 30/30 [==============================] - 0s 14ms/step - loss: 15.1156 - mae: 15.6018 Epoch 395/500 30/30 [==============================] - 0s 14ms/step - loss: 14.8878 - mae: 15.3734 Epoch 396/500 30/30 [==============================] - 0s 14ms/step - loss: 15.1636 - mae: 15.6498 Epoch 397/500 30/30 [==============================] - 0s 14ms/step - loss: 14.8613 - mae: 15.3466 Epoch 398/500 30/30 [==============================] - 0s 14ms/step - loss: 15.0361 - mae: 15.5222 Epoch 399/500 30/30 [==============================] - 0s 14ms/step - loss: 14.8893 - mae: 15.3750 Epoch 400/500 30/30 [==============================] - 0s 15ms/step - loss: 14.8472 - mae: 15.3328 Epoch 401/500 30/30 [==============================] - 0s 13ms/step - loss: 15.6436 - mae: 16.1307 Epoch 402/500 30/30 [==============================] - 0s 16ms/step - loss: 15.9384 - mae: 16.4261 Epoch 403/500 30/30 [==============================] - 0s 14ms/step - loss: 14.9776 - mae: 15.4637 Epoch 404/500 30/30 [==============================] - 0s 14ms/step - loss: 15.2612 - mae: 15.7481 Epoch 405/500 30/30 [==============================] - 0s 14ms/step - loss: 14.8092 - mae: 15.2946 Epoch 406/500 30/30 [==============================] - 0s 14ms/step - loss: 14.8157 - mae: 15.3010 Epoch 407/500 30/30 [==============================] - 0s 15ms/step - loss: 14.8365 - mae: 15.3220 Epoch 408/500 30/30 [==============================] - 0s 14ms/step - loss: 14.7986 - mae: 15.2840 Epoch 409/500 30/30 [==============================] - 0s 15ms/step - loss: 14.7763 - mae: 15.2610 Epoch 410/500 30/30 [==============================] - 0s 15ms/step - loss: 14.8700 - mae: 15.3556 Epoch 411/500 30/30 [==============================] - 0s 14ms/step - loss: 15.0433 - mae: 15.5297 Epoch 412/500 30/30 [==============================] - 1s 17ms/step - loss: 14.8291 - mae: 15.3144 Epoch 413/500 30/30 [==============================] - 0s 14ms/step - loss: 14.7543 - mae: 15.2395 Epoch 414/500 30/30 [==============================] - 0s 14ms/step - loss: 14.8680 - mae: 15.3535 Epoch 415/500 30/30 [==============================] - 0s 14ms/step - loss: 14.9279 - mae: 15.4138 Epoch 416/500 30/30 [==============================] - 0s 15ms/step - loss: 15.0915 - mae: 15.5782 Epoch 417/500 30/30 [==============================] - 0s 15ms/step - loss: 14.8199 - mae: 15.3061 Epoch 418/500 30/30 [==============================] - 0s 14ms/step - loss: 14.8473 - mae: 15.3334 Epoch 419/500 30/30 [==============================] - 0s 14ms/step - loss: 14.7691 - mae: 15.2549 Epoch 420/500 30/30 [==============================] - 0s 14ms/step - loss: 14.8012 - mae: 15.2871 Epoch 421/500 30/30 [==============================] - 0s 14ms/step - loss: 14.8548 - mae: 15.3404 Epoch 422/500 30/30 [==============================] - 0s 15ms/step - loss: 14.8743 - mae: 15.3599 Epoch 423/500 30/30 [==============================] - 0s 14ms/step - loss: 15.0132 - mae: 15.4996 Epoch 424/500 30/30 [==============================] - 0s 14ms/step - loss: 14.7946 - mae: 15.2807 Epoch 425/500 30/30 [==============================] - 0s 14ms/step - loss: 14.6989 - mae: 15.1838 Epoch 426/500 30/30 [==============================] - 0s 14ms/step - loss: 14.7783 - mae: 15.2639 Epoch 427/500 30/30 [==============================] - 0s 16ms/step - loss: 14.8919 - mae: 15.3780 Epoch 428/500 30/30 [==============================] - 1s 18ms/step - loss: 14.9089 - mae: 15.3947 Epoch 429/500 30/30 [==============================] - 1s 18ms/step - loss: 14.8476 - mae: 15.3332 Epoch 430/500 30/30 [==============================] - 0s 14ms/step - loss: 14.7627 - mae: 15.2484 Epoch 431/500 30/30 [==============================] - 0s 14ms/step - loss: 14.8583 - mae: 15.3440 Epoch 432/500 30/30 [==============================] - 0s 14ms/step - loss: 14.7538 - mae: 15.2391 Epoch 433/500 30/30 [==============================] - 0s 15ms/step - loss: 14.7444 - mae: 15.2301 Epoch 434/500 30/30 [==============================] - 0s 15ms/step - loss: 14.7706 - mae: 15.2559 Epoch 435/500 30/30 [==============================] - 0s 14ms/step - loss: 14.9036 - mae: 15.3899 Epoch 436/500 30/30 [==============================] - 0s 14ms/step - loss: 14.6807 - mae: 15.1661 Epoch 437/500 30/30 [==============================] - 0s 14ms/step - loss: 14.9748 - mae: 15.4613 Epoch 438/500 30/30 [==============================] - 0s 14ms/step - loss: 14.8981 - mae: 15.3843 Epoch 439/500 30/30 [==============================] - 0s 14ms/step - loss: 14.7594 - mae: 15.2446 Epoch 440/500 30/30 [==============================] - 0s 15ms/step - loss: 14.7507 - mae: 15.2364 Epoch 441/500 30/30 [==============================] - 0s 16ms/step - loss: 14.7284 - mae: 15.2135 Epoch 442/500 30/30 [==============================] - 0s 17ms/step - loss: 14.7428 - mae: 15.2281 Epoch 443/500 30/30 [==============================] - 0s 14ms/step - loss: 14.8573 - mae: 15.3436 Epoch 444/500 30/30 [==============================] - 0s 14ms/step - loss: 14.8124 - mae: 15.2985 Epoch 445/500 30/30 [==============================] - 0s 15ms/step - loss: 14.8117 - mae: 15.2975 Epoch 446/500 30/30 [==============================] - 0s 14ms/step - loss: 14.8139 - mae: 15.3003 Epoch 447/500 30/30 [==============================] - 0s 14ms/step - loss: 14.7197 - mae: 15.2053 Epoch 448/500 30/30 [==============================] - 0s 14ms/step - loss: 14.6914 - mae: 15.1769 Epoch 449/500 30/30 [==============================] - 0s 15ms/step - loss: 14.9118 - mae: 15.3977 Epoch 450/500 30/30 [==============================] - 0s 14ms/step - loss: 14.9488 - mae: 15.4352 Epoch 451/500 30/30 [==============================] - 0s 15ms/step - loss: 14.7782 - mae: 15.2642 Epoch 452/500 30/30 [==============================] - 0s 14ms/step - loss: 14.7453 - mae: 15.2312 Epoch 453/500 30/30 [==============================] - 0s 14ms/step - loss: 14.7248 - mae: 15.2105 Epoch 454/500 30/30 [==============================] - 1s 17ms/step - loss: 14.6106 - mae: 15.0958 Epoch 455/500 30/30 [==============================] - 0s 14ms/step - loss: 14.6153 - mae: 15.1010 Epoch 456/500 30/30 [==============================] - 0s 14ms/step - loss: 14.7214 - mae: 15.2072 Epoch 457/500 30/30 [==============================] - 0s 14ms/step - loss: 14.6813 - mae: 15.1670 Epoch 458/500 30/30 [==============================] - 0s 14ms/step - loss: 14.6108 - mae: 15.0960 Epoch 459/500 30/30 [==============================] - 0s 16ms/step - loss: 14.6650 - mae: 15.1502 Epoch 460/500 30/30 [==============================] - 0s 15ms/step - loss: 14.5851 - mae: 15.0707 Epoch 461/500 30/30 [==============================] - 0s 14ms/step - loss: 14.5661 - mae: 15.0515 Epoch 462/500 30/30 [==============================] - 0s 16ms/step - loss: 14.5763 - mae: 15.0615 Epoch 463/500 30/30 [==============================] - 0s 14ms/step - loss: 14.5853 - mae: 15.0706 Epoch 464/500 30/30 [==============================] - 0s 14ms/step - loss: 14.6161 - mae: 15.1017 Epoch 465/500 30/30 [==============================] - 0s 14ms/step - loss: 14.5613 - mae: 15.0463 Epoch 466/500 30/30 [==============================] - 0s 14ms/step - loss: 14.6468 - mae: 15.1325 Epoch 467/500 30/30 [==============================] - 0s 14ms/step - loss: 14.5848 - mae: 15.0699 Epoch 468/500 30/30 [==============================] - 1s 18ms/step - loss: 14.6184 - mae: 15.1037 Epoch 469/500 30/30 [==============================] - 0s 15ms/step - loss: 14.5933 - mae: 15.0786 Epoch 470/500 30/30 [==============================] - 0s 14ms/step - loss: 14.8568 - mae: 15.3430 Epoch 471/500 30/30 [==============================] - 0s 14ms/step - loss: 14.6584 - mae: 15.1439 Epoch 472/500 30/30 [==============================] - 0s 14ms/step - loss: 14.8504 - mae: 15.3366 Epoch 473/500 30/30 [==============================] - 0s 14ms/step - loss: 14.6431 - mae: 15.1293 Epoch 474/500 30/30 [==============================] - 0s 14ms/step - loss: 14.5289 - mae: 15.0141 Epoch 475/500 30/30 [==============================] - 0s 14ms/step - loss: 14.5658 - mae: 15.0509 Epoch 476/500 30/30 [==============================] - 0s 14ms/step - loss: 14.5842 - mae: 15.0696 Epoch 477/500 30/30 [==============================] - 0s 15ms/step - loss: 14.6515 - mae: 15.1372 Epoch 478/500 30/30 [==============================] - 0s 15ms/step - loss: 14.6158 - mae: 15.1013 Epoch 479/500 30/30 [==============================] - 0s 17ms/step - loss: 14.5865 - mae: 15.0718 Epoch 480/500 30/30 [==============================] - 0s 15ms/step - loss: 14.5492 - mae: 15.0345 Epoch 481/500 30/30 [==============================] - 0s 14ms/step - loss: 14.5075 - mae: 14.9931 Epoch 482/500 30/30 [==============================] - 0s 14ms/step - loss: 14.5184 - mae: 15.0038 Epoch 483/500 30/30 [==============================] - 0s 14ms/step - loss: 14.5044 - mae: 14.9896 Epoch 484/500 30/30 [==============================] - 0s 14ms/step - loss: 14.4902 - mae: 14.9754 Epoch 485/500 30/30 [==============================] - 0s 14ms/step - loss: 14.5195 - mae: 15.0049 Epoch 486/500 30/30 [==============================] - 0s 14ms/step - loss: 14.8027 - mae: 15.2887 Epoch 487/500 30/30 [==============================] - 0s 15ms/step - loss: 15.7255 - mae: 16.2135 Epoch 488/500 30/30 [==============================] - 0s 14ms/step - loss: 14.5453 - mae: 15.0308 Epoch 489/500 30/30 [==============================] - 0s 14ms/step - loss: 14.5053 - mae: 14.9908 Epoch 490/500 30/30 [==============================] - 0s 15ms/step - loss: 14.5601 - mae: 15.0457 Epoch 491/500 30/30 [==============================] - 0s 14ms/step - loss: 14.5002 - mae: 14.9853 Epoch 492/500 30/30 [==============================] - 0s 14ms/step - loss: 14.5193 - mae: 15.0045 Epoch 493/500 30/30 [==============================] - 0s 14ms/step - loss: 14.5330 - mae: 15.0182 Epoch 494/500 30/30 [==============================] - 0s 14ms/step - loss: 14.4550 - mae: 14.9402 Epoch 495/500 30/30 [==============================] - 1s 17ms/step - loss: 14.5057 - mae: 14.9907 Epoch 496/500 30/30 [==============================] - 0s 15ms/step - loss: 14.4685 - mae: 14.9537 Epoch 497/500 30/30 [==============================] - 0s 14ms/step - loss: 14.5498 - mae: 15.0353 Epoch 498/500 30/30 [==============================] - 0s 14ms/step - loss: 14.6195 - mae: 15.1050 Epoch 499/500 30/30 [==============================] - 0s 15ms/step - loss: 14.5915 - mae: 15.0775 Epoch 500/500 30/30 [==============================] - 0s 15ms/step - loss: 14.5214 - mae: 15.0068 . &lt;Figure size 432x288 with 0 Axes&gt; . &lt;Figure size 432x288 with 0 Axes&gt; . rnn_forecast = model_forecast(model, series[..., np.newaxis], window_size) rnn_forecast = rnn_forecast[split_time - window_size:-1, -1, 0] plt.figure(figsize=(10, 6)) plot_series(time_valid, x_valid) plot_series(time_valid, rnn_forecast) . tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy() . 14.773848 . import matplotlib.image as mpimg import matplotlib.pyplot as plt #-- # Retrieve a list of list results on training and test data # sets for each training epoch #-- loss=history.history[&#39;loss&#39;] epochs=range(len(loss)) # Get number of epochs # # Plot training and validation loss per epoch # plt.plot(epochs, loss, &#39;r&#39;) plt.title(&#39;Training loss&#39;) plt.xlabel(&quot;Epochs&quot;) plt.ylabel(&quot;Loss&quot;) plt.legend([&quot;Loss&quot;]) plt.figure() zoomed_loss = loss[200:] zoomed_epochs = range(200,500) # # Plot training and validation loss per epoch # plt.plot(zoomed_epochs, zoomed_loss, &#39;r&#39;) plt.title(&#39;Training loss&#39;) plt.xlabel(&quot;Epochs&quot;) plt.ylabel(&quot;Loss&quot;) plt.legend([&quot;Loss&quot;]) plt.figure() . &lt;Figure size 432x288 with 0 Axes&gt; . &lt;Figure size 432x288 with 0 Axes&gt; .",
            "url": "https://rafaelsf80.github.io/notebooks/timeseries/2020/08/04/time-series-sunspots.html",
            "relUrl": "/timeseries/2020/08/04/time-series-sunspots.html",
            "date": " • Aug 4, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Redes convolucionales imágenes de lenguaje de signos",
            "content": "Introducci&#243;n . Se resolverá la competición de Kaggle sobre imágenes de lenguaje de signos (clasificación multiclase), disponible en este enlace. El dataset de lenguajes de signos surgió como una evolución de MNIST y Fashion MNIST, pero siguiendo la misma filosofía y formato: píxeles 28x28 en blanco y negro, con 26 etiquetas (0-25) correspondiente a letras A-Z (no hay casos para 9=J 25=Z por ser movimientos). El dataset de training contiene 27,455 muestras y el de pruebas 7,172. . import csv import numpy as np import tensorflow as tf from tensorflow.keras.preprocessing.image import ImageDataGenerator from os import getcwd . # You will need to write code that will read the file passed # into this function. The first line contains the column headers # so you should ignore it # Each successive line contians 785 comma separated values between 0 and 255 # The first value is the label # The rest are the pixel values for that picture # The function will return 2 np.array types. One with all the labels # One with all the images # # Tips: # If you read a full line (as &#39;row&#39;) then row[0] has the label # and row[1:785] has the 784 pixel values # Take a look at np.array_split to turn the 784 pixels into 28x28 # You are reading in strings, but need the values to be floats # Check out np.array().astype for a conversion def get_data(filename): # np.array of shape (data length, labels &amp; pixels) my_arr = np.loadtxt(filename, delimiter=&#39;,&#39;, skiprows=1) # get label &amp; image arrays labels = my_arr[:,0].astype(&#39;int&#39;) images = my_arr[:,1:] # reshape image from 784 to (28, 28) images = images.astype(&#39;float&#39;).reshape(images.shape[0], 28, 28) # just in case to avoid memory problem my_arr = None return images, labels path_sign_mnist_train = f&quot;{getcwd()}/../tmp2/sign_mnist_train.csv&quot; path_sign_mnist_test = f&quot;{getcwd()}/../tmp2/sign_mnist_test.csv&quot; training_images, training_labels = get_data(path_sign_mnist_train) testing_images, testing_labels = get_data(path_sign_mnist_test) # Keep these print(training_images.shape) print(training_labels.shape) print(testing_images.shape) print(testing_labels.shape) # Their output should be: # (27455, 28, 28) # (27455,) # (7172, 28, 28) # (7172,) . (27455, 28, 28) (27455,) (7172, 28, 28) (7172,) . # In this section you will have to add another dimension to the data # So, for example, if your array is (10000, 28, 28) # You will need to make it (10000, 28, 28, 1) # Hint: np.expand_dims training_images = np.expand_dims(training_images, axis=-1) testing_images = np.expand_dims(testing_images, axis=-1) # Create an ImageDataGenerator and do Image Augmentation train_datagen = ImageDataGenerator( rescale=1./255, rotation_range=40, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode=&#39;nearest&#39;) validation_datagen = ImageDataGenerator( rescale=1./255) # Keep These print(training_images.shape) print(testing_images.shape) # Their output should be: # (27455, 28, 28, 1) # (7172, 28, 28, 1) . (27455, 28, 28, 1) (7172, 28, 28, 1) . # Define the model # Use no more than 2 Conv2D and 2 MaxPooling2D model = tf.keras.models.Sequential([ tf.keras.layers.Conv2D(128, (3,3), activation=&#39;relu&#39;, input_shape=(28, 28, 1)), tf.keras.layers.MaxPooling2D(2,2), tf.keras.layers.Conv2D(32, (3,3), activation=&#39;relu&#39;), tf.keras.layers.MaxPooling2D(2,2), # Flatten the results to feed into a DNN tf.keras.layers.Flatten(), # 512 neuron hidden layer tf.keras.layers.Dense(512, activation=&#39;relu&#39;), tf.keras.layers.Dense(25, activation=&#39;softmax&#39;) ]) # Compile Model. model.compile(optimizer = &#39;adam&#39;, loss = &#39;categorical_crossentropy&#39;, metrics = [&#39;accuracy&#39;]) # Train the Model from tensorflow.keras.utils import to_categorical training_labels1 = to_categorical(training_labels) testing_labels1 = to_categorical(testing_labels) train_generator = train_datagen.flow(training_images, training_labels1, batch_size=500) validation_generator = validation_datagen.flow(testing_images, testing_labels1, batch_size=500) history = model.fit_generator(train_generator, validation_data=validation_generator, steps_per_epoch=100, epochs=3, validation_steps=30, verbose=2) model.evaluate(testing_images, testing_labels1, verbose=0) . Epoch 1/3 100/100 - 33s - loss: 3.0163 - accuracy: 0.0977 - val_loss: 2.5446 - val_accuracy: 0.2448 Epoch 2/3 100/100 - 31s - loss: 2.5668 - accuracy: 0.2247 - val_loss: 1.9653 - val_accuracy: 0.4085 Epoch 3/3 100/100 - 32s - loss: 2.1926 - accuracy: 0.3254 - val_loss: 1.5454 - val_accuracy: 0.4992 . [306.7763181091751, 0.18335192] . # Plot the chart for accuracy and loss on both training and validation %matplotlib inline import matplotlib.pyplot as plt acc = history.history[&#39;accuracy&#39; ] val_acc = history.history[&#39;val_accuracy&#39; ] loss = history.history[&#39;loss&#39; ] val_loss = history.history[&#39;val_loss&#39; ] epochs = range(len(acc)) plt.plot(epochs, acc, &#39;r&#39;, label=&#39;Training accuracy&#39;) plt.plot(epochs, val_acc, &#39;b&#39;, label=&#39;Validation accuracy&#39;) plt.title(&#39;Training and validation accuracy&#39;) plt.legend() plt.figure() plt.plot(epochs, loss, &#39;r&#39;, label=&#39;Training Loss&#39;) plt.plot(epochs, val_loss, &#39;b&#39;, label=&#39;Validation Loss&#39;) plt.title(&#39;Training and validation loss&#39;) plt.legend() plt.show() .",
            "url": "https://rafaelsf80.github.io/notebooks/computer%20vision/2020/08/02/sign-language-multiclass.html",
            "relUrl": "/computer%20vision/2020/08/02/sign-language-multiclass.html",
            "date": " • Aug 2, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Redes convolucionales Humans-Horses",
            "content": "# Introducción . import os import tensorflow as tf from tensorflow.keras import layers from tensorflow.keras import Model from os import getcwd . path_inception = f&quot;{getcwd()}/../tmp2/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5&quot; # Import the inception model from tensorflow.keras.applications.inception_v3 import InceptionV3 # Create an instance of the inception model from the local pre-trained weights local_weights_file = path_inception pre_trained_model = InceptionV3(input_shape = (150, 150, 3), include_top = False, weights = None) pre_trained_model.load_weights(local_weights_file) # Make all the layers in the pre-trained model non-trainable for layer in pre_trained_model.layers: layer.trainable = False # Print the model summary pre_trained_model.summary() # Expected Output is extremely large, but should end with: #batch_normalization_v1_281 (Bat (None, 3, 3, 192) 576 conv2d_281[0][0] #__________________________________________________________________________________________________ #activation_273 (Activation) (None, 3, 3, 320) 0 batch_normalization_v1_273[0][0] #__________________________________________________________________________________________________ #mixed9_1 (Concatenate) (None, 3, 3, 768) 0 activation_275[0][0] # activation_276[0][0] #__________________________________________________________________________________________________ #concatenate_5 (Concatenate) (None, 3, 3, 768) 0 activation_279[0][0] # activation_280[0][0] #__________________________________________________________________________________________________ #activation_281 (Activation) (None, 3, 3, 192) 0 batch_normalization_v1_281[0][0] #__________________________________________________________________________________________________ #mixed10 (Concatenate) (None, 3, 3, 2048) 0 activation_273[0][0] # mixed9_1[0][0] # concatenate_5[0][0] # activation_281[0][0] #================================================================================================== #Total params: 21,802,784 #Trainable params: 0 #Non-trainable params: 21,802,784 . Model: &#34;inception_v3&#34; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_1 (InputLayer) [(None, 150, 150, 3) 0 __________________________________________________________________________________________________ conv2d (Conv2D) (None, 74, 74, 32) 864 input_1[0][0] __________________________________________________________________________________________________ batch_normalization (BatchNorma (None, 74, 74, 32) 96 conv2d[0][0] __________________________________________________________________________________________________ activation (Activation) (None, 74, 74, 32) 0 batch_normalization[0][0] __________________________________________________________________________________________________ conv2d_1 (Conv2D) (None, 72, 72, 32) 9216 activation[0][0] __________________________________________________________________________________________________ batch_normalization_1 (BatchNor (None, 72, 72, 32) 96 conv2d_1[0][0] __________________________________________________________________________________________________ activation_1 (Activation) (None, 72, 72, 32) 0 batch_normalization_1[0][0] __________________________________________________________________________________________________ conv2d_2 (Conv2D) (None, 72, 72, 64) 18432 activation_1[0][0] __________________________________________________________________________________________________ batch_normalization_2 (BatchNor (None, 72, 72, 64) 192 conv2d_2[0][0] __________________________________________________________________________________________________ activation_2 (Activation) (None, 72, 72, 64) 0 batch_normalization_2[0][0] __________________________________________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 35, 35, 64) 0 activation_2[0][0] __________________________________________________________________________________________________ conv2d_3 (Conv2D) (None, 35, 35, 80) 5120 max_pooling2d[0][0] __________________________________________________________________________________________________ batch_normalization_3 (BatchNor (None, 35, 35, 80) 240 conv2d_3[0][0] __________________________________________________________________________________________________ activation_3 (Activation) (None, 35, 35, 80) 0 batch_normalization_3[0][0] __________________________________________________________________________________________________ conv2d_4 (Conv2D) (None, 33, 33, 192) 138240 activation_3[0][0] __________________________________________________________________________________________________ batch_normalization_4 (BatchNor (None, 33, 33, 192) 576 conv2d_4[0][0] __________________________________________________________________________________________________ activation_4 (Activation) (None, 33, 33, 192) 0 batch_normalization_4[0][0] __________________________________________________________________________________________________ max_pooling2d_1 (MaxPooling2D) (None, 16, 16, 192) 0 activation_4[0][0] __________________________________________________________________________________________________ conv2d_8 (Conv2D) (None, 16, 16, 64) 12288 max_pooling2d_1[0][0] __________________________________________________________________________________________________ batch_normalization_8 (BatchNor (None, 16, 16, 64) 192 conv2d_8[0][0] __________________________________________________________________________________________________ activation_8 (Activation) (None, 16, 16, 64) 0 batch_normalization_8[0][0] __________________________________________________________________________________________________ conv2d_6 (Conv2D) (None, 16, 16, 48) 9216 max_pooling2d_1[0][0] __________________________________________________________________________________________________ conv2d_9 (Conv2D) (None, 16, 16, 96) 55296 activation_8[0][0] __________________________________________________________________________________________________ batch_normalization_6 (BatchNor (None, 16, 16, 48) 144 conv2d_6[0][0] __________________________________________________________________________________________________ batch_normalization_9 (BatchNor (None, 16, 16, 96) 288 conv2d_9[0][0] __________________________________________________________________________________________________ activation_6 (Activation) (None, 16, 16, 48) 0 batch_normalization_6[0][0] __________________________________________________________________________________________________ activation_9 (Activation) (None, 16, 16, 96) 0 batch_normalization_9[0][0] __________________________________________________________________________________________________ average_pooling2d (AveragePooli (None, 16, 16, 192) 0 max_pooling2d_1[0][0] __________________________________________________________________________________________________ conv2d_5 (Conv2D) (None, 16, 16, 64) 12288 max_pooling2d_1[0][0] __________________________________________________________________________________________________ conv2d_7 (Conv2D) (None, 16, 16, 64) 76800 activation_6[0][0] __________________________________________________________________________________________________ conv2d_10 (Conv2D) (None, 16, 16, 96) 82944 activation_9[0][0] __________________________________________________________________________________________________ conv2d_11 (Conv2D) (None, 16, 16, 32) 6144 average_pooling2d[0][0] __________________________________________________________________________________________________ batch_normalization_5 (BatchNor (None, 16, 16, 64) 192 conv2d_5[0][0] __________________________________________________________________________________________________ batch_normalization_7 (BatchNor (None, 16, 16, 64) 192 conv2d_7[0][0] __________________________________________________________________________________________________ batch_normalization_10 (BatchNo (None, 16, 16, 96) 288 conv2d_10[0][0] __________________________________________________________________________________________________ batch_normalization_11 (BatchNo (None, 16, 16, 32) 96 conv2d_11[0][0] __________________________________________________________________________________________________ activation_5 (Activation) (None, 16, 16, 64) 0 batch_normalization_5[0][0] __________________________________________________________________________________________________ activation_7 (Activation) (None, 16, 16, 64) 0 batch_normalization_7[0][0] __________________________________________________________________________________________________ activation_10 (Activation) (None, 16, 16, 96) 0 batch_normalization_10[0][0] __________________________________________________________________________________________________ activation_11 (Activation) (None, 16, 16, 32) 0 batch_normalization_11[0][0] __________________________________________________________________________________________________ mixed0 (Concatenate) (None, 16, 16, 256) 0 activation_5[0][0] activation_7[0][0] activation_10[0][0] activation_11[0][0] __________________________________________________________________________________________________ conv2d_15 (Conv2D) (None, 16, 16, 64) 16384 mixed0[0][0] __________________________________________________________________________________________________ batch_normalization_15 (BatchNo (None, 16, 16, 64) 192 conv2d_15[0][0] __________________________________________________________________________________________________ activation_15 (Activation) (None, 16, 16, 64) 0 batch_normalization_15[0][0] __________________________________________________________________________________________________ conv2d_13 (Conv2D) (None, 16, 16, 48) 12288 mixed0[0][0] __________________________________________________________________________________________________ conv2d_16 (Conv2D) (None, 16, 16, 96) 55296 activation_15[0][0] __________________________________________________________________________________________________ batch_normalization_13 (BatchNo (None, 16, 16, 48) 144 conv2d_13[0][0] __________________________________________________________________________________________________ batch_normalization_16 (BatchNo (None, 16, 16, 96) 288 conv2d_16[0][0] __________________________________________________________________________________________________ activation_13 (Activation) (None, 16, 16, 48) 0 batch_normalization_13[0][0] __________________________________________________________________________________________________ activation_16 (Activation) (None, 16, 16, 96) 0 batch_normalization_16[0][0] __________________________________________________________________________________________________ average_pooling2d_1 (AveragePoo (None, 16, 16, 256) 0 mixed0[0][0] __________________________________________________________________________________________________ conv2d_12 (Conv2D) (None, 16, 16, 64) 16384 mixed0[0][0] __________________________________________________________________________________________________ conv2d_14 (Conv2D) (None, 16, 16, 64) 76800 activation_13[0][0] __________________________________________________________________________________________________ conv2d_17 (Conv2D) (None, 16, 16, 96) 82944 activation_16[0][0] __________________________________________________________________________________________________ conv2d_18 (Conv2D) (None, 16, 16, 64) 16384 average_pooling2d_1[0][0] __________________________________________________________________________________________________ batch_normalization_12 (BatchNo (None, 16, 16, 64) 192 conv2d_12[0][0] __________________________________________________________________________________________________ batch_normalization_14 (BatchNo (None, 16, 16, 64) 192 conv2d_14[0][0] __________________________________________________________________________________________________ batch_normalization_17 (BatchNo (None, 16, 16, 96) 288 conv2d_17[0][0] __________________________________________________________________________________________________ batch_normalization_18 (BatchNo (None, 16, 16, 64) 192 conv2d_18[0][0] __________________________________________________________________________________________________ activation_12 (Activation) (None, 16, 16, 64) 0 batch_normalization_12[0][0] __________________________________________________________________________________________________ activation_14 (Activation) (None, 16, 16, 64) 0 batch_normalization_14[0][0] __________________________________________________________________________________________________ activation_17 (Activation) (None, 16, 16, 96) 0 batch_normalization_17[0][0] __________________________________________________________________________________________________ activation_18 (Activation) (None, 16, 16, 64) 0 batch_normalization_18[0][0] __________________________________________________________________________________________________ mixed1 (Concatenate) (None, 16, 16, 288) 0 activation_12[0][0] activation_14[0][0] activation_17[0][0] activation_18[0][0] __________________________________________________________________________________________________ conv2d_22 (Conv2D) (None, 16, 16, 64) 18432 mixed1[0][0] __________________________________________________________________________________________________ batch_normalization_22 (BatchNo (None, 16, 16, 64) 192 conv2d_22[0][0] __________________________________________________________________________________________________ activation_22 (Activation) (None, 16, 16, 64) 0 batch_normalization_22[0][0] __________________________________________________________________________________________________ conv2d_20 (Conv2D) (None, 16, 16, 48) 13824 mixed1[0][0] __________________________________________________________________________________________________ conv2d_23 (Conv2D) (None, 16, 16, 96) 55296 activation_22[0][0] __________________________________________________________________________________________________ batch_normalization_20 (BatchNo (None, 16, 16, 48) 144 conv2d_20[0][0] __________________________________________________________________________________________________ batch_normalization_23 (BatchNo (None, 16, 16, 96) 288 conv2d_23[0][0] __________________________________________________________________________________________________ activation_20 (Activation) (None, 16, 16, 48) 0 batch_normalization_20[0][0] __________________________________________________________________________________________________ activation_23 (Activation) (None, 16, 16, 96) 0 batch_normalization_23[0][0] __________________________________________________________________________________________________ average_pooling2d_2 (AveragePoo (None, 16, 16, 288) 0 mixed1[0][0] __________________________________________________________________________________________________ conv2d_19 (Conv2D) (None, 16, 16, 64) 18432 mixed1[0][0] __________________________________________________________________________________________________ conv2d_21 (Conv2D) (None, 16, 16, 64) 76800 activation_20[0][0] __________________________________________________________________________________________________ conv2d_24 (Conv2D) (None, 16, 16, 96) 82944 activation_23[0][0] __________________________________________________________________________________________________ conv2d_25 (Conv2D) (None, 16, 16, 64) 18432 average_pooling2d_2[0][0] __________________________________________________________________________________________________ batch_normalization_19 (BatchNo (None, 16, 16, 64) 192 conv2d_19[0][0] __________________________________________________________________________________________________ batch_normalization_21 (BatchNo (None, 16, 16, 64) 192 conv2d_21[0][0] __________________________________________________________________________________________________ batch_normalization_24 (BatchNo (None, 16, 16, 96) 288 conv2d_24[0][0] __________________________________________________________________________________________________ batch_normalization_25 (BatchNo (None, 16, 16, 64) 192 conv2d_25[0][0] __________________________________________________________________________________________________ activation_19 (Activation) (None, 16, 16, 64) 0 batch_normalization_19[0][0] __________________________________________________________________________________________________ activation_21 (Activation) (None, 16, 16, 64) 0 batch_normalization_21[0][0] __________________________________________________________________________________________________ activation_24 (Activation) (None, 16, 16, 96) 0 batch_normalization_24[0][0] __________________________________________________________________________________________________ activation_25 (Activation) (None, 16, 16, 64) 0 batch_normalization_25[0][0] __________________________________________________________________________________________________ mixed2 (Concatenate) (None, 16, 16, 288) 0 activation_19[0][0] activation_21[0][0] activation_24[0][0] activation_25[0][0] __________________________________________________________________________________________________ conv2d_27 (Conv2D) (None, 16, 16, 64) 18432 mixed2[0][0] __________________________________________________________________________________________________ batch_normalization_27 (BatchNo (None, 16, 16, 64) 192 conv2d_27[0][0] __________________________________________________________________________________________________ activation_27 (Activation) (None, 16, 16, 64) 0 batch_normalization_27[0][0] __________________________________________________________________________________________________ conv2d_28 (Conv2D) (None, 16, 16, 96) 55296 activation_27[0][0] __________________________________________________________________________________________________ batch_normalization_28 (BatchNo (None, 16, 16, 96) 288 conv2d_28[0][0] __________________________________________________________________________________________________ activation_28 (Activation) (None, 16, 16, 96) 0 batch_normalization_28[0][0] __________________________________________________________________________________________________ conv2d_26 (Conv2D) (None, 7, 7, 384) 995328 mixed2[0][0] __________________________________________________________________________________________________ conv2d_29 (Conv2D) (None, 7, 7, 96) 82944 activation_28[0][0] __________________________________________________________________________________________________ batch_normalization_26 (BatchNo (None, 7, 7, 384) 1152 conv2d_26[0][0] __________________________________________________________________________________________________ batch_normalization_29 (BatchNo (None, 7, 7, 96) 288 conv2d_29[0][0] __________________________________________________________________________________________________ activation_26 (Activation) (None, 7, 7, 384) 0 batch_normalization_26[0][0] __________________________________________________________________________________________________ activation_29 (Activation) (None, 7, 7, 96) 0 batch_normalization_29[0][0] __________________________________________________________________________________________________ max_pooling2d_2 (MaxPooling2D) (None, 7, 7, 288) 0 mixed2[0][0] __________________________________________________________________________________________________ mixed3 (Concatenate) (None, 7, 7, 768) 0 activation_26[0][0] activation_29[0][0] max_pooling2d_2[0][0] __________________________________________________________________________________________________ conv2d_34 (Conv2D) (None, 7, 7, 128) 98304 mixed3[0][0] __________________________________________________________________________________________________ batch_normalization_34 (BatchNo (None, 7, 7, 128) 384 conv2d_34[0][0] __________________________________________________________________________________________________ activation_34 (Activation) (None, 7, 7, 128) 0 batch_normalization_34[0][0] __________________________________________________________________________________________________ conv2d_35 (Conv2D) (None, 7, 7, 128) 114688 activation_34[0][0] __________________________________________________________________________________________________ batch_normalization_35 (BatchNo (None, 7, 7, 128) 384 conv2d_35[0][0] __________________________________________________________________________________________________ activation_35 (Activation) (None, 7, 7, 128) 0 batch_normalization_35[0][0] __________________________________________________________________________________________________ conv2d_31 (Conv2D) (None, 7, 7, 128) 98304 mixed3[0][0] __________________________________________________________________________________________________ conv2d_36 (Conv2D) (None, 7, 7, 128) 114688 activation_35[0][0] __________________________________________________________________________________________________ batch_normalization_31 (BatchNo (None, 7, 7, 128) 384 conv2d_31[0][0] __________________________________________________________________________________________________ batch_normalization_36 (BatchNo (None, 7, 7, 128) 384 conv2d_36[0][0] __________________________________________________________________________________________________ activation_31 (Activation) (None, 7, 7, 128) 0 batch_normalization_31[0][0] __________________________________________________________________________________________________ activation_36 (Activation) (None, 7, 7, 128) 0 batch_normalization_36[0][0] __________________________________________________________________________________________________ conv2d_32 (Conv2D) (None, 7, 7, 128) 114688 activation_31[0][0] __________________________________________________________________________________________________ conv2d_37 (Conv2D) (None, 7, 7, 128) 114688 activation_36[0][0] __________________________________________________________________________________________________ batch_normalization_32 (BatchNo (None, 7, 7, 128) 384 conv2d_32[0][0] __________________________________________________________________________________________________ batch_normalization_37 (BatchNo (None, 7, 7, 128) 384 conv2d_37[0][0] __________________________________________________________________________________________________ activation_32 (Activation) (None, 7, 7, 128) 0 batch_normalization_32[0][0] __________________________________________________________________________________________________ activation_37 (Activation) (None, 7, 7, 128) 0 batch_normalization_37[0][0] __________________________________________________________________________________________________ average_pooling2d_3 (AveragePoo (None, 7, 7, 768) 0 mixed3[0][0] __________________________________________________________________________________________________ conv2d_30 (Conv2D) (None, 7, 7, 192) 147456 mixed3[0][0] __________________________________________________________________________________________________ conv2d_33 (Conv2D) (None, 7, 7, 192) 172032 activation_32[0][0] __________________________________________________________________________________________________ conv2d_38 (Conv2D) (None, 7, 7, 192) 172032 activation_37[0][0] __________________________________________________________________________________________________ conv2d_39 (Conv2D) (None, 7, 7, 192) 147456 average_pooling2d_3[0][0] __________________________________________________________________________________________________ batch_normalization_30 (BatchNo (None, 7, 7, 192) 576 conv2d_30[0][0] __________________________________________________________________________________________________ batch_normalization_33 (BatchNo (None, 7, 7, 192) 576 conv2d_33[0][0] __________________________________________________________________________________________________ batch_normalization_38 (BatchNo (None, 7, 7, 192) 576 conv2d_38[0][0] __________________________________________________________________________________________________ batch_normalization_39 (BatchNo (None, 7, 7, 192) 576 conv2d_39[0][0] __________________________________________________________________________________________________ activation_30 (Activation) (None, 7, 7, 192) 0 batch_normalization_30[0][0] __________________________________________________________________________________________________ activation_33 (Activation) (None, 7, 7, 192) 0 batch_normalization_33[0][0] __________________________________________________________________________________________________ activation_38 (Activation) (None, 7, 7, 192) 0 batch_normalization_38[0][0] __________________________________________________________________________________________________ activation_39 (Activation) (None, 7, 7, 192) 0 batch_normalization_39[0][0] __________________________________________________________________________________________________ mixed4 (Concatenate) (None, 7, 7, 768) 0 activation_30[0][0] activation_33[0][0] activation_38[0][0] activation_39[0][0] __________________________________________________________________________________________________ conv2d_44 (Conv2D) (None, 7, 7, 160) 122880 mixed4[0][0] __________________________________________________________________________________________________ batch_normalization_44 (BatchNo (None, 7, 7, 160) 480 conv2d_44[0][0] __________________________________________________________________________________________________ activation_44 (Activation) (None, 7, 7, 160) 0 batch_normalization_44[0][0] __________________________________________________________________________________________________ conv2d_45 (Conv2D) (None, 7, 7, 160) 179200 activation_44[0][0] __________________________________________________________________________________________________ batch_normalization_45 (BatchNo (None, 7, 7, 160) 480 conv2d_45[0][0] __________________________________________________________________________________________________ activation_45 (Activation) (None, 7, 7, 160) 0 batch_normalization_45[0][0] __________________________________________________________________________________________________ conv2d_41 (Conv2D) (None, 7, 7, 160) 122880 mixed4[0][0] __________________________________________________________________________________________________ conv2d_46 (Conv2D) (None, 7, 7, 160) 179200 activation_45[0][0] __________________________________________________________________________________________________ batch_normalization_41 (BatchNo (None, 7, 7, 160) 480 conv2d_41[0][0] __________________________________________________________________________________________________ batch_normalization_46 (BatchNo (None, 7, 7, 160) 480 conv2d_46[0][0] __________________________________________________________________________________________________ activation_41 (Activation) (None, 7, 7, 160) 0 batch_normalization_41[0][0] __________________________________________________________________________________________________ activation_46 (Activation) (None, 7, 7, 160) 0 batch_normalization_46[0][0] __________________________________________________________________________________________________ conv2d_42 (Conv2D) (None, 7, 7, 160) 179200 activation_41[0][0] __________________________________________________________________________________________________ conv2d_47 (Conv2D) (None, 7, 7, 160) 179200 activation_46[0][0] __________________________________________________________________________________________________ batch_normalization_42 (BatchNo (None, 7, 7, 160) 480 conv2d_42[0][0] __________________________________________________________________________________________________ batch_normalization_47 (BatchNo (None, 7, 7, 160) 480 conv2d_47[0][0] __________________________________________________________________________________________________ activation_42 (Activation) (None, 7, 7, 160) 0 batch_normalization_42[0][0] __________________________________________________________________________________________________ activation_47 (Activation) (None, 7, 7, 160) 0 batch_normalization_47[0][0] __________________________________________________________________________________________________ average_pooling2d_4 (AveragePoo (None, 7, 7, 768) 0 mixed4[0][0] __________________________________________________________________________________________________ conv2d_40 (Conv2D) (None, 7, 7, 192) 147456 mixed4[0][0] __________________________________________________________________________________________________ conv2d_43 (Conv2D) (None, 7, 7, 192) 215040 activation_42[0][0] __________________________________________________________________________________________________ conv2d_48 (Conv2D) (None, 7, 7, 192) 215040 activation_47[0][0] __________________________________________________________________________________________________ conv2d_49 (Conv2D) (None, 7, 7, 192) 147456 average_pooling2d_4[0][0] __________________________________________________________________________________________________ batch_normalization_40 (BatchNo (None, 7, 7, 192) 576 conv2d_40[0][0] __________________________________________________________________________________________________ batch_normalization_43 (BatchNo (None, 7, 7, 192) 576 conv2d_43[0][0] __________________________________________________________________________________________________ batch_normalization_48 (BatchNo (None, 7, 7, 192) 576 conv2d_48[0][0] __________________________________________________________________________________________________ batch_normalization_49 (BatchNo (None, 7, 7, 192) 576 conv2d_49[0][0] __________________________________________________________________________________________________ activation_40 (Activation) (None, 7, 7, 192) 0 batch_normalization_40[0][0] __________________________________________________________________________________________________ activation_43 (Activation) (None, 7, 7, 192) 0 batch_normalization_43[0][0] __________________________________________________________________________________________________ activation_48 (Activation) (None, 7, 7, 192) 0 batch_normalization_48[0][0] __________________________________________________________________________________________________ activation_49 (Activation) (None, 7, 7, 192) 0 batch_normalization_49[0][0] __________________________________________________________________________________________________ mixed5 (Concatenate) (None, 7, 7, 768) 0 activation_40[0][0] activation_43[0][0] activation_48[0][0] activation_49[0][0] __________________________________________________________________________________________________ conv2d_54 (Conv2D) (None, 7, 7, 160) 122880 mixed5[0][0] __________________________________________________________________________________________________ batch_normalization_54 (BatchNo (None, 7, 7, 160) 480 conv2d_54[0][0] __________________________________________________________________________________________________ activation_54 (Activation) (None, 7, 7, 160) 0 batch_normalization_54[0][0] __________________________________________________________________________________________________ conv2d_55 (Conv2D) (None, 7, 7, 160) 179200 activation_54[0][0] __________________________________________________________________________________________________ batch_normalization_55 (BatchNo (None, 7, 7, 160) 480 conv2d_55[0][0] __________________________________________________________________________________________________ activation_55 (Activation) (None, 7, 7, 160) 0 batch_normalization_55[0][0] __________________________________________________________________________________________________ conv2d_51 (Conv2D) (None, 7, 7, 160) 122880 mixed5[0][0] __________________________________________________________________________________________________ conv2d_56 (Conv2D) (None, 7, 7, 160) 179200 activation_55[0][0] __________________________________________________________________________________________________ batch_normalization_51 (BatchNo (None, 7, 7, 160) 480 conv2d_51[0][0] __________________________________________________________________________________________________ batch_normalization_56 (BatchNo (None, 7, 7, 160) 480 conv2d_56[0][0] __________________________________________________________________________________________________ activation_51 (Activation) (None, 7, 7, 160) 0 batch_normalization_51[0][0] __________________________________________________________________________________________________ activation_56 (Activation) (None, 7, 7, 160) 0 batch_normalization_56[0][0] __________________________________________________________________________________________________ conv2d_52 (Conv2D) (None, 7, 7, 160) 179200 activation_51[0][0] __________________________________________________________________________________________________ conv2d_57 (Conv2D) (None, 7, 7, 160) 179200 activation_56[0][0] __________________________________________________________________________________________________ batch_normalization_52 (BatchNo (None, 7, 7, 160) 480 conv2d_52[0][0] __________________________________________________________________________________________________ batch_normalization_57 (BatchNo (None, 7, 7, 160) 480 conv2d_57[0][0] __________________________________________________________________________________________________ activation_52 (Activation) (None, 7, 7, 160) 0 batch_normalization_52[0][0] __________________________________________________________________________________________________ activation_57 (Activation) (None, 7, 7, 160) 0 batch_normalization_57[0][0] __________________________________________________________________________________________________ average_pooling2d_5 (AveragePoo (None, 7, 7, 768) 0 mixed5[0][0] __________________________________________________________________________________________________ conv2d_50 (Conv2D) (None, 7, 7, 192) 147456 mixed5[0][0] __________________________________________________________________________________________________ conv2d_53 (Conv2D) (None, 7, 7, 192) 215040 activation_52[0][0] __________________________________________________________________________________________________ conv2d_58 (Conv2D) (None, 7, 7, 192) 215040 activation_57[0][0] __________________________________________________________________________________________________ conv2d_59 (Conv2D) (None, 7, 7, 192) 147456 average_pooling2d_5[0][0] __________________________________________________________________________________________________ batch_normalization_50 (BatchNo (None, 7, 7, 192) 576 conv2d_50[0][0] __________________________________________________________________________________________________ batch_normalization_53 (BatchNo (None, 7, 7, 192) 576 conv2d_53[0][0] __________________________________________________________________________________________________ batch_normalization_58 (BatchNo (None, 7, 7, 192) 576 conv2d_58[0][0] __________________________________________________________________________________________________ batch_normalization_59 (BatchNo (None, 7, 7, 192) 576 conv2d_59[0][0] __________________________________________________________________________________________________ activation_50 (Activation) (None, 7, 7, 192) 0 batch_normalization_50[0][0] __________________________________________________________________________________________________ activation_53 (Activation) (None, 7, 7, 192) 0 batch_normalization_53[0][0] __________________________________________________________________________________________________ activation_58 (Activation) (None, 7, 7, 192) 0 batch_normalization_58[0][0] __________________________________________________________________________________________________ activation_59 (Activation) (None, 7, 7, 192) 0 batch_normalization_59[0][0] __________________________________________________________________________________________________ mixed6 (Concatenate) (None, 7, 7, 768) 0 activation_50[0][0] activation_53[0][0] activation_58[0][0] activation_59[0][0] __________________________________________________________________________________________________ conv2d_64 (Conv2D) (None, 7, 7, 192) 147456 mixed6[0][0] __________________________________________________________________________________________________ batch_normalization_64 (BatchNo (None, 7, 7, 192) 576 conv2d_64[0][0] __________________________________________________________________________________________________ activation_64 (Activation) (None, 7, 7, 192) 0 batch_normalization_64[0][0] __________________________________________________________________________________________________ conv2d_65 (Conv2D) (None, 7, 7, 192) 258048 activation_64[0][0] __________________________________________________________________________________________________ batch_normalization_65 (BatchNo (None, 7, 7, 192) 576 conv2d_65[0][0] __________________________________________________________________________________________________ activation_65 (Activation) (None, 7, 7, 192) 0 batch_normalization_65[0][0] __________________________________________________________________________________________________ conv2d_61 (Conv2D) (None, 7, 7, 192) 147456 mixed6[0][0] __________________________________________________________________________________________________ conv2d_66 (Conv2D) (None, 7, 7, 192) 258048 activation_65[0][0] __________________________________________________________________________________________________ batch_normalization_61 (BatchNo (None, 7, 7, 192) 576 conv2d_61[0][0] __________________________________________________________________________________________________ batch_normalization_66 (BatchNo (None, 7, 7, 192) 576 conv2d_66[0][0] __________________________________________________________________________________________________ activation_61 (Activation) (None, 7, 7, 192) 0 batch_normalization_61[0][0] __________________________________________________________________________________________________ activation_66 (Activation) (None, 7, 7, 192) 0 batch_normalization_66[0][0] __________________________________________________________________________________________________ conv2d_62 (Conv2D) (None, 7, 7, 192) 258048 activation_61[0][0] __________________________________________________________________________________________________ conv2d_67 (Conv2D) (None, 7, 7, 192) 258048 activation_66[0][0] __________________________________________________________________________________________________ batch_normalization_62 (BatchNo (None, 7, 7, 192) 576 conv2d_62[0][0] __________________________________________________________________________________________________ batch_normalization_67 (BatchNo (None, 7, 7, 192) 576 conv2d_67[0][0] __________________________________________________________________________________________________ activation_62 (Activation) (None, 7, 7, 192) 0 batch_normalization_62[0][0] __________________________________________________________________________________________________ activation_67 (Activation) (None, 7, 7, 192) 0 batch_normalization_67[0][0] __________________________________________________________________________________________________ average_pooling2d_6 (AveragePoo (None, 7, 7, 768) 0 mixed6[0][0] __________________________________________________________________________________________________ conv2d_60 (Conv2D) (None, 7, 7, 192) 147456 mixed6[0][0] __________________________________________________________________________________________________ conv2d_63 (Conv2D) (None, 7, 7, 192) 258048 activation_62[0][0] __________________________________________________________________________________________________ conv2d_68 (Conv2D) (None, 7, 7, 192) 258048 activation_67[0][0] __________________________________________________________________________________________________ conv2d_69 (Conv2D) (None, 7, 7, 192) 147456 average_pooling2d_6[0][0] __________________________________________________________________________________________________ batch_normalization_60 (BatchNo (None, 7, 7, 192) 576 conv2d_60[0][0] __________________________________________________________________________________________________ batch_normalization_63 (BatchNo (None, 7, 7, 192) 576 conv2d_63[0][0] __________________________________________________________________________________________________ batch_normalization_68 (BatchNo (None, 7, 7, 192) 576 conv2d_68[0][0] __________________________________________________________________________________________________ batch_normalization_69 (BatchNo (None, 7, 7, 192) 576 conv2d_69[0][0] __________________________________________________________________________________________________ activation_60 (Activation) (None, 7, 7, 192) 0 batch_normalization_60[0][0] __________________________________________________________________________________________________ activation_63 (Activation) (None, 7, 7, 192) 0 batch_normalization_63[0][0] __________________________________________________________________________________________________ activation_68 (Activation) (None, 7, 7, 192) 0 batch_normalization_68[0][0] __________________________________________________________________________________________________ activation_69 (Activation) (None, 7, 7, 192) 0 batch_normalization_69[0][0] __________________________________________________________________________________________________ mixed7 (Concatenate) (None, 7, 7, 768) 0 activation_60[0][0] activation_63[0][0] activation_68[0][0] activation_69[0][0] __________________________________________________________________________________________________ conv2d_72 (Conv2D) (None, 7, 7, 192) 147456 mixed7[0][0] __________________________________________________________________________________________________ batch_normalization_72 (BatchNo (None, 7, 7, 192) 576 conv2d_72[0][0] __________________________________________________________________________________________________ activation_72 (Activation) (None, 7, 7, 192) 0 batch_normalization_72[0][0] __________________________________________________________________________________________________ conv2d_73 (Conv2D) (None, 7, 7, 192) 258048 activation_72[0][0] __________________________________________________________________________________________________ batch_normalization_73 (BatchNo (None, 7, 7, 192) 576 conv2d_73[0][0] __________________________________________________________________________________________________ activation_73 (Activation) (None, 7, 7, 192) 0 batch_normalization_73[0][0] __________________________________________________________________________________________________ conv2d_70 (Conv2D) (None, 7, 7, 192) 147456 mixed7[0][0] __________________________________________________________________________________________________ conv2d_74 (Conv2D) (None, 7, 7, 192) 258048 activation_73[0][0] __________________________________________________________________________________________________ batch_normalization_70 (BatchNo (None, 7, 7, 192) 576 conv2d_70[0][0] __________________________________________________________________________________________________ batch_normalization_74 (BatchNo (None, 7, 7, 192) 576 conv2d_74[0][0] __________________________________________________________________________________________________ activation_70 (Activation) (None, 7, 7, 192) 0 batch_normalization_70[0][0] __________________________________________________________________________________________________ activation_74 (Activation) (None, 7, 7, 192) 0 batch_normalization_74[0][0] __________________________________________________________________________________________________ conv2d_71 (Conv2D) (None, 3, 3, 320) 552960 activation_70[0][0] __________________________________________________________________________________________________ conv2d_75 (Conv2D) (None, 3, 3, 192) 331776 activation_74[0][0] __________________________________________________________________________________________________ batch_normalization_71 (BatchNo (None, 3, 3, 320) 960 conv2d_71[0][0] __________________________________________________________________________________________________ batch_normalization_75 (BatchNo (None, 3, 3, 192) 576 conv2d_75[0][0] __________________________________________________________________________________________________ activation_71 (Activation) (None, 3, 3, 320) 0 batch_normalization_71[0][0] __________________________________________________________________________________________________ activation_75 (Activation) (None, 3, 3, 192) 0 batch_normalization_75[0][0] __________________________________________________________________________________________________ max_pooling2d_3 (MaxPooling2D) (None, 3, 3, 768) 0 mixed7[0][0] __________________________________________________________________________________________________ mixed8 (Concatenate) (None, 3, 3, 1280) 0 activation_71[0][0] activation_75[0][0] max_pooling2d_3[0][0] __________________________________________________________________________________________________ conv2d_80 (Conv2D) (None, 3, 3, 448) 573440 mixed8[0][0] __________________________________________________________________________________________________ batch_normalization_80 (BatchNo (None, 3, 3, 448) 1344 conv2d_80[0][0] __________________________________________________________________________________________________ activation_80 (Activation) (None, 3, 3, 448) 0 batch_normalization_80[0][0] __________________________________________________________________________________________________ conv2d_77 (Conv2D) (None, 3, 3, 384) 491520 mixed8[0][0] __________________________________________________________________________________________________ conv2d_81 (Conv2D) (None, 3, 3, 384) 1548288 activation_80[0][0] __________________________________________________________________________________________________ batch_normalization_77 (BatchNo (None, 3, 3, 384) 1152 conv2d_77[0][0] __________________________________________________________________________________________________ batch_normalization_81 (BatchNo (None, 3, 3, 384) 1152 conv2d_81[0][0] __________________________________________________________________________________________________ activation_77 (Activation) (None, 3, 3, 384) 0 batch_normalization_77[0][0] __________________________________________________________________________________________________ activation_81 (Activation) (None, 3, 3, 384) 0 batch_normalization_81[0][0] __________________________________________________________________________________________________ conv2d_78 (Conv2D) (None, 3, 3, 384) 442368 activation_77[0][0] __________________________________________________________________________________________________ conv2d_79 (Conv2D) (None, 3, 3, 384) 442368 activation_77[0][0] __________________________________________________________________________________________________ conv2d_82 (Conv2D) (None, 3, 3, 384) 442368 activation_81[0][0] __________________________________________________________________________________________________ conv2d_83 (Conv2D) (None, 3, 3, 384) 442368 activation_81[0][0] __________________________________________________________________________________________________ average_pooling2d_7 (AveragePoo (None, 3, 3, 1280) 0 mixed8[0][0] __________________________________________________________________________________________________ conv2d_76 (Conv2D) (None, 3, 3, 320) 409600 mixed8[0][0] __________________________________________________________________________________________________ batch_normalization_78 (BatchNo (None, 3, 3, 384) 1152 conv2d_78[0][0] __________________________________________________________________________________________________ batch_normalization_79 (BatchNo (None, 3, 3, 384) 1152 conv2d_79[0][0] __________________________________________________________________________________________________ batch_normalization_82 (BatchNo (None, 3, 3, 384) 1152 conv2d_82[0][0] __________________________________________________________________________________________________ batch_normalization_83 (BatchNo (None, 3, 3, 384) 1152 conv2d_83[0][0] __________________________________________________________________________________________________ conv2d_84 (Conv2D) (None, 3, 3, 192) 245760 average_pooling2d_7[0][0] __________________________________________________________________________________________________ batch_normalization_76 (BatchNo (None, 3, 3, 320) 960 conv2d_76[0][0] __________________________________________________________________________________________________ activation_78 (Activation) (None, 3, 3, 384) 0 batch_normalization_78[0][0] __________________________________________________________________________________________________ activation_79 (Activation) (None, 3, 3, 384) 0 batch_normalization_79[0][0] __________________________________________________________________________________________________ activation_82 (Activation) (None, 3, 3, 384) 0 batch_normalization_82[0][0] __________________________________________________________________________________________________ activation_83 (Activation) (None, 3, 3, 384) 0 batch_normalization_83[0][0] __________________________________________________________________________________________________ batch_normalization_84 (BatchNo (None, 3, 3, 192) 576 conv2d_84[0][0] __________________________________________________________________________________________________ activation_76 (Activation) (None, 3, 3, 320) 0 batch_normalization_76[0][0] __________________________________________________________________________________________________ mixed9_0 (Concatenate) (None, 3, 3, 768) 0 activation_78[0][0] activation_79[0][0] __________________________________________________________________________________________________ concatenate (Concatenate) (None, 3, 3, 768) 0 activation_82[0][0] activation_83[0][0] __________________________________________________________________________________________________ activation_84 (Activation) (None, 3, 3, 192) 0 batch_normalization_84[0][0] __________________________________________________________________________________________________ mixed9 (Concatenate) (None, 3, 3, 2048) 0 activation_76[0][0] mixed9_0[0][0] concatenate[0][0] activation_84[0][0] __________________________________________________________________________________________________ conv2d_89 (Conv2D) (None, 3, 3, 448) 917504 mixed9[0][0] __________________________________________________________________________________________________ batch_normalization_89 (BatchNo (None, 3, 3, 448) 1344 conv2d_89[0][0] __________________________________________________________________________________________________ activation_89 (Activation) (None, 3, 3, 448) 0 batch_normalization_89[0][0] __________________________________________________________________________________________________ conv2d_86 (Conv2D) (None, 3, 3, 384) 786432 mixed9[0][0] __________________________________________________________________________________________________ conv2d_90 (Conv2D) (None, 3, 3, 384) 1548288 activation_89[0][0] __________________________________________________________________________________________________ batch_normalization_86 (BatchNo (None, 3, 3, 384) 1152 conv2d_86[0][0] __________________________________________________________________________________________________ batch_normalization_90 (BatchNo (None, 3, 3, 384) 1152 conv2d_90[0][0] __________________________________________________________________________________________________ activation_86 (Activation) (None, 3, 3, 384) 0 batch_normalization_86[0][0] __________________________________________________________________________________________________ activation_90 (Activation) (None, 3, 3, 384) 0 batch_normalization_90[0][0] __________________________________________________________________________________________________ conv2d_87 (Conv2D) (None, 3, 3, 384) 442368 activation_86[0][0] __________________________________________________________________________________________________ conv2d_88 (Conv2D) (None, 3, 3, 384) 442368 activation_86[0][0] __________________________________________________________________________________________________ conv2d_91 (Conv2D) (None, 3, 3, 384) 442368 activation_90[0][0] __________________________________________________________________________________________________ conv2d_92 (Conv2D) (None, 3, 3, 384) 442368 activation_90[0][0] __________________________________________________________________________________________________ average_pooling2d_8 (AveragePoo (None, 3, 3, 2048) 0 mixed9[0][0] __________________________________________________________________________________________________ conv2d_85 (Conv2D) (None, 3, 3, 320) 655360 mixed9[0][0] __________________________________________________________________________________________________ batch_normalization_87 (BatchNo (None, 3, 3, 384) 1152 conv2d_87[0][0] __________________________________________________________________________________________________ batch_normalization_88 (BatchNo (None, 3, 3, 384) 1152 conv2d_88[0][0] __________________________________________________________________________________________________ batch_normalization_91 (BatchNo (None, 3, 3, 384) 1152 conv2d_91[0][0] __________________________________________________________________________________________________ batch_normalization_92 (BatchNo (None, 3, 3, 384) 1152 conv2d_92[0][0] __________________________________________________________________________________________________ conv2d_93 (Conv2D) (None, 3, 3, 192) 393216 average_pooling2d_8[0][0] __________________________________________________________________________________________________ batch_normalization_85 (BatchNo (None, 3, 3, 320) 960 conv2d_85[0][0] __________________________________________________________________________________________________ activation_87 (Activation) (None, 3, 3, 384) 0 batch_normalization_87[0][0] __________________________________________________________________________________________________ activation_88 (Activation) (None, 3, 3, 384) 0 batch_normalization_88[0][0] __________________________________________________________________________________________________ activation_91 (Activation) (None, 3, 3, 384) 0 batch_normalization_91[0][0] __________________________________________________________________________________________________ activation_92 (Activation) (None, 3, 3, 384) 0 batch_normalization_92[0][0] __________________________________________________________________________________________________ batch_normalization_93 (BatchNo (None, 3, 3, 192) 576 conv2d_93[0][0] __________________________________________________________________________________________________ activation_85 (Activation) (None, 3, 3, 320) 0 batch_normalization_85[0][0] __________________________________________________________________________________________________ mixed9_1 (Concatenate) (None, 3, 3, 768) 0 activation_87[0][0] activation_88[0][0] __________________________________________________________________________________________________ concatenate_1 (Concatenate) (None, 3, 3, 768) 0 activation_91[0][0] activation_92[0][0] __________________________________________________________________________________________________ activation_93 (Activation) (None, 3, 3, 192) 0 batch_normalization_93[0][0] __________________________________________________________________________________________________ mixed10 (Concatenate) (None, 3, 3, 2048) 0 activation_85[0][0] mixed9_1[0][0] concatenate_1[0][0] activation_93[0][0] ================================================================================================== Total params: 21,802,784 Trainable params: 0 Non-trainable params: 21,802,784 __________________________________________________________________________________________________ . last_layer = pre_trained_model.get_layer(&#39;mixed7&#39;) print(&#39;last layer output shape: &#39;, last_layer.output_shape) last_output = last_layer.output # Expected Output: # (&#39;last layer output shape: &#39;, (None, 7, 7, 768)) . last layer output shape: (None, 7, 7, 768) . # Define a Callback class that stops training once accuracy reaches 97.0% class myCallback(tf.keras.callbacks.Callback): def on_epoch_end(self, epoch, logs={}): if(logs.get(&#39;accuracy&#39;)&gt;0.97): print(&quot; nReached 97.0% accuracy so cancelling training!&quot;) self.model.stop_training = True . from tensorflow.keras.optimizers import RMSprop # Flatten the output layer to 1 dimension x = layers.Flatten()(last_output) # Add a fully connected layer with 1,024 hidden units and ReLU activation x = layers.Dense(1024, activation=&#39;relu&#39;)(x) # Add a dropout rate of 0.2 x = layers.Dropout(0.2)(x) # Add a final sigmoid layer for classification x = layers.Dense (1, activation=&#39;sigmoid&#39;)(x) model = Model( pre_trained_model.input, x) model.compile(optimizer = RMSprop(lr=0.0001), loss = &#39;binary_crossentropy&#39;, metrics = [&#39;accuracy&#39;]) model.summary() # Expected output will be large. Last few lines should be: # mixed7 (Concatenate) (None, 7, 7, 768) 0 activation_248[0][0] # activation_251[0][0] # activation_256[0][0] # activation_257[0][0] # __________________________________________________________________________________________________ # flatten_4 (Flatten) (None, 37632) 0 mixed7[0][0] # __________________________________________________________________________________________________ # dense_8 (Dense) (None, 1024) 38536192 flatten_4[0][0] # __________________________________________________________________________________________________ # dropout_4 (Dropout) (None, 1024) 0 dense_8[0][0] # __________________________________________________________________________________________________ # dense_9 (Dense) (None, 1) 1025 dropout_4[0][0] # ================================================================================================== # Total params: 47,512,481 # Trainable params: 38,537,217 # Non-trainable params: 8,975,264 . Model: &#34;model&#34; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_1 (InputLayer) [(None, 150, 150, 3) 0 __________________________________________________________________________________________________ conv2d (Conv2D) (None, 74, 74, 32) 864 input_1[0][0] __________________________________________________________________________________________________ batch_normalization (BatchNorma (None, 74, 74, 32) 96 conv2d[0][0] __________________________________________________________________________________________________ activation (Activation) (None, 74, 74, 32) 0 batch_normalization[0][0] __________________________________________________________________________________________________ conv2d_1 (Conv2D) (None, 72, 72, 32) 9216 activation[0][0] __________________________________________________________________________________________________ batch_normalization_1 (BatchNor (None, 72, 72, 32) 96 conv2d_1[0][0] __________________________________________________________________________________________________ activation_1 (Activation) (None, 72, 72, 32) 0 batch_normalization_1[0][0] __________________________________________________________________________________________________ conv2d_2 (Conv2D) (None, 72, 72, 64) 18432 activation_1[0][0] __________________________________________________________________________________________________ batch_normalization_2 (BatchNor (None, 72, 72, 64) 192 conv2d_2[0][0] __________________________________________________________________________________________________ activation_2 (Activation) (None, 72, 72, 64) 0 batch_normalization_2[0][0] __________________________________________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 35, 35, 64) 0 activation_2[0][0] __________________________________________________________________________________________________ conv2d_3 (Conv2D) (None, 35, 35, 80) 5120 max_pooling2d[0][0] __________________________________________________________________________________________________ batch_normalization_3 (BatchNor (None, 35, 35, 80) 240 conv2d_3[0][0] __________________________________________________________________________________________________ activation_3 (Activation) (None, 35, 35, 80) 0 batch_normalization_3[0][0] __________________________________________________________________________________________________ conv2d_4 (Conv2D) (None, 33, 33, 192) 138240 activation_3[0][0] __________________________________________________________________________________________________ batch_normalization_4 (BatchNor (None, 33, 33, 192) 576 conv2d_4[0][0] __________________________________________________________________________________________________ activation_4 (Activation) (None, 33, 33, 192) 0 batch_normalization_4[0][0] __________________________________________________________________________________________________ max_pooling2d_1 (MaxPooling2D) (None, 16, 16, 192) 0 activation_4[0][0] __________________________________________________________________________________________________ conv2d_8 (Conv2D) (None, 16, 16, 64) 12288 max_pooling2d_1[0][0] __________________________________________________________________________________________________ batch_normalization_8 (BatchNor (None, 16, 16, 64) 192 conv2d_8[0][0] __________________________________________________________________________________________________ activation_8 (Activation) (None, 16, 16, 64) 0 batch_normalization_8[0][0] __________________________________________________________________________________________________ conv2d_6 (Conv2D) (None, 16, 16, 48) 9216 max_pooling2d_1[0][0] __________________________________________________________________________________________________ conv2d_9 (Conv2D) (None, 16, 16, 96) 55296 activation_8[0][0] __________________________________________________________________________________________________ batch_normalization_6 (BatchNor (None, 16, 16, 48) 144 conv2d_6[0][0] __________________________________________________________________________________________________ batch_normalization_9 (BatchNor (None, 16, 16, 96) 288 conv2d_9[0][0] __________________________________________________________________________________________________ activation_6 (Activation) (None, 16, 16, 48) 0 batch_normalization_6[0][0] __________________________________________________________________________________________________ activation_9 (Activation) (None, 16, 16, 96) 0 batch_normalization_9[0][0] __________________________________________________________________________________________________ average_pooling2d (AveragePooli (None, 16, 16, 192) 0 max_pooling2d_1[0][0] __________________________________________________________________________________________________ conv2d_5 (Conv2D) (None, 16, 16, 64) 12288 max_pooling2d_1[0][0] __________________________________________________________________________________________________ conv2d_7 (Conv2D) (None, 16, 16, 64) 76800 activation_6[0][0] __________________________________________________________________________________________________ conv2d_10 (Conv2D) (None, 16, 16, 96) 82944 activation_9[0][0] __________________________________________________________________________________________________ conv2d_11 (Conv2D) (None, 16, 16, 32) 6144 average_pooling2d[0][0] __________________________________________________________________________________________________ batch_normalization_5 (BatchNor (None, 16, 16, 64) 192 conv2d_5[0][0] __________________________________________________________________________________________________ batch_normalization_7 (BatchNor (None, 16, 16, 64) 192 conv2d_7[0][0] __________________________________________________________________________________________________ batch_normalization_10 (BatchNo (None, 16, 16, 96) 288 conv2d_10[0][0] __________________________________________________________________________________________________ batch_normalization_11 (BatchNo (None, 16, 16, 32) 96 conv2d_11[0][0] __________________________________________________________________________________________________ activation_5 (Activation) (None, 16, 16, 64) 0 batch_normalization_5[0][0] __________________________________________________________________________________________________ activation_7 (Activation) (None, 16, 16, 64) 0 batch_normalization_7[0][0] __________________________________________________________________________________________________ activation_10 (Activation) (None, 16, 16, 96) 0 batch_normalization_10[0][0] __________________________________________________________________________________________________ activation_11 (Activation) (None, 16, 16, 32) 0 batch_normalization_11[0][0] __________________________________________________________________________________________________ mixed0 (Concatenate) (None, 16, 16, 256) 0 activation_5[0][0] activation_7[0][0] activation_10[0][0] activation_11[0][0] __________________________________________________________________________________________________ conv2d_15 (Conv2D) (None, 16, 16, 64) 16384 mixed0[0][0] __________________________________________________________________________________________________ batch_normalization_15 (BatchNo (None, 16, 16, 64) 192 conv2d_15[0][0] __________________________________________________________________________________________________ activation_15 (Activation) (None, 16, 16, 64) 0 batch_normalization_15[0][0] __________________________________________________________________________________________________ conv2d_13 (Conv2D) (None, 16, 16, 48) 12288 mixed0[0][0] __________________________________________________________________________________________________ conv2d_16 (Conv2D) (None, 16, 16, 96) 55296 activation_15[0][0] __________________________________________________________________________________________________ batch_normalization_13 (BatchNo (None, 16, 16, 48) 144 conv2d_13[0][0] __________________________________________________________________________________________________ batch_normalization_16 (BatchNo (None, 16, 16, 96) 288 conv2d_16[0][0] __________________________________________________________________________________________________ activation_13 (Activation) (None, 16, 16, 48) 0 batch_normalization_13[0][0] __________________________________________________________________________________________________ activation_16 (Activation) (None, 16, 16, 96) 0 batch_normalization_16[0][0] __________________________________________________________________________________________________ average_pooling2d_1 (AveragePoo (None, 16, 16, 256) 0 mixed0[0][0] __________________________________________________________________________________________________ conv2d_12 (Conv2D) (None, 16, 16, 64) 16384 mixed0[0][0] __________________________________________________________________________________________________ conv2d_14 (Conv2D) (None, 16, 16, 64) 76800 activation_13[0][0] __________________________________________________________________________________________________ conv2d_17 (Conv2D) (None, 16, 16, 96) 82944 activation_16[0][0] __________________________________________________________________________________________________ conv2d_18 (Conv2D) (None, 16, 16, 64) 16384 average_pooling2d_1[0][0] __________________________________________________________________________________________________ batch_normalization_12 (BatchNo (None, 16, 16, 64) 192 conv2d_12[0][0] __________________________________________________________________________________________________ batch_normalization_14 (BatchNo (None, 16, 16, 64) 192 conv2d_14[0][0] __________________________________________________________________________________________________ batch_normalization_17 (BatchNo (None, 16, 16, 96) 288 conv2d_17[0][0] __________________________________________________________________________________________________ batch_normalization_18 (BatchNo (None, 16, 16, 64) 192 conv2d_18[0][0] __________________________________________________________________________________________________ activation_12 (Activation) (None, 16, 16, 64) 0 batch_normalization_12[0][0] __________________________________________________________________________________________________ activation_14 (Activation) (None, 16, 16, 64) 0 batch_normalization_14[0][0] __________________________________________________________________________________________________ activation_17 (Activation) (None, 16, 16, 96) 0 batch_normalization_17[0][0] __________________________________________________________________________________________________ activation_18 (Activation) (None, 16, 16, 64) 0 batch_normalization_18[0][0] __________________________________________________________________________________________________ mixed1 (Concatenate) (None, 16, 16, 288) 0 activation_12[0][0] activation_14[0][0] activation_17[0][0] activation_18[0][0] __________________________________________________________________________________________________ conv2d_22 (Conv2D) (None, 16, 16, 64) 18432 mixed1[0][0] __________________________________________________________________________________________________ batch_normalization_22 (BatchNo (None, 16, 16, 64) 192 conv2d_22[0][0] __________________________________________________________________________________________________ activation_22 (Activation) (None, 16, 16, 64) 0 batch_normalization_22[0][0] __________________________________________________________________________________________________ conv2d_20 (Conv2D) (None, 16, 16, 48) 13824 mixed1[0][0] __________________________________________________________________________________________________ conv2d_23 (Conv2D) (None, 16, 16, 96) 55296 activation_22[0][0] __________________________________________________________________________________________________ batch_normalization_20 (BatchNo (None, 16, 16, 48) 144 conv2d_20[0][0] __________________________________________________________________________________________________ batch_normalization_23 (BatchNo (None, 16, 16, 96) 288 conv2d_23[0][0] __________________________________________________________________________________________________ activation_20 (Activation) (None, 16, 16, 48) 0 batch_normalization_20[0][0] __________________________________________________________________________________________________ activation_23 (Activation) (None, 16, 16, 96) 0 batch_normalization_23[0][0] __________________________________________________________________________________________________ average_pooling2d_2 (AveragePoo (None, 16, 16, 288) 0 mixed1[0][0] __________________________________________________________________________________________________ conv2d_19 (Conv2D) (None, 16, 16, 64) 18432 mixed1[0][0] __________________________________________________________________________________________________ conv2d_21 (Conv2D) (None, 16, 16, 64) 76800 activation_20[0][0] __________________________________________________________________________________________________ conv2d_24 (Conv2D) (None, 16, 16, 96) 82944 activation_23[0][0] __________________________________________________________________________________________________ conv2d_25 (Conv2D) (None, 16, 16, 64) 18432 average_pooling2d_2[0][0] __________________________________________________________________________________________________ batch_normalization_19 (BatchNo (None, 16, 16, 64) 192 conv2d_19[0][0] __________________________________________________________________________________________________ batch_normalization_21 (BatchNo (None, 16, 16, 64) 192 conv2d_21[0][0] __________________________________________________________________________________________________ batch_normalization_24 (BatchNo (None, 16, 16, 96) 288 conv2d_24[0][0] __________________________________________________________________________________________________ batch_normalization_25 (BatchNo (None, 16, 16, 64) 192 conv2d_25[0][0] __________________________________________________________________________________________________ activation_19 (Activation) (None, 16, 16, 64) 0 batch_normalization_19[0][0] __________________________________________________________________________________________________ activation_21 (Activation) (None, 16, 16, 64) 0 batch_normalization_21[0][0] __________________________________________________________________________________________________ activation_24 (Activation) (None, 16, 16, 96) 0 batch_normalization_24[0][0] __________________________________________________________________________________________________ activation_25 (Activation) (None, 16, 16, 64) 0 batch_normalization_25[0][0] __________________________________________________________________________________________________ mixed2 (Concatenate) (None, 16, 16, 288) 0 activation_19[0][0] activation_21[0][0] activation_24[0][0] activation_25[0][0] __________________________________________________________________________________________________ conv2d_27 (Conv2D) (None, 16, 16, 64) 18432 mixed2[0][0] __________________________________________________________________________________________________ batch_normalization_27 (BatchNo (None, 16, 16, 64) 192 conv2d_27[0][0] __________________________________________________________________________________________________ activation_27 (Activation) (None, 16, 16, 64) 0 batch_normalization_27[0][0] __________________________________________________________________________________________________ conv2d_28 (Conv2D) (None, 16, 16, 96) 55296 activation_27[0][0] __________________________________________________________________________________________________ batch_normalization_28 (BatchNo (None, 16, 16, 96) 288 conv2d_28[0][0] __________________________________________________________________________________________________ activation_28 (Activation) (None, 16, 16, 96) 0 batch_normalization_28[0][0] __________________________________________________________________________________________________ conv2d_26 (Conv2D) (None, 7, 7, 384) 995328 mixed2[0][0] __________________________________________________________________________________________________ conv2d_29 (Conv2D) (None, 7, 7, 96) 82944 activation_28[0][0] __________________________________________________________________________________________________ batch_normalization_26 (BatchNo (None, 7, 7, 384) 1152 conv2d_26[0][0] __________________________________________________________________________________________________ batch_normalization_29 (BatchNo (None, 7, 7, 96) 288 conv2d_29[0][0] __________________________________________________________________________________________________ activation_26 (Activation) (None, 7, 7, 384) 0 batch_normalization_26[0][0] __________________________________________________________________________________________________ activation_29 (Activation) (None, 7, 7, 96) 0 batch_normalization_29[0][0] __________________________________________________________________________________________________ max_pooling2d_2 (MaxPooling2D) (None, 7, 7, 288) 0 mixed2[0][0] __________________________________________________________________________________________________ mixed3 (Concatenate) (None, 7, 7, 768) 0 activation_26[0][0] activation_29[0][0] max_pooling2d_2[0][0] __________________________________________________________________________________________________ conv2d_34 (Conv2D) (None, 7, 7, 128) 98304 mixed3[0][0] __________________________________________________________________________________________________ batch_normalization_34 (BatchNo (None, 7, 7, 128) 384 conv2d_34[0][0] __________________________________________________________________________________________________ activation_34 (Activation) (None, 7, 7, 128) 0 batch_normalization_34[0][0] __________________________________________________________________________________________________ conv2d_35 (Conv2D) (None, 7, 7, 128) 114688 activation_34[0][0] __________________________________________________________________________________________________ batch_normalization_35 (BatchNo (None, 7, 7, 128) 384 conv2d_35[0][0] __________________________________________________________________________________________________ activation_35 (Activation) (None, 7, 7, 128) 0 batch_normalization_35[0][0] __________________________________________________________________________________________________ conv2d_31 (Conv2D) (None, 7, 7, 128) 98304 mixed3[0][0] __________________________________________________________________________________________________ conv2d_36 (Conv2D) (None, 7, 7, 128) 114688 activation_35[0][0] __________________________________________________________________________________________________ batch_normalization_31 (BatchNo (None, 7, 7, 128) 384 conv2d_31[0][0] __________________________________________________________________________________________________ batch_normalization_36 (BatchNo (None, 7, 7, 128) 384 conv2d_36[0][0] __________________________________________________________________________________________________ activation_31 (Activation) (None, 7, 7, 128) 0 batch_normalization_31[0][0] __________________________________________________________________________________________________ activation_36 (Activation) (None, 7, 7, 128) 0 batch_normalization_36[0][0] __________________________________________________________________________________________________ conv2d_32 (Conv2D) (None, 7, 7, 128) 114688 activation_31[0][0] __________________________________________________________________________________________________ conv2d_37 (Conv2D) (None, 7, 7, 128) 114688 activation_36[0][0] __________________________________________________________________________________________________ batch_normalization_32 (BatchNo (None, 7, 7, 128) 384 conv2d_32[0][0] __________________________________________________________________________________________________ batch_normalization_37 (BatchNo (None, 7, 7, 128) 384 conv2d_37[0][0] __________________________________________________________________________________________________ activation_32 (Activation) (None, 7, 7, 128) 0 batch_normalization_32[0][0] __________________________________________________________________________________________________ activation_37 (Activation) (None, 7, 7, 128) 0 batch_normalization_37[0][0] __________________________________________________________________________________________________ average_pooling2d_3 (AveragePoo (None, 7, 7, 768) 0 mixed3[0][0] __________________________________________________________________________________________________ conv2d_30 (Conv2D) (None, 7, 7, 192) 147456 mixed3[0][0] __________________________________________________________________________________________________ conv2d_33 (Conv2D) (None, 7, 7, 192) 172032 activation_32[0][0] __________________________________________________________________________________________________ conv2d_38 (Conv2D) (None, 7, 7, 192) 172032 activation_37[0][0] __________________________________________________________________________________________________ conv2d_39 (Conv2D) (None, 7, 7, 192) 147456 average_pooling2d_3[0][0] __________________________________________________________________________________________________ batch_normalization_30 (BatchNo (None, 7, 7, 192) 576 conv2d_30[0][0] __________________________________________________________________________________________________ batch_normalization_33 (BatchNo (None, 7, 7, 192) 576 conv2d_33[0][0] __________________________________________________________________________________________________ batch_normalization_38 (BatchNo (None, 7, 7, 192) 576 conv2d_38[0][0] __________________________________________________________________________________________________ batch_normalization_39 (BatchNo (None, 7, 7, 192) 576 conv2d_39[0][0] __________________________________________________________________________________________________ activation_30 (Activation) (None, 7, 7, 192) 0 batch_normalization_30[0][0] __________________________________________________________________________________________________ activation_33 (Activation) (None, 7, 7, 192) 0 batch_normalization_33[0][0] __________________________________________________________________________________________________ activation_38 (Activation) (None, 7, 7, 192) 0 batch_normalization_38[0][0] __________________________________________________________________________________________________ activation_39 (Activation) (None, 7, 7, 192) 0 batch_normalization_39[0][0] __________________________________________________________________________________________________ mixed4 (Concatenate) (None, 7, 7, 768) 0 activation_30[0][0] activation_33[0][0] activation_38[0][0] activation_39[0][0] __________________________________________________________________________________________________ conv2d_44 (Conv2D) (None, 7, 7, 160) 122880 mixed4[0][0] __________________________________________________________________________________________________ batch_normalization_44 (BatchNo (None, 7, 7, 160) 480 conv2d_44[0][0] __________________________________________________________________________________________________ activation_44 (Activation) (None, 7, 7, 160) 0 batch_normalization_44[0][0] __________________________________________________________________________________________________ conv2d_45 (Conv2D) (None, 7, 7, 160) 179200 activation_44[0][0] __________________________________________________________________________________________________ batch_normalization_45 (BatchNo (None, 7, 7, 160) 480 conv2d_45[0][0] __________________________________________________________________________________________________ activation_45 (Activation) (None, 7, 7, 160) 0 batch_normalization_45[0][0] __________________________________________________________________________________________________ conv2d_41 (Conv2D) (None, 7, 7, 160) 122880 mixed4[0][0] __________________________________________________________________________________________________ conv2d_46 (Conv2D) (None, 7, 7, 160) 179200 activation_45[0][0] __________________________________________________________________________________________________ batch_normalization_41 (BatchNo (None, 7, 7, 160) 480 conv2d_41[0][0] __________________________________________________________________________________________________ batch_normalization_46 (BatchNo (None, 7, 7, 160) 480 conv2d_46[0][0] __________________________________________________________________________________________________ activation_41 (Activation) (None, 7, 7, 160) 0 batch_normalization_41[0][0] __________________________________________________________________________________________________ activation_46 (Activation) (None, 7, 7, 160) 0 batch_normalization_46[0][0] __________________________________________________________________________________________________ conv2d_42 (Conv2D) (None, 7, 7, 160) 179200 activation_41[0][0] __________________________________________________________________________________________________ conv2d_47 (Conv2D) (None, 7, 7, 160) 179200 activation_46[0][0] __________________________________________________________________________________________________ batch_normalization_42 (BatchNo (None, 7, 7, 160) 480 conv2d_42[0][0] __________________________________________________________________________________________________ batch_normalization_47 (BatchNo (None, 7, 7, 160) 480 conv2d_47[0][0] __________________________________________________________________________________________________ activation_42 (Activation) (None, 7, 7, 160) 0 batch_normalization_42[0][0] __________________________________________________________________________________________________ activation_47 (Activation) (None, 7, 7, 160) 0 batch_normalization_47[0][0] __________________________________________________________________________________________________ average_pooling2d_4 (AveragePoo (None, 7, 7, 768) 0 mixed4[0][0] __________________________________________________________________________________________________ conv2d_40 (Conv2D) (None, 7, 7, 192) 147456 mixed4[0][0] __________________________________________________________________________________________________ conv2d_43 (Conv2D) (None, 7, 7, 192) 215040 activation_42[0][0] __________________________________________________________________________________________________ conv2d_48 (Conv2D) (None, 7, 7, 192) 215040 activation_47[0][0] __________________________________________________________________________________________________ conv2d_49 (Conv2D) (None, 7, 7, 192) 147456 average_pooling2d_4[0][0] __________________________________________________________________________________________________ batch_normalization_40 (BatchNo (None, 7, 7, 192) 576 conv2d_40[0][0] __________________________________________________________________________________________________ batch_normalization_43 (BatchNo (None, 7, 7, 192) 576 conv2d_43[0][0] __________________________________________________________________________________________________ batch_normalization_48 (BatchNo (None, 7, 7, 192) 576 conv2d_48[0][0] __________________________________________________________________________________________________ batch_normalization_49 (BatchNo (None, 7, 7, 192) 576 conv2d_49[0][0] __________________________________________________________________________________________________ activation_40 (Activation) (None, 7, 7, 192) 0 batch_normalization_40[0][0] __________________________________________________________________________________________________ activation_43 (Activation) (None, 7, 7, 192) 0 batch_normalization_43[0][0] __________________________________________________________________________________________________ activation_48 (Activation) (None, 7, 7, 192) 0 batch_normalization_48[0][0] __________________________________________________________________________________________________ activation_49 (Activation) (None, 7, 7, 192) 0 batch_normalization_49[0][0] __________________________________________________________________________________________________ mixed5 (Concatenate) (None, 7, 7, 768) 0 activation_40[0][0] activation_43[0][0] activation_48[0][0] activation_49[0][0] __________________________________________________________________________________________________ conv2d_54 (Conv2D) (None, 7, 7, 160) 122880 mixed5[0][0] __________________________________________________________________________________________________ batch_normalization_54 (BatchNo (None, 7, 7, 160) 480 conv2d_54[0][0] __________________________________________________________________________________________________ activation_54 (Activation) (None, 7, 7, 160) 0 batch_normalization_54[0][0] __________________________________________________________________________________________________ conv2d_55 (Conv2D) (None, 7, 7, 160) 179200 activation_54[0][0] __________________________________________________________________________________________________ batch_normalization_55 (BatchNo (None, 7, 7, 160) 480 conv2d_55[0][0] __________________________________________________________________________________________________ activation_55 (Activation) (None, 7, 7, 160) 0 batch_normalization_55[0][0] __________________________________________________________________________________________________ conv2d_51 (Conv2D) (None, 7, 7, 160) 122880 mixed5[0][0] __________________________________________________________________________________________________ conv2d_56 (Conv2D) (None, 7, 7, 160) 179200 activation_55[0][0] __________________________________________________________________________________________________ batch_normalization_51 (BatchNo (None, 7, 7, 160) 480 conv2d_51[0][0] __________________________________________________________________________________________________ batch_normalization_56 (BatchNo (None, 7, 7, 160) 480 conv2d_56[0][0] __________________________________________________________________________________________________ activation_51 (Activation) (None, 7, 7, 160) 0 batch_normalization_51[0][0] __________________________________________________________________________________________________ activation_56 (Activation) (None, 7, 7, 160) 0 batch_normalization_56[0][0] __________________________________________________________________________________________________ conv2d_52 (Conv2D) (None, 7, 7, 160) 179200 activation_51[0][0] __________________________________________________________________________________________________ conv2d_57 (Conv2D) (None, 7, 7, 160) 179200 activation_56[0][0] __________________________________________________________________________________________________ batch_normalization_52 (BatchNo (None, 7, 7, 160) 480 conv2d_52[0][0] __________________________________________________________________________________________________ batch_normalization_57 (BatchNo (None, 7, 7, 160) 480 conv2d_57[0][0] __________________________________________________________________________________________________ activation_52 (Activation) (None, 7, 7, 160) 0 batch_normalization_52[0][0] __________________________________________________________________________________________________ activation_57 (Activation) (None, 7, 7, 160) 0 batch_normalization_57[0][0] __________________________________________________________________________________________________ average_pooling2d_5 (AveragePoo (None, 7, 7, 768) 0 mixed5[0][0] __________________________________________________________________________________________________ conv2d_50 (Conv2D) (None, 7, 7, 192) 147456 mixed5[0][0] __________________________________________________________________________________________________ conv2d_53 (Conv2D) (None, 7, 7, 192) 215040 activation_52[0][0] __________________________________________________________________________________________________ conv2d_58 (Conv2D) (None, 7, 7, 192) 215040 activation_57[0][0] __________________________________________________________________________________________________ conv2d_59 (Conv2D) (None, 7, 7, 192) 147456 average_pooling2d_5[0][0] __________________________________________________________________________________________________ batch_normalization_50 (BatchNo (None, 7, 7, 192) 576 conv2d_50[0][0] __________________________________________________________________________________________________ batch_normalization_53 (BatchNo (None, 7, 7, 192) 576 conv2d_53[0][0] __________________________________________________________________________________________________ batch_normalization_58 (BatchNo (None, 7, 7, 192) 576 conv2d_58[0][0] __________________________________________________________________________________________________ batch_normalization_59 (BatchNo (None, 7, 7, 192) 576 conv2d_59[0][0] __________________________________________________________________________________________________ activation_50 (Activation) (None, 7, 7, 192) 0 batch_normalization_50[0][0] __________________________________________________________________________________________________ activation_53 (Activation) (None, 7, 7, 192) 0 batch_normalization_53[0][0] __________________________________________________________________________________________________ activation_58 (Activation) (None, 7, 7, 192) 0 batch_normalization_58[0][0] __________________________________________________________________________________________________ activation_59 (Activation) (None, 7, 7, 192) 0 batch_normalization_59[0][0] __________________________________________________________________________________________________ mixed6 (Concatenate) (None, 7, 7, 768) 0 activation_50[0][0] activation_53[0][0] activation_58[0][0] activation_59[0][0] __________________________________________________________________________________________________ conv2d_64 (Conv2D) (None, 7, 7, 192) 147456 mixed6[0][0] __________________________________________________________________________________________________ batch_normalization_64 (BatchNo (None, 7, 7, 192) 576 conv2d_64[0][0] __________________________________________________________________________________________________ activation_64 (Activation) (None, 7, 7, 192) 0 batch_normalization_64[0][0] __________________________________________________________________________________________________ conv2d_65 (Conv2D) (None, 7, 7, 192) 258048 activation_64[0][0] __________________________________________________________________________________________________ batch_normalization_65 (BatchNo (None, 7, 7, 192) 576 conv2d_65[0][0] __________________________________________________________________________________________________ activation_65 (Activation) (None, 7, 7, 192) 0 batch_normalization_65[0][0] __________________________________________________________________________________________________ conv2d_61 (Conv2D) (None, 7, 7, 192) 147456 mixed6[0][0] __________________________________________________________________________________________________ conv2d_66 (Conv2D) (None, 7, 7, 192) 258048 activation_65[0][0] __________________________________________________________________________________________________ batch_normalization_61 (BatchNo (None, 7, 7, 192) 576 conv2d_61[0][0] __________________________________________________________________________________________________ batch_normalization_66 (BatchNo (None, 7, 7, 192) 576 conv2d_66[0][0] __________________________________________________________________________________________________ activation_61 (Activation) (None, 7, 7, 192) 0 batch_normalization_61[0][0] __________________________________________________________________________________________________ activation_66 (Activation) (None, 7, 7, 192) 0 batch_normalization_66[0][0] __________________________________________________________________________________________________ conv2d_62 (Conv2D) (None, 7, 7, 192) 258048 activation_61[0][0] __________________________________________________________________________________________________ conv2d_67 (Conv2D) (None, 7, 7, 192) 258048 activation_66[0][0] __________________________________________________________________________________________________ batch_normalization_62 (BatchNo (None, 7, 7, 192) 576 conv2d_62[0][0] __________________________________________________________________________________________________ batch_normalization_67 (BatchNo (None, 7, 7, 192) 576 conv2d_67[0][0] __________________________________________________________________________________________________ activation_62 (Activation) (None, 7, 7, 192) 0 batch_normalization_62[0][0] __________________________________________________________________________________________________ activation_67 (Activation) (None, 7, 7, 192) 0 batch_normalization_67[0][0] __________________________________________________________________________________________________ average_pooling2d_6 (AveragePoo (None, 7, 7, 768) 0 mixed6[0][0] __________________________________________________________________________________________________ conv2d_60 (Conv2D) (None, 7, 7, 192) 147456 mixed6[0][0] __________________________________________________________________________________________________ conv2d_63 (Conv2D) (None, 7, 7, 192) 258048 activation_62[0][0] __________________________________________________________________________________________________ conv2d_68 (Conv2D) (None, 7, 7, 192) 258048 activation_67[0][0] __________________________________________________________________________________________________ conv2d_69 (Conv2D) (None, 7, 7, 192) 147456 average_pooling2d_6[0][0] __________________________________________________________________________________________________ batch_normalization_60 (BatchNo (None, 7, 7, 192) 576 conv2d_60[0][0] __________________________________________________________________________________________________ batch_normalization_63 (BatchNo (None, 7, 7, 192) 576 conv2d_63[0][0] __________________________________________________________________________________________________ batch_normalization_68 (BatchNo (None, 7, 7, 192) 576 conv2d_68[0][0] __________________________________________________________________________________________________ batch_normalization_69 (BatchNo (None, 7, 7, 192) 576 conv2d_69[0][0] __________________________________________________________________________________________________ activation_60 (Activation) (None, 7, 7, 192) 0 batch_normalization_60[0][0] __________________________________________________________________________________________________ activation_63 (Activation) (None, 7, 7, 192) 0 batch_normalization_63[0][0] __________________________________________________________________________________________________ activation_68 (Activation) (None, 7, 7, 192) 0 batch_normalization_68[0][0] __________________________________________________________________________________________________ activation_69 (Activation) (None, 7, 7, 192) 0 batch_normalization_69[0][0] __________________________________________________________________________________________________ mixed7 (Concatenate) (None, 7, 7, 768) 0 activation_60[0][0] activation_63[0][0] activation_68[0][0] activation_69[0][0] __________________________________________________________________________________________________ flatten (Flatten) (None, 37632) 0 mixed7[0][0] __________________________________________________________________________________________________ dense (Dense) (None, 1024) 38536192 flatten[0][0] __________________________________________________________________________________________________ dropout (Dropout) (None, 1024) 0 dense[0][0] __________________________________________________________________________________________________ dense_1 (Dense) (None, 1) 1025 dropout[0][0] ================================================================================================== Total params: 47,512,481 Trainable params: 38,537,217 Non-trainable params: 8,975,264 __________________________________________________________________________________________________ . # Get the Horse or Human dataset path_horse_or_human = f&quot;{getcwd()}/../tmp2/horse-or-human.zip&quot; # Get the Horse or Human Validation dataset path_validation_horse_or_human = f&quot;{getcwd()}/../tmp2/validation-horse-or-human.zip&quot; from tensorflow.keras.preprocessing.image import ImageDataGenerator import os import zipfile import shutil shutil.rmtree(&#39;/tmp&#39;) local_zip = path_horse_or_human zip_ref = zipfile.ZipFile(local_zip, &#39;r&#39;) zip_ref.extractall(&#39;/tmp/training&#39;) zip_ref.close() local_zip = path_validation_horse_or_human zip_ref = zipfile.ZipFile(local_zip, &#39;r&#39;) zip_ref.extractall(&#39;/tmp/validation&#39;) zip_ref.close() . # Define our example directories and files train_dir = &#39;/tmp/training&#39; validation_dir = &#39;/tmp/validation&#39; train_horses_dir = os.path.join(train_dir, &#39;horses&#39;) train_humans_dir = os.path.join(train_dir, &#39;humans&#39;) validation_horses_dir = os.path.join(validation_dir, &#39;horses&#39;) validation_humans_dir = os.path.join(validation_dir, &#39;humans&#39;) train_horses_fnames = os.listdir(train_horses_dir) train_humans_fnames = os.listdir(train_humans_dir) validation_horses_fnames = os.listdir(validation_horses_dir) validation_humans_fnames = os.listdir(validation_humans_dir) print(len(train_horses_fnames)) print(len(train_humans_fnames)) print(len(validation_horses_fnames)) print(len(validation_humans_fnames)) # Expected Output: # 500 # 527 # 128 # 128 . 500 527 128 128 . # Add our data-augmentation parameters to ImageDataGenerator train_datagen = ImageDataGenerator(rescale = 1./255., rotation_range = 40, width_shift_range = 0.2, height_shift_range = 0.2, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True) # Note that the validation data should not be augmented! test_datagen = ImageDataGenerator(rescale = 1./255.) # Flow training images in batches of 20 using train_datagen generator train_generator = train_datagen.flow_from_directory(train_dir, batch_size = 20, class_mode = &#39;binary&#39;, target_size = (150, 150)) # Flow validation images in batches of 20 using test_datagen generator validation_generator = test_datagen.flow_from_directory( validation_dir, batch_size = 20, class_mode = &#39;binary&#39;, target_size = (150, 150)) # Expected Output: # Found 1027 images belonging to 2 classes. # Found 256 images belonging to 2 classes. . Found 1027 images belonging to 2 classes. Found 256 images belonging to 2 classes. . # Run this and see how many epochs it should take before the callback # fires, and stops training at 97% accuracy callbacks = myCallback() history = model.fit_generator( train_generator, validation_data = validation_generator, steps_per_epoch = 100, epochs = 3, validation_steps = 10, verbose = 2, callbacks=[myCallback()]) . Epoch 1/3 Reached 97.0% accuracy so cancelling training! 100/100 - 84s - loss: 0.0352 - accuracy: 0.9889 - val_loss: 0.2615 - val_accuracy: 0.9750 . %matplotlib inline import matplotlib.pyplot as plt acc = history.history[&#39;accuracy&#39;] val_acc = history.history[&#39;val_accuracy&#39;] loss = history.history[&#39;loss&#39;] val_loss = history.history[&#39;val_loss&#39;] epochs = range(len(acc)) plt.plot(epochs, acc, &#39;r&#39;, label=&#39;Training accuracy&#39;) plt.plot(epochs, val_acc, &#39;b&#39;, label=&#39;Validation accuracy&#39;) plt.title(&#39;Training and validation accuracy&#39;) plt.legend(loc=0) plt.figure() plt.show() . &lt;Figure size 432x288 with 0 Axes&gt; .",
            "url": "https://rafaelsf80.github.io/notebooks/computer%20vision/2020/08/02/humans-horses-transfer-learning.html",
            "relUrl": "/computer%20vision/2020/08/02/humans-horses-transfer-learning.html",
            "date": " • Aug 2, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Redes convolucionales Cats-Dogs",
            "content": "Introducci&#243;n . Se va a resolver la competición de Kaggle expuesta aqui. Se usarán 25,000 . # ATTENTION: Please do not alter any of the provided code in the exercise. Only add your own code where indicated # ATTENTION: Please do not add or remove any cells in the exercise. The grader will check specific cells based on the cell position. # ATTENTION: Please use the provided epoch values when training. # In this exercise you will train a CNN on the FULL Cats-v-dogs dataset # This will require you doing a lot of data preprocessing because # the dataset isn&#39;t split into training and validation for you # This code block has all the required inputs import os import zipfile import random import shutil import tensorflow as tf from tensorflow.keras.optimizers import RMSprop from tensorflow.keras.preprocessing.image import ImageDataGenerator from shutil import copyfile from os import getcwd . # This code block unzips the full Cats-v-Dogs dataset to /tmp # which will create a tmp/PetImages directory containing subdirectories # called &#39;Cat&#39; and &#39;Dog&#39; (that&#39;s how the original researchers structured it) path_cats_and_dogs = f&quot;{getcwd()}/../tmp2/cats-and-dogs.zip&quot; shutil.rmtree(&#39;/tmp&#39;) local_zip = path_cats_and_dogs zip_ref = zipfile.ZipFile(local_zip, &#39;r&#39;) zip_ref.extractall(&#39;/tmp&#39;) zip_ref.close() . print(len(os.listdir(&#39;/tmp/PetImages/Cat/&#39;))) print(len(os.listdir(&#39;/tmp/PetImages/Dog/&#39;))) # Expected Output: # 1500 # 1500 . 1500 1500 . # Use os.mkdir to create your directories # You will need a directory for cats-v-dogs, and subdirectories for training # and testing. These in turn will need subdirectories for &#39;cats&#39; and &#39;dogs&#39; try: os.chdir(&#39;/tmp&#39;) os.mkdir(&#39;cats-v-dogs&#39;) os.mkdir(&#39;cats-v-dogs/training&#39;) os.mkdir(&#39;cats-v-dogs/testing&#39;) os.mkdir(&#39;cats-v-dogs/training/dogs&#39;) os.mkdir(&#39;cats-v-dogs/training/cats&#39;) os.mkdir(&#39;cats-v-dogs/testing/dogs&#39;) os.mkdir(&#39;cats-v-dogs/testing/cats&#39;) except OSError: pass . # Write a python function called split_data which takes # a SOURCE directory containing the files # a TRAINING directory that a portion of the files will be copied to # a TESTING directory that a portion of the files will be copie to # a SPLIT SIZE to determine the portion # The files should also be randomized, so that the training set is a random # X% of the files, and the test set is the remaining files # SO, for example, if SOURCE is PetImages/Cat, and SPLIT SIZE is .9 # Then 90% of the images in PetImages/Cat will be copied to the TRAINING dir # and 10% of the images will be copied to the TESTING dir # Also -- All images should be checked, and if they have a zero file length, # they will not be copied over # # os.listdir(DIRECTORY) gives you a listing of the contents of that directory # os.path.getsize(PATH) gives you the size of the file # copyfile(source, destination) copies a file from source to destination # random.sample(list, len(list)) shuffles a list def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE): # YOUR CODE STARTS HERE lista_ficheros = os.listdir(SOURCE) random.sample(lista_ficheros, len(lista_ficheros)) lista_ficheros_training = lista_ficheros[:round(SPLIT_SIZE*len(lista_ficheros))] lista_ficheros_testing = lista_ficheros[round(SPLIT_SIZE*len(lista_ficheros)):] for fichero in lista_ficheros_training: if os.path.getsize(SOURCE+fichero)&gt;0: copyfile(SOURCE+fichero, TRAINING+fichero) for fichero in lista_ficheros_testing: if os.path.getsize(SOURCE+fichero)&gt;0: copyfile(SOURCE+fichero, TESTING+fichero) # YOUR CODE ENDS HERE CAT_SOURCE_DIR = &quot;/tmp/PetImages/Cat/&quot; TRAINING_CATS_DIR = &quot;/tmp/cats-v-dogs/training/cats/&quot; TESTING_CATS_DIR = &quot;/tmp/cats-v-dogs/testing/cats/&quot; DOG_SOURCE_DIR = &quot;/tmp/PetImages/Dog/&quot; TRAINING_DOGS_DIR = &quot;/tmp/cats-v-dogs/training/dogs/&quot; TESTING_DOGS_DIR = &quot;/tmp/cats-v-dogs/testing/dogs/&quot; split_size = .9 split_data(CAT_SOURCE_DIR, TRAINING_CATS_DIR, TESTING_CATS_DIR, split_size) split_data(DOG_SOURCE_DIR, TRAINING_DOGS_DIR, TESTING_DOGS_DIR, split_size) . print(len(os.listdir(&#39;/tmp/cats-v-dogs/training/cats/&#39;))) print(len(os.listdir(&#39;/tmp/cats-v-dogs/training/dogs/&#39;))) print(len(os.listdir(&#39;/tmp/cats-v-dogs/testing/cats/&#39;))) print(len(os.listdir(&#39;/tmp/cats-v-dogs/testing/dogs/&#39;))) # Expected output: # 1350 # 1350 # 150 # 150 . 1350 1350 150 150 . # DEFINE A KERAS MODEL TO CLASSIFY CATS V DOGS # USE AT LEAST 3 CONVOLUTION LAYERS model = tf.keras.models.Sequential([ tf.keras.layers.Conv2D(16, (3,3), activation=&#39;relu&#39;, input_shape=(150, 150, 3)), tf.keras.layers.MaxPooling2D(2,2), tf.keras.layers.Conv2D(32, (3,3), activation=&#39;relu&#39;), tf.keras.layers.MaxPooling2D(2,2), tf.keras.layers.Conv2D(64, (3,3), activation=&#39;relu&#39;), tf.keras.layers.MaxPooling2D(2,2), # Flatten the results to feed into a DNN tf.keras.layers.Flatten(), # 512 neuron hidden layer tf.keras.layers.Dense(512, activation=&#39;relu&#39;), # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class (&#39;cats&#39;) and 1 for the other (&#39;dogs&#39;) tf.keras.layers.Dense(1, activation=&#39;sigmoid&#39;) # YOUR CODE HERE ]) model.compile(optimizer=RMSprop(lr=0.001), loss=&#39;binary_crossentropy&#39;, metrics=[&#39;acc&#39;]) . NOTE: . In the cell below you MUST use a batch size of 10 (batch_size=10) for the train_generator and the validation_generator. Using a batch size greater than 10 will exceed memory limits on the Coursera platform. . TRAINING_DIR = &#39;/tmp/cats-v-dogs/training/&#39; train_datagen = ImageDataGenerator( rescale=1./255, rotation_range=20, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode=&#39;nearest&#39;) # NOTE: YOU MUST USE A BATCH SIZE OF 10 (batch_size=10) FOR THE # TRAIN GENERATOR. train_generator = train_datagen.flow_from_directory(TRAINING_DIR, batch_size=10, class_mode=&#39;binary&#39;, target_size=(150, 150)) VALIDATION_DIR = &#39;/tmp/cats-v-dogs/testing/&#39; validation_datagen = ImageDataGenerator( rescale = 1.0/255. ) # NOTE: YOU MUST USE A BACTH SIZE OF 10 (batch_size=10) FOR THE # VALIDATION GENERATOR. validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR, batch_size=10, class_mode = &#39;binary&#39;, target_size = (150, 150)) # Expected Output: # Found 2700 images belonging to 2 classes. # Found 300 images belonging to 2 classes. . Found 2700 images belonging to 2 classes. Found 300 images belonging to 2 classes. . history = model.fit_generator(train_generator, epochs=2, verbose=1, validation_data=validation_generator) . Epoch 1/2 270/270 [==============================] - 66s 243ms/step - loss: 0.6589 - acc: 0.6241 - val_loss: 0.5918 - val_acc: 0.6767 Epoch 2/2 270/270 [==============================] - 61s 226ms/step - loss: 0.6444 - acc: 0.6400 - val_loss: 0.6259 - val_acc: 0.6567 . # PLOT LOSS AND ACCURACY %matplotlib inline import matplotlib.image as mpimg import matplotlib.pyplot as plt #-- # Retrieve a list of list results on training and test data # sets for each training epoch #-- acc=history.history[&#39;acc&#39;] val_acc=history.history[&#39;val_acc&#39;] loss=history.history[&#39;loss&#39;] val_loss=history.history[&#39;val_loss&#39;] epochs=range(len(acc)) # Get number of epochs # # Plot training and validation accuracy per epoch # plt.plot(epochs, acc, &#39;r&#39;, &quot;Training Accuracy&quot;) plt.plot(epochs, val_acc, &#39;b&#39;, &quot;Validation Accuracy&quot;) plt.title(&#39;Training and validation accuracy&#39;) plt.figure() # # Plot training and validation loss per epoch # plt.plot(epochs, loss, &#39;r&#39;, &quot;Training Loss&quot;) plt.plot(epochs, val_loss, &#39;b&#39;, &quot;Validation Loss&quot;) plt.title(&#39;Training and validation loss&#39;) # Desired output. Charts with training and validation metrics. No crash :) . Text(0.5, 1.0, &#39;Training and validation loss&#39;) .",
            "url": "https://rafaelsf80.github.io/notebooks/computer%20vision/2020/08/02/cats-dogs-augmentation.html",
            "relUrl": "/computer%20vision/2020/08/02/cats-dogs-augmentation.html",
            "date": " • Aug 2, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "CNNs: perros y gatos con Data augmentation",
            "content": "# Introducción Se va a resolver la competición de Kaggle expuesta [aqui](https://www.kaggle.com/c/dogs-vs-cats/data). Se usarán 25,000 imágenes de perros y gatos. . # ATTENTION: Please do not alter any of the provided code in the exercise. Only add your own code where indicated # ATTENTION: Please do not add or remove any cells in the exercise. The grader will check specific cells based on the cell position. # ATTENTION: Please use the provided epoch values when training. # In this exercise you will train a CNN on the FULL Cats-v-dogs dataset # This will require you doing a lot of data preprocessing because # the dataset isn&#39;t split into training and validation for you # This code block has all the required inputs import os import zipfile import random import tensorflow as tf import shutil from tensorflow.keras.optimizers import RMSprop from tensorflow.keras.preprocessing.image import ImageDataGenerator from shutil import copyfile from os import getcwd . !kaggle competitions download -c dogs-vs-cats #shutil.rmtree(&#39;/tmp&#39;) #local_zip = path_cats_and_dogs #zip_ref = zipfile.ZipFile(local_zip, &#39;r&#39;) #zip_ref.extractall(&#39;/tmp&#39;) #zip_ref.close() . Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run &#39;chmod 600 /Users/rafaelsanchez/.kaggle/kaggle.json&#39; Downloading dogs-vs-cats.zip to /Users/rafaelsanchez/git/notebooks/_notebooks 100%|████████████████████████████████████████| 812M/812M [14:52&lt;00:00, 1.26MB/s] 100%|█████████████████████████████████████████| 812M/812M [14:52&lt;00:00, 954kB/s] . print(len(os.listdir(&#39;/tmp/PetImages/Cat/&#39;))) print(len(os.listdir(&#39;/tmp/PetImages/Dog/&#39;))) # Expected Output: # 1500 # 1500 . 1500 1500 . # Use os.mkdir to create your directories # You will need a directory for cats-v-dogs, and subdirectories for training # and testing. These in turn will need subdirectories for &#39;cats&#39; and &#39;dogs&#39; try: os.chdir(&#39;/tmp&#39;) os.mkdir(&#39;cats-v-dogs&#39;) os.mkdir(&#39;cats-v-dogs/training&#39;) os.mkdir(&#39;cats-v-dogs/testing&#39;) os.mkdir(&#39;cats-v-dogs/training/dogs&#39;) os.mkdir(&#39;cats-v-dogs/training/cats&#39;) os.mkdir(&#39;cats-v-dogs/testing/dogs&#39;) os.mkdir(&#39;cats-v-dogs/testing/cats&#39;) except OSError: pass . # Write a python function called split_data which takes # a SOURCE directory containing the files # a TRAINING directory that a portion of the files will be copied to # a TESTING directory that a portion of the files will be copie to # a SPLIT SIZE to determine the portion # The files should also be randomized, so that the training set is a random # X% of the files, and the test set is the remaining files # SO, for example, if SOURCE is PetImages/Cat, and SPLIT SIZE is .9 # Then 90% of the images in PetImages/Cat will be copied to the TRAINING dir # and 10% of the images will be copied to the TESTING dir # Also -- All images should be checked, and if they have a zero file length, # they will not be copied over # # os.listdir(DIRECTORY) gives you a listing of the contents of that directory # os.path.getsize(PATH) gives you the size of the file # copyfile(source, destination) copies a file from source to destination # random.sample(list, len(list)) shuffles a list def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE): # YOUR CODE STARTS HERE lista_ficheros = os.listdir(SOURCE) random.sample(lista_ficheros, len(lista_ficheros)) lista_ficheros_training = lista_ficheros[:round(SPLIT_SIZE*len(lista_ficheros))] lista_ficheros_testing = lista_ficheros[round(SPLIT_SIZE*len(lista_ficheros)):] for fichero in lista_ficheros_training: if os.path.getsize(SOURCE+fichero)&gt;0: copyfile(SOURCE+fichero, TRAINING+fichero) for fichero in lista_ficheros_testing: if os.path.getsize(SOURCE+fichero)&gt;0: copyfile(SOURCE+fichero, TESTING+fichero) # YOUR CODE ENDS HERE CAT_SOURCE_DIR = &quot;/tmp/PetImages/Cat/&quot; TRAINING_CATS_DIR = &quot;/tmp/cats-v-dogs/training/cats/&quot; TESTING_CATS_DIR = &quot;/tmp/cats-v-dogs/testing/cats/&quot; DOG_SOURCE_DIR = &quot;/tmp/PetImages/Dog/&quot; TRAINING_DOGS_DIR = &quot;/tmp/cats-v-dogs/training/dogs/&quot; TESTING_DOGS_DIR = &quot;/tmp/cats-v-dogs/testing/dogs/&quot; split_size = .9 split_data(CAT_SOURCE_DIR, TRAINING_CATS_DIR, TESTING_CATS_DIR, split_size) split_data(DOG_SOURCE_DIR, TRAINING_DOGS_DIR, TESTING_DOGS_DIR, split_size) . print(len(os.listdir(&#39;/tmp/cats-v-dogs/training/cats/&#39;))) print(len(os.listdir(&#39;/tmp/cats-v-dogs/training/dogs/&#39;))) print(len(os.listdir(&#39;/tmp/cats-v-dogs/testing/cats/&#39;))) print(len(os.listdir(&#39;/tmp/cats-v-dogs/testing/dogs/&#39;))) # Expected output: # 1350 # 1350 # 150 # 150 . 1350 1350 150 150 . # Usamos 3 capas convolucionales model = tf.keras.models.Sequential([ tf.keras.layers.Conv2D(16, (3,3), activation=&#39;relu&#39;, input_shape=(150, 150, 3)), tf.keras.layers.MaxPooling2D(2,2), tf.keras.layers.Conv2D(32, (3,3), activation=&#39;relu&#39;), tf.keras.layers.MaxPooling2D(2,2), tf.keras.layers.Conv2D(64, (3,3), activation=&#39;relu&#39;), tf.keras.layers.MaxPooling2D(2,2), # Aplanamos para entrar en una capa densa tf.keras.layers.Flatten(), # 512 unidades tf.keras.layers.Dense(512, activation=&#39;relu&#39;), # activation sigmoid porque es una única salida (1 si gato, 0 si perro) tf.keras.layers.Dense(1, activation=&#39;sigmoid&#39;) ]) model.compile(optimizer=RMSprop(lr=0.001), loss=&#39;binary_crossentropy&#39;, metrics=[&#39;acc&#39;]) . NOTE: . In the cell below you MUST use a batch size of 10 (batch_size=10) for the train_generator and the validation_generator. Using a batch size greater than 10 will exceed memory limits on the Coursera platform. . TRAINING_DIR = &#39;/tmp/cats-v-dogs/training/&#39; train_datagen = ImageDataGenerator( rescale = 1.0/255. ) # NOTE: YOU MUST USE A BATCH SIZE OF 10 (batch_size=10) FOR THE # TRAIN GENERATOR. train_generator = train_datagen.flow_from_directory(TRAINING_DIR, batch_size=10, class_mode=&#39;binary&#39;, target_size=(150, 150)) VALIDATION_DIR = &#39;/tmp/cats-v-dogs/testing/&#39; validation_datagen = ImageDataGenerator( rescale = 1.0/255. ) # NOTE: YOU MUST USE A BACTH SIZE OF 10 (batch_size=10) FOR THE # VALIDATION GENERATOR. validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR, batch_size=10, class_mode = &#39;binary&#39;, target_size = (150, 150)) # Expected Output: # Found 2700 images belonging to 2 classes. # Found 300 images belonging to 2 classes. . Found 2700 images belonging to 2 classes. Found 300 images belonging to 2 classes. . history = model.fit_generator(train_generator, epochs=2, verbose=1, validation_data=validation_generator) . Epoch 1/2 270/270 [==============================] - 41s 150ms/step - loss: 0.5252 - acc: 0.7322 - val_loss: 0.6337 - val_acc: 0.6700 Epoch 2/2 270/270 [==============================] - 40s 149ms/step - loss: 0.4512 - acc: 0.7930 - val_loss: 0.5766 - val_acc: 0.7333 . # PLOT LOSS AND ACCURACY %matplotlib inline import matplotlib.image as mpimg import matplotlib.pyplot as plt #-- # Retrieve a list of list results on training and test data # sets for each training epoch #-- acc=history.history[&#39;acc&#39;] val_acc=history.history[&#39;val_acc&#39;] loss=history.history[&#39;loss&#39;] val_loss=history.history[&#39;val_loss&#39;] epochs=range(len(acc)) # Get number of epochs # # Plot training and validation accuracy per epoch # plt.plot(epochs, acc, &#39;r&#39;, &quot;Training Accuracy&quot;) plt.plot(epochs, val_acc, &#39;b&#39;, &quot;Validation Accuracy&quot;) plt.title(&#39;Training and validation accuracy&#39;) plt.figure() # # Plot training and validation loss per epoch # plt.plot(epochs, loss, &#39;r&#39;, &quot;Training Loss&quot;) plt.plot(epochs, val_loss, &#39;b&#39;, &quot;Validation Loss&quot;) plt.title(&#39;Training and validation loss&#39;) # Desired output. Charts with training and validation metrics. No crash :) . Text(0.5, 1.0, &#39;Training and validation loss&#39;) .",
            "url": "https://rafaelsf80.github.io/notebooks/computer%20vision/2020/08/02/cats-dogs-25k.html",
            "relUrl": "/computer%20vision/2020/08/02/cats-dogs-25k.html",
            "date": " • Aug 2, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Redes convolucionales Happy-Sad",
            "content": "Introducci&#243;n . Below is code with a link to a happy or sad dataset which contains 80 images, 40 happy and 40 sad. Create a convolutional neural network that trains to 100% accuracy on these images, which cancels training upon hitting training accuracy of &gt;.999 . Hint -- it will work best with 3 convolutional layers. . import tensorflow as tf import os import zipfile from os import path, getcwd, chdir # DO NOT CHANGE THE LINE BELOW. If you are developing in a local # environment, then grab happy-or-sad.zip from the Coursera Jupyter Notebook # and place it inside a local folder and edit the path to that location path = f&quot;{getcwd()}/../tmp2/happy-or-sad.zip&quot; zip_ref = zipfile.ZipFile(path, &#39;r&#39;) zip_ref.extractall(&quot;/tmp/h-or-s&quot;) zip_ref.close() . # GRADED FUNCTION: train_happy_sad_model def train_happy_sad_model(): # Please write your code only where you are indicated. # please do not remove # model fitting inline comments. DESIRED_ACCURACY = 0.999 class myCallback(tf.keras.callbacks.Callback): def on_epoch_end(self, epoch, logs={}): if(logs.get(&#39;acc&#39;)&gt;DESIRED_ACCURACY): print(&quot; nReached 99.9% accuracy so cancelling training!&quot;) self.model.stop_training = True callbacks = myCallback() # This Code Block should Define and Compile the Model. Please assume the images are 150 X 150 in your implementation. model = tf.keras.models.Sequential([ tf.keras.layers.Conv2D(16, (3,3), activation=&#39;relu&#39;, input_shape=(150, 150, 3)), # Input 300x300, 3 canales tf.keras.layers.MaxPooling2D(2, 2), tf.keras.layers.Conv2D(32, (3,3), activation=&#39;relu&#39;), tf.keras.layers.MaxPooling2D(2,2), tf.keras.layers.Conv2D(64, (3,3), activation=&#39;relu&#39;), tf.keras.layers.MaxPooling2D(2,2), tf.keras.layers.Flatten(), # Aplanar hacia DNN tf.keras.layers.Dense(512, activation=&#39;relu&#39;), # 512 unidades tf.keras.layers.Dense(1, activation=&#39;sigmoid&#39;) ]) from tensorflow.keras.optimizers import RMSprop model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=RMSprop(lr=0.001), metrics=[&#39;accuracy&#39;]) # This code block should create an instance of an ImageDataGenerator called train_datagen # And a train_generator by calling train_datagen.flow_from_directory from tensorflow.keras.preprocessing.image import ImageDataGenerator train_datagen = ImageDataGenerator(rescale=1/255) # Reescalado validation_datagen = ImageDataGenerator(rescale=1/255) # Please use a target_size of 150 X 150. train_generator = train_datagen.flow_from_directory( &#39;/tmp/h-or-s/&#39;, # Directorio target_size=(150, 150), # Reescalado a 300x300 batch_size=128, # Batches de 128 class_mode=&#39;binary&#39;) # Horses vs Humans # Expected output: &#39;Found 80 images belonging to 2 classes&#39; # This code block should call model.fit_generator and train for # a number of epochs. # model fitting history = model.fit_generator( train_generator, steps_per_epoch=8, epochs=15, verbose=1, callbacks=[callbacks]) # model fitting return history.history[&#39;acc&#39;][-1] . # The Expected output: &quot;Reached 99.9% accuracy so cancelling training!&quot;&quot; train_happy_sad_model() . Found 80 images belonging to 2 classes. Epoch 1/15 8/8 [==============================] - 4s 462ms/step - loss: 1.8616 - acc: 0.5000 Epoch 2/15 8/8 [==============================] - 3s 314ms/step - loss: 0.5336 - acc: 0.7125 Epoch 3/15 8/8 [==============================] - 2s 312ms/step - loss: 0.1999 - acc: 0.9156 Epoch 4/15 8/8 [==============================] - 2s 312ms/step - loss: 0.0734 - acc: 0.9859 Epoch 5/15 7/8 [=========================&gt;....] - ETA: 0s - loss: 0.0201 - acc: 1.0000 Reached 99.9% accuracy so cancelling training! 8/8 [==============================] - 3s 313ms/step - loss: 0.0190 - acc: 1.0000 . 1.0 .",
            "url": "https://rafaelsf80.github.io/notebooks/computer%20vision/2020/08/01/happy-sad.html",
            "relUrl": "/computer%20vision/2020/08/01/happy-sad.html",
            "date": " • Aug 1, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Redes convolucionales Fashion MNIST",
            "content": "Introducci&#243;n . In the videos you looked at how you would improve Fashion MNIST using Convolutions. For your exercise see if you can improve MNIST to 99.8% accuracy or more using only a single convolutional layer and a single MaxPooling 2D. You should stop training once the accuracy goes above this amount. It should happen in less than 20 epochs, so it&#39;s ok to hard code the number of epochs for training, but your training must end once it hits the above metric. If it doesn&#39;t, then you&#39;ll need to redesign your layers. . I&#39;ve started the code for you -- you need to finish it! . When 99.8% accuracy has been hit, you should print out the string &quot;Reached 99.8% accuracy so cancelling training!&quot; . import tensorflow as tf from os import path, getcwd, chdir # DO NOT CHANGE THE LINE BELOW. If you are developing in a local # environment, then grab mnist.npz from the Coursera Jupyter Notebook # and place it inside a local folder and edit the path to that location path = f&quot;{getcwd()}/../tmp2/mnist.npz&quot; . config = tf.ConfigProto() config.gpu_options.allow_growth = True sess = tf.Session(config=config) . # GRADED FUNCTION: train_mnist_conv def train_mnist_conv(): # Please write your code only where you are indicated. # please do not remove model fitting inline comments. # YOUR CODE STARTS HERE class myCallback(tf.keras.callbacks.Callback): def on_epoch_end(self, epoch, logs={}): if(logs.get(&#39;acc&#39;)&gt;0.998): print(&quot; nReached 99.8% accuracy so cancelling training!&quot;) self.model.stop_training = True callbacks = myCallback() # YOUR CODE ENDS HERE mnist = tf.keras.datasets.mnist (training_images, training_labels), (test_images, test_labels) = mnist.load_data(path=path) # YOUR CODE STARTS HERE training_images=training_images.reshape(60000, 28, 28, 1) training_images=training_images / 255.0 test_images = test_images.reshape(10000, 28, 28, 1) test_images=test_images/255.0 # YOUR CODE ENDS HERE model = tf.keras.models.Sequential([ # YOUR CODE STARTS HERE tf.keras.layers.Conv2D(64, (3,3), activation=&#39;relu&#39;, input_shape=(28, 28, 1)), tf.keras.layers.MaxPooling2D(2, 2), tf.keras.layers.Flatten(), tf.keras.layers.Dense(128, activation=&#39;relu&#39;), tf.keras.layers.Dense(10, activation=&#39;softmax&#39;) # YOUR CODE ENDS HERE ]) model.compile(optimizer=&#39;adam&#39;, loss=&#39;sparse_categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) # model fitting history = model.fit( # YOUR CODE STARTS HERE training_images, training_labels, epochs=20, callbacks=[callbacks] # YOUR CODE ENDS HERE ) # model fitting return history.epoch, history.history[&#39;acc&#39;][-1] . _, _ = train_mnist_conv() . Epoch 1/20 60000/60000 [==============================] - 16s 265us/sample - loss: 0.1334 - acc: 0.9595 Epoch 2/20 60000/60000 [==============================] - 16s 267us/sample - loss: 0.0470 - acc: 0.9854 Epoch 3/20 60000/60000 [==============================] - 17s 284us/sample - loss: 0.0298 - acc: 0.9902 Epoch 4/20 60000/60000 [==============================] - 17s 289us/sample - loss: 0.0190 - acc: 0.9937 - l Epoch 5/20 60000/60000 [==============================] - 18s 307us/sample - loss: 0.0141 - acc: 0.9951 Epoch 6/20 60000/60000 [==============================] - 17s 281us/sample - loss: 0.0093 - acc: 0.9966 Epoch 7/20 60000/60000 [==============================] - 17s 285us/sample - loss: 0.0083 - acc: 0.9972 Epoch 8/20 59968/60000 [============================&gt;.] - ETA: 0s - loss: 0.0044 - acc: 0.9986 Reached 99.8% accuracy so cancelling training! 60000/60000 [==============================] - 17s 285us/sample - loss: 0.0044 - acc: 0.9986 .",
            "url": "https://rafaelsf80.github.io/notebooks/computer%20vision/2020/08/01/fashion-mnist-conv.html",
            "relUrl": "/computer%20vision/2020/08/01/fashion-mnist-conv.html",
            "date": " • Aug 1, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "AutoML NLP Classifier with a large confussion matrix",
            "content": "This notebook shows how to predict an existing trained model using AutoML NLP, in order to manually get bigger than 10x10 confussion matrix, which is the maximum supported by the AutoML service . We will use a public dataset about Stack Overflow questions available in Google Cloud marketplace, that has been trained using AutoML NLP. You can explore the dataset in BigQuery just by following the instructions of the former link. In this notebook,the model is already built and deployed in AutoML NLP service. To keep things simple our pre-processed table includes questions containing 4 possible programming-related tags: Java, Javascript, Python or C#. Confussion matrix bigger than 10x10 will be implied. . BigQuery has a public dataset that includes more than 17 million Stack Overflow questions. We are going to download some posts labeled as one of the four most used languages today: java, javascript, python and C#, but to make this a harder problem to our model, we have replaced every instance of that word with another less used language today (but well-known some decades ago) called blank. Otherwise, it will be very easy for the model to detect that a post is a java-related post just by finding the word java on it. . You can access the pre-processed fortran-filled dataset as a tar file here. Each of the four labels has approximate 10k samples for training/eval and 10k samples for test. . Authenticate in Google Cloud from Colab . from google.colab import auth auth.authenticate_user() print(&#39;Authenticated&#39;) . Batch predict on AutoML NLP . We will make a batch predict on an already deployed AutoML NLP model . # Batch predict for an already trained AutoML NLP model from google.cloud import automl project_id = &quot;windy-site-254307&quot; model_id = &quot;TCN627409922111307776&quot; input_uri = &quot;gs://stackoverflow-automl-nlp/dataset_batchpredict.csv&quot; output_uri = &quot;gs://stackoverflow-automl-nlp&quot; prediction_client = automl.PredictionServiceClient() # Get the full path of the model. model_full_id = prediction_client.model_path( project_id, &quot;us-central1&quot;, model_id ) gcs_source = automl.types.GcsSource(input_uris=[input_uri]) input_config = automl.types.BatchPredictInputConfig(gcs_source=gcs_source) gcs_destination = automl.types.GcsDestination(output_uri_prefix=output_uri) output_config = automl.types.BatchPredictOutputConfig( gcs_destination=gcs_destination ) response = prediction_client.batch_predict( model_full_id, input_config, output_config ) print(&quot;Waiting for operation to complete...&quot;) print( &quot;Batch Prediction results saved to Cloud Storage bucket. {}&quot;.format( response.result() ) ) . # Download results from GCS. See first line for reference and modify URI accordingly result = !gsutil ls gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z file_list = [] for i in range(len(result)): file = result[i] !gsutil cp $file . file_list.append( result[i].split(&#39;/&#39;)[4].strip() ) . Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_1.jsonl... Operation completed over 1 objects/120.0 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_10.jsonl... Operation completed over 1 objects/221.5 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_11.jsonl... Operation completed over 1 objects/96.5 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_12.jsonl... Operation completed over 1 objects/24.6 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_13.jsonl... Operation completed over 1 objects/26.7 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_14.jsonl... Operation completed over 1 objects/10.7 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_15.jsonl... Operation completed over 1 objects/37.2 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_16.jsonl... Operation completed over 1 objects/60.5 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_17.jsonl... Operation completed over 1 objects/21.3 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_18.jsonl... Operation completed over 1 objects/14.8 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_19.jsonl... Operation completed over 1 objects/47.1 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_2.jsonl... Operation completed over 1 objects/87.8 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_20.jsonl... Operation completed over 1 objects/27.4 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_21.jsonl... Operation completed over 1 objects/8.2 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_22.jsonl... Operation completed over 1 objects/6.6 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_23.jsonl... Operation completed over 1 objects/206.2 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_24.jsonl... Operation completed over 1 objects/199.6 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_25.jsonl... Operation completed over 1 objects/60.5 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_26.jsonl... Operation completed over 1 objects/46.7 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_27.jsonl... Operation completed over 1 objects/59.4 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_28.jsonl... Operation completed over 1 objects/31.6 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_29.jsonl... Operation completed over 1 objects/9.2 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_3.jsonl... Operation completed over 1 objects/107.3 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_30.jsonl... Operation completed over 1 objects/4.1 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_31.jsonl... Operation completed over 1 objects/103.1 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_32.jsonl... Operation completed over 1 objects/94.4 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_4.jsonl... Operation completed over 1 objects/18.5 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_5.jsonl... Operation completed over 1 objects/45.5 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_6.jsonl... Operation completed over 1 objects/35.1 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_7.jsonl... Operation completed over 1 objects/64.5 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_8.jsonl... Operation completed over 1 objects/42.6 KiB. Copying gs://stackoverflow-automl-nlp/prediction-stackoverflow_4labels_20200602-2020-06-29T08:49:31.326Z/text_classification_9.jsonl... / [0 files][ 0.0 B/ 98.1 KiB]/ [1 files][ 98.1 KiB/ 98.1 KiB] Operation completed over 1 objects/98.1 KiB. . Process result files . # Example of annotations # [{&#39;annotationSpecId&#39;: &#39;1249570775711612928&#39;, &#39;classification&#39;: {&#39;score&#39;: 0.020207971}, &#39;displayName&#39;: &#39;java&#39;}, {&#39;annotationSpecId&#39;: &#39;2402492280318459904&#39;, &#39;classification&#39;: {&#39;score&#39;: 0.96145684}, &#39;displayName&#39;: &#39;csharp&#39;}, {&#39;annotationSpecId&#39;: &#39;5861256794139000832&#39;, &#39;classification&#39;: {&#39;score&#39;: 0.0013875663000000001}, &#39;displayName&#39;: &#39;javascript&#39;}, {&#39;annotationSpecId&#39;: &#39;7014178298745847808&#39;, &#39;classification&#39;: {&#39;score&#39;: 0.017511099999999998}, &#39;displayName&#39;: &#39;python&#39;}] # Example of textSnippet # {&#39;contentUri&#39;: &#39;gs://stackoverflow-automl-nlp/test/csharp/1003.txt&#39;} import pandas as pd # Init enum from enum import Enum class Language(Enum): java = 0 csharp = 1 javascript = 2 python = 3 # Init vectors for the confussion matrix y_true = [] y_pred = [] # Read downloaded files one by one and generate y_true and y_pred vectos for the confussion matrix for file in file_list: #file=&quot;text_classification_1.jsonl&quot; # Read file df = pd.read_json(file, lines=True) print(&quot;Reading {0} annotations and {1} text snippets of {2}&quot;.format(len(df[&#39;annotations&#39;].to_list()), len(df[&#39;textSnippet&#39;].to_list()), file)) import json for i in range(len(df[&#39;annotations&#39;].to_list())): # Decode textSnippet and get true_label textsnippet_str = str(df[&#39;textSnippet&#39;].to_list()[i]).replace(&quot;&#39;&quot;, &#39;&quot;&#39;) textsnippet_decoded = json.loads(textsnippet_str) true_label = textsnippet_decoded[&#39;contentUri&#39;].split(&#39;/&#39;)[4].strip() y_true.append(true_label) # Decode annotations annotation_str = str(df[&#39;annotations&#39;].to_list()[i]).replace(&quot;&#39;&quot;, &#39;&quot;&#39;) annotation_decoded = json.loads(annotation_str) # Decode scores and add them to the corresponding line in the confussion matrix scores = [annotation_decoded[Language[&#39;java&#39;].value][&#39;classification&#39;][&#39;score&#39;], annotation_decoded[Language[&#39;csharp&#39;].value][&#39;classification&#39;][&#39;score&#39;], annotation_decoded[Language[&#39;javascript&#39;].value][&#39;classification&#39;][&#39;score&#39;], annotation_decoded[Language[&#39;python&#39;].value][&#39;classification&#39;][&#39;score&#39;]] max_value = max(scores) max_index = scores.index(max_value) y_pred.append(Language(max_index).name) . Reading 236 annotations and 236 text snippets of text_classification_1.jsonl Reading 435 annotations and 435 text snippets of text_classification_10.jsonl Reading 190 annotations and 190 text snippets of text_classification_11.jsonl Reading 48 annotations and 48 text snippets of text_classification_12.jsonl Reading 52 annotations and 52 text snippets of text_classification_13.jsonl Reading 21 annotations and 21 text snippets of text_classification_14.jsonl Reading 73 annotations and 73 text snippets of text_classification_15.jsonl Reading 119 annotations and 119 text snippets of text_classification_16.jsonl Reading 42 annotations and 42 text snippets of text_classification_17.jsonl Reading 29 annotations and 29 text snippets of text_classification_18.jsonl Reading 92 annotations and 92 text snippets of text_classification_19.jsonl Reading 172 annotations and 172 text snippets of text_classification_2.jsonl Reading 54 annotations and 54 text snippets of text_classification_20.jsonl Reading 16 annotations and 16 text snippets of text_classification_21.jsonl Reading 13 annotations and 13 text snippets of text_classification_22.jsonl Reading 405 annotations and 405 text snippets of text_classification_23.jsonl Reading 392 annotations and 392 text snippets of text_classification_24.jsonl Reading 119 annotations and 119 text snippets of text_classification_25.jsonl Reading 92 annotations and 92 text snippets of text_classification_26.jsonl Reading 116 annotations and 116 text snippets of text_classification_27.jsonl Reading 62 annotations and 62 text snippets of text_classification_28.jsonl Reading 18 annotations and 18 text snippets of text_classification_29.jsonl Reading 211 annotations and 211 text snippets of text_classification_3.jsonl Reading 8 annotations and 8 text snippets of text_classification_30.jsonl Reading 203 annotations and 203 text snippets of text_classification_31.jsonl Reading 185 annotations and 185 text snippets of text_classification_32.jsonl Reading 36 annotations and 36 text snippets of text_classification_4.jsonl Reading 89 annotations and 89 text snippets of text_classification_5.jsonl Reading 69 annotations and 69 text snippets of text_classification_6.jsonl Reading 127 annotations and 127 text snippets of text_classification_7.jsonl Reading 84 annotations and 84 text snippets of text_classification_8.jsonl Reading 192 annotations and 192 text snippets of text_classification_9.jsonl . Get confusion matrix . from sklearn.metrics import confusion_matrix matriz_de_confusion = confusion_matrix(y_true, y_pred, labels=[&quot;java&quot;, &quot;javascript&quot;, &quot;csharp&quot;, &quot;python&quot;]) #DO NOT USE THIS MATRIX #conf_max_12x12 = confusion_matrix(y_train, y_train_pred) #conf_max_12x12=([[1000, 3, 24, 9, 10, 49, 49, 50, 26, 23, 12, 98 ], # [ 23, 2000, 24, 9, 10, 49, 49, 50, 26, 23, 12, 98 ], # [ 56, 3, 1300, 9, 10, 49, 49, 50, 26, 23, 12, 98 ], # [ 23, 3, 24, 1400, 10, 49, 49, 50, 26, 23, 12, 98 ], # [ 35, 3, 24, 9, 1500, 49, 49, 50, 26, 23, 12, 98 ], # [ 35, 3, 24, 9, 10, 1400, 49, 50, 26, 23, 12, 98 ], # [ 35, 3, 24, 9, 10, 49, 1300, 50, 26, 23, 12, 98 ], # [ 35, 3, 24, 9, 10, 49, 49, 1200, 26, 23, 12, 98 ], # [ 35, 3, 24, 9, 10, 49, 49, 50, 1100, 23, 12, 98 ], # [ 35, 3, 24, 9, 10, 49, 49, 50, 26, 1000, 12, 98 ], # [ 35, 3, 24, 9, 10, 49, 49, 50, 26, 23, 1100, 98 ], # [ 35, 3, 24, 9, 10, 49, 49, 50, 26, 23, 12, 1200 ]]) . #!pip3 install seaborn import matplotlib.pyplot as plt import numpy as np import seaborn as sns # Normalise and Plot target_names = [&#39;java&#39;, &#39;csharp&#39;, &#39;javascript&#39;, &#39;python&#39;] cmn = matriz_de_confusion.astype(&#39;float&#39;) / matriz_de_confusion.sum(axis=1)[:, np.newaxis] fig, ax = plt.subplots(figsize=(10,10)) sns.heatmap(cmn, annot=True, fmt=&#39;.2f&#39;, xticklabels=target_names, yticklabels=target_names) plt.ylabel(&#39;Actual&#39;) plt.xlabel(&#39;Predicted&#39;) plt.show(block=False) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt;",
            "url": "https://rafaelsf80.github.io/notebooks/natural%20language%20processing/2020/06/26/automl-nlp-classifier-with-confussion-matrix.html",
            "relUrl": "/natural%20language%20processing/2020/06/26/automl-nlp-classifier-with-confussion-matrix.html",
            "date": " • Jun 26, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Text Classification using TextVectorization layer",
            "content": "This notebook contains a walkthrough of text classification from scratch, starting from a directory of plain text files (a common scenario in practice). We demonstrate multiclass text classification using a dataset of Stack Overflow questions. . !pip3 install -q tf-nightly import tensorflow as tf . import numpy as np from tensorflow.keras import preprocessing print(tf.__version__) . 2.2.0-dev20200507 . Multiclass text classification . This notebook shows a multicalss classifier of Stack Overflow posts as one of the most used languages today, namely Java, Javascript, Python or C#. This is an example of multiclass classification. . We will use a public dataset about Stack Overflow questions available in Google Cloud marketplace. You can explore the dataset in BigQuery just by following the instructions of the former link. In this notebook, you will build a model to predict the tags of questions from Stack Overflow, using a pre-processed table already built and coming from the BigQuery dataset. To keep things simple our pre-processed table includes questions containing 4 possible programming-related tags: Java, Javascript, Python or Csharp. . This notebook uses tf.keras to build and train models in TensorFlow, as well as some TensorFlow experimental features, like the TextVectorization layer for word splitting &amp; indexing. . Download the BigQuery dataset . BigQuery has a public dataset that includes more than 17 million Stack Overflow questions. We are going to download some posts labeled as one of the four most used languages today: java, javascript, python and C#, but to make this a harder problem to our model, we have replaced every instance of that word with another less used language today (but well-known some decades ago) called blank. Otherwise, it will be very easy for the model to detect that a post is a java-related post just by finding the word java on it. . You can access the pre-processed blank-filled dataset as a tar file here in Google Cloud Storage. Each of the four labels has approximate 10k samples for training/eval and 10k samples for test. . !gsutil cp gs://tensorflow-blog-rnn/so_posts_4labels_blank_80k.tar.gz . !tar -xf so_posts_4labels_blank_80k.tar.gz . Updates are available for some Cloud SDK components. To install them, please run: $ gcloud components update Copying gs://tensorflow-blog-rnn/so_posts_4labels_blank_80k.tar.gz... Operation completed over 1 objects/29.0 MiB. . batch_size = 32 raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory( &#39;train&#39;, batch_size=batch_size, validation_split=0.2, subset=&#39;training&#39;, seed=42) raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory( &#39;train&#39;, batch_size=batch_size, validation_split=0.2, subset=&#39;validation&#39;, seed=42) raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory( &#39;test&#39;, batch_size=batch_size) . Found 40000 files belonging to 4 classes. Using 32000 files for training. Found 40000 files belonging to 4 classes. Using 8000 files for validation. Found 40000 files belonging to 4 classes. . Caching may reduce the processing time. Let&#39;s prove it. . import time start = time.time() for text_batch, label_batch in raw_train_ds: pass end = time.time() print(end - start) . 7.62075400352478 . import time start = time.time() for text_batch, label_batch in raw_train_ds: pass end = time.time() print(end - start) . 1.898169994354248 . raw_train_ds = raw_train_ds.cache() import time start = time.time() for text_batch, label_batch in raw_train_ds: pass end = time.time() print(end - start) . 2.132780075073242 . import time start = time.time() for text_batch, label_batch in raw_train_ds: pass end = time.time() print(end - start) . 0.1296229362487793 . Explore the data . The dataset comes pre-processed, by replacing key words java, javascript, C# or python by blank. In total, there are 4 labels (classes). . Note we can evaluate tensors using .numpy(), thanks to the eager execution of TensorFlow 2: . import time for text_batch, label_batch in raw_train_ds.take(1): for i in range(5): print(text_batch.numpy()[i]) print(label_batch.numpy()[i]) . b&#39;how do i find missing dates in a list of sorted dates? in blank how do i find all the missing days in a sorted list of dates? n&#39; 3 b&#39;&#34;find the sequences of numbers in the list? there is a task to find all the sequences of numbers in the list, then add them another list. for example, there is such a sequence of numbers in list ... 12222533343332...only numbers must appear in the resulting list like this 44 77 88 000 a prerequisite is that repeated numbers must stand side by side .for example, so ... 5122225333433325...5 should not fall into the resulting list because they are not near each other, respectively (not a sequence)..list&amp;lt;integer&amp;gt; toplist = new arraylist&amp;lt;&amp;gt;();. list&amp;lt;integer&amp;gt; result = new arraylist&amp;lt;&amp;gt;();. int count = 0;. boolean flag = true;.. while (count &amp;lt; toplist.size()){. while (flag) {. for (int j = count + 1; j &amp;lt; toplist.size(); j++) {. if (toplist.get(count).equals(toplist.get(j))) {. result.add(toplist.get(j));. system.out.println(result);. flag = false;. }else {. flag = true;. }. }. count++;. }. }...i try to compare the elements in pairs and add them to the sheet, but it is added to a couple of more elements for example instead of 22222, i get 222222. and instead of 333 and one more sequence 333. i get 333 and 33. how can i improve?&#34; n&#39; 1 b&#39;&#34;is there a standard function code like `lambda x, y: x.custom_method(y)`? i know that i can call magic methods using functions from operator module, for example:..operator.add(a, b)...is equal to..a.__add__(b)...is there a standard function for calling a custom method (like operator.methodcaller but also accepts method arguments when called)?.currently i have code like this:..def methodapply(name):. &#34;&#34;&#34;&#34;&#34;&#34;apply a custom method... usage:. methodapply( &#39;some &#39;)(a, *args, **kwargs) =&amp;gt; a.some(*args, **kwargs).. &#34;&#34;&#34;&#34;&#34;&#34;. def func(instance, *args, **kwargs):. return getattr(instance, name)(*args, **kwargs). func.__doc__ = &#34;&#34;&#34;&#34;&#34;&#34;call {!r} instance method&#34;&#34;&#34;&#34;&#34;&#34;.format(name). return func&#34; n&#39; 3 b&#39;&#34;blank: refer to objects dynamically apologies if this is a silly question...i have a list of potential dictionary keys here:.. form_fields = [ &#39;sex &#39;,. &#39;birth &#39;,. &#39;location &#39;,. &#39;politics &#39;]...i am currently manually adding values to these keys like so:.. self.participant.vars[&#34;&#34;sex&#34;&#34;] = [constants.fields_dict[&#34;&#34;sex&#34;&#34;][0], constants.fields_dict[&#34;&#34;sex&#34;&#34;][1], self.player.name]. self.participant.vars[&#34;&#34;birth&#34;&#34;] = [constants.fields_dict[&#34;&#34;birth&#34;&#34;][0], constants.fields_dict[&#34;&#34;birth&#34;&#34;][1],self.player.age]. self.participant.vars[&#34;&#34;location&#34;&#34;] = [constants.fields_dict[&#34;&#34;location&#34;&#34;][0], constants.fields_dict[&#34;&#34;location&#34;&#34;][1],self.player.politics]...i &#39;d like to be able to do a use a for loop to do this all at once like so:..for i in form_fields:. self.participant.vars[i] = [constants.fields_dict[i][0], constants.fields_dict[i][1], self.player.`i`]...obviously, however, i can &#39;t reference the object self.player.i like that. is there a way to reference that object dynamically?&#34; n&#39; 3 b&#39;&#34;list.split output trouble i was practicing some coding and i decided to make a parrot translator. the basic point of this game is, that after every word in a sentence, you should put the syllable &#34;&#34;pa&#34;&#34;. i had written the code for that:.. print(&#34;&#34;this is the parrot translator!&#34;&#34;). original = input(&#34;&#34;please enter a sentence you want to translate: &#34;&#34;).. words = list(original.split()).. for words in words:. print(words + &#34;&#34;pa&#34;&#34;)...but the problem i have and i dont know how to fix is, when i split the sentence, the output wont be in the same line, but every word will be at it &#39;s own.&#34; n&#39; 3 . Each label is an integer value between 0 and 3, correponsing to one of our four labels (0 to 3): . Prepare data for training . Since the data is pre-processed, we do not need to make any additional steps like removing HTML tags, as we did in Part 1 of this notebook. . We can go directly to instantiate our text vectorization layer (experimental feature). We are using this layer to normalize, split, and map strings to integers, so we set our output_mode to int. We also set the same constants as Part 1 for the model, like an explicit maximum sequence_length. . from tensorflow.keras.layers.experimental.preprocessing import TextVectorization max_features = 5000 embedding_dim = 128 sequence_length = 500 vectorize_layer = TextVectorization( max_tokens=max_features, output_mode=&#39;int&#39;, output_sequence_length=sequence_length) . # Make a text-only dataset (no labels) and call adapt text_ds = raw_train_ds.map(lambda x, y: x) vectorize_layer.adapt(text_ds) . Vectorize the data . def vectorize_text(text, label): text = tf.expand_dims(text, -1) return vectorize_layer(text), label # Vectorize the data. train_ds = raw_train_ds.map(vectorize_text) val_ds = raw_val_ds.map(vectorize_text) test_ds = raw_test_ds.map(vectorize_text) # Do async prefetching / buffering of the data for best performance on GPU. train_ds = train_ds.cache().prefetch(buffer_size=10) val_ds = val_ds.cache().prefetch(buffer_size=10) test_ds = test_ds.cache().prefetch(buffer_size=10) . The vectorization layer transforms each input word of a sentence into a numerical representation, i.e. a list of token indices or vocabulary, with size defined by max_features (5000). Note that the output size is fixed, truncated by sequence_length (500), regardless of how many tokens resulted from the previous step, and this will be the input to our model. . Let&#39;s take a moment to understand the output of the vectorization layer. The output of each sentence is fixed to 500 integers, as stated by sequence_length. It should be noted that most of the values are zero, and this is due to the fact that there is no corresponding token in our vocabulary. . for text_batch, label_batch in train_ds.take(1): for i in range(5): print(text_batch.numpy()[i]) print(label_batch.numpy()[i]) . [ 22 40 3 139 490 1037 6 5 53 9 1131 1037 6 16 22 40 3 139 73 2 490 711 6 5 1131 53 9 1037 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] 3 [ 139 2 3705 9 170 6 2 53 66 7 5 604 4 139 73 2 3705 9 170 6 2 53 87 132 181 159 53 12 138 66 7 300 5 846 9 170 6 53 1 170 310 918 6 2 2217 53 46 13 2046 3045 2928 1177 5 1 7 14 1732 170 310 4466 779 80 779 12 138 51 1 91 21 3240 99 2 2217 53 193 208 60 21 2668 115 142 3880 21 5 1 1 15 1790 3831 131 15 1790 29 185 19 248 1008 89 111 185 61 1 111 1008 12 29 172 185 27 172 61 1 172 11 1 1 1 1008 113 54 1008 89 185 3 118 4 501 2 283 6 1491 8 132 181 4 2 1854 23 10 7 407 4 5 1512 9 169 283 12 138 262 9 1 3 41 1 8 262 9 1 8 71 169 846 1 3 41 1 8 1325 22 34 3 2094 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] 1 [ 7 66 5 1013 39 28 46 1390 85 157 1 3 95 14 3 34 148 2146 251 48 303 33 570 415 12 1 1 580 1 66 5 1013 39 12 446 5 648 64 46 1 23 173 1859 64 503 47 1 3 17 28 46 2810 1 1120 5 648 64 1315 1 155 2061 58 1 2061 162 1 155 2061 25 1 1 2061 1 148 296 256 1 25 1432 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] 3 [ 16 1652 4 222 736 4105 11 13 7 5 2685 3811 17 5 53 9 3815 367 623 105 1 1 2954 492 1 35 412 1252 421 128 4 227 623 46 51 1 1 1 1 1 1 1 1 1 1 46 4 32 229 4 40 5 70 5 12 143 4 40 13 73 62 441 46 1 3 6 1 1 1 1 1 241 3 158 339 2 63 1 46 14 7 66 5 81 4 339 14 63 736 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] 3 [ 1 126 718 3 120 1 83 832 8 3 1972 4 112 5 1 1 2 668 311 9 13 253 7 14 151 236 213 6 5 891 56 91 286 2 1 1 3 532 486 2 28 12 14 1 7 2 1 1 562 2389 187 5 891 56 43 4 2175 330 1 12 330 6 330 1 1 2 121 3 17 8 3 130 95 22 4 399 7 47 3 502 2 891 2 126 612 32 6 2 116 72 23 236 213 74 32 62 97 622 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] 3 . Build the model . The input data consists of an array of integer-encoded vocabulary, with a fixed size. The labels to predict are between 0 and 3, so instead of using a binary classifier, we will use a softmax classifier. We compile the model with an Adam optimizer and a different loss function from Part 1 (Sparse categorical crossentropy). . One of the parameters of the embedding layer is max_features+1and not max_features, and the reason is to add an extra token for an unknown word to our vocabulary in the input string. . from tensorflow.keras import layers # A integer input for vocab indices. inputs = tf.keras.Input(shape=(None,), dtype=&#39;int64&#39;) x = layers.Embedding(max_features + 1, embedding_dim)(inputs) x = layers.Bidirectional(layers.LSTM(128))(x) predictions = layers.Dense(4, activation=&#39;softmax&#39;, name=&#39;predictions&#39;)(x) model = tf.keras.Model(inputs, predictions) model.compile( loss=&#39;sparse_categorical_crossentropy&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;]) . Train the model . Train the model by passing the Dataset object to the model&#39;s fit function. Set the number of epochs. . epochs = 5 # Fit the model using the train and test datasets. history = model.fit( train_ds, validation_data=val_ds, epochs=epochs) . Epoch 1/5 225/1000 [=====&gt;........................] - ETA: 11:23 - loss: 1.1989 - accuracy: 0.4439 . KeyboardInterrupt Traceback (most recent call last) &lt;ipython-input-16-25f03af6a77d&gt; in &lt;module&gt; 5 train_ds, 6 validation_data=val_ds, -&gt; 7 epochs=epochs) ~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs) 70 def _method_wrapper(self, *args, **kwargs): 71 if not self._in_multi_worker_mode(): # pylint: disable=protected-access &gt; 72 return method(self, *args, **kwargs) 73 74 # Running inside `run_distribute_coordinator` already. ~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing) 905 batch_size=batch_size): 906 callbacks.on_train_batch_begin(step) --&gt; 907 tmp_logs = train_function(iterator) 908 if data_handler.should_sync: 909 context.async_wait() ~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds) 764 else: 765 compiler = &#34;nonXla&#34; --&gt; 766 result = self._call(*args, **kwds) 767 768 new_tracing_count = self._get_tracing_count() ~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds) 791 # In this case we have created variables on the first call, so we run the 792 # defunned version which is guaranteed to never create variables. --&gt; 793 return self._stateless_fn(*args, **kwds) # pylint: disable=not-callable 794 elif self._stateful_fn is not None: 795 # Release the lock early so that multiple threads can perform the call ~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs) 2810 with self._lock: 2811 graph_function, args, kwargs = self._maybe_define_function(args, kwargs) -&gt; 2812 return graph_function._filtered_call(args, kwargs) # pylint: disable=protected-access 2813 2814 @property ~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs, cancellation_manager) 1836 resource_variable_ops.BaseResourceVariable))), 1837 captured_inputs=self.captured_inputs, -&gt; 1838 cancellation_manager=cancellation_manager) 1839 1840 def _call_flat(self, args, captured_inputs, cancellation_manager=None): ~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager) 1913 # No tape is watching; skip to running the function. 1914 return self._build_call_outputs(self._inference_function.call( -&gt; 1915 ctx, args, cancellation_manager=cancellation_manager)) 1916 forward_backward = self._select_forward_and_backward_functions( 1917 args, ~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager) 547 inputs=args, 548 attrs=attrs, --&gt; 549 ctx=ctx) 550 else: 551 outputs = execute.execute_with_cancellation( ~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name) 58 ctx.ensure_initialized() 59 tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name, &gt; 60 inputs, attrs, num_outputs) 61 except core._NotOkStatusException as e: 62 if name is not None: KeyboardInterrupt: . model.summary() . Evaluate the model . And let&#39;s see how the model performs. Two values will be returned: loss (a number which represents our error, lower values are better), and accuracy. . loss, accuracy = model.evaluate(test_ds) print(&quot;Loss: &quot;, loss) print(&quot;Accuracy: &quot;, accuracy) . Learn more . This notebook uses tf.keras, a high-level API to build and train models in TensorFlow. For a more advanced text classification tutorial using tf.keras, see the MLCC Text Classification Guide. In this notebook, we also use some TensorFlow experimental features, like the TextVectorization layer for word splitting &amp; indexing. .",
            "url": "https://rafaelsf80.github.io/notebooks/natural%20language%20processing/2020/06/25/text-classifier-with-textvectorization-layer.html",
            "relUrl": "/natural%20language%20processing/2020/06/25/text-classifier-with-textvectorization-layer.html",
            "date": " • Jun 25, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Text Classification using TextVectorization layer PYTHON ONLY",
            "content": "!pip3 install -q tf-nightly import tensorflow as tf . import numpy as np from tensorflow.keras import preprocessing print(tf.__version__) . Download the BigQuery dataset . !gsutil cp gs://tensorflow-blog-rnn/so_posts_4labels_blank_80k.tar.gz . !tar -xf so_posts_4labels_blank_80k.tar.gz . batch_size = 32 raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory( &#39;train&#39;, batch_size=batch_size, validation_split=0.2, subset=&#39;training&#39;, seed=42) raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory( &#39;train&#39;, batch_size=batch_size, validation_split=0.2, subset=&#39;validation&#39;, seed=42) raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory( &#39;test&#39;, batch_size=batch_size) . Explore the data . import time for text_batch, label_batch in raw_train_ds.take(1): for i in range(5): print(text_batch.numpy()[i]) print(label_batch.numpy()[i]) . Prepare data for training . from tensorflow.keras.layers.experimental.preprocessing import TextVectorization max_features = 5000 embedding_dim = 128 sequence_length = 500 vectorize_layer = TextVectorization( max_tokens=max_features, output_mode=&#39;int&#39;, output_sequence_length=sequence_length) . # Make a text-only dataset (no labels) and call adapt text_ds = raw_train_ds.map(lambda x, y: x) vectorize_layer.adapt(text_ds) . Vectorize the data . def vectorize_text(text, label): text = tf.expand_dims(text, -1) return vectorize_layer(text), label # Vectorize the data. train_ds = raw_train_ds.map(vectorize_text) val_ds = raw_val_ds.map(vectorize_text) test_ds = raw_test_ds.map(vectorize_text) # Do async prefetching / buffering of the data for best performance on GPU. train_ds = train_ds.cache().prefetch(buffer_size=10) val_ds = val_ds.cache().prefetch(buffer_size=10) test_ds = test_ds.cache().prefetch(buffer_size=10) . for text_batch, label_batch in train_ds.take(1): for i in range(5): print(text_batch.numpy()[i]) print(label_batch.numpy()[i]) . Build the model . from tensorflow.keras import layers # A integer input for vocab indices. inputs = tf.keras.Input(shape=(None,), dtype=&#39;int64&#39;) x = layers.Embedding(max_features + 1, embedding_dim)(inputs) x = layers.Bidirectional(layers.LSTM(128))(x) predictions = layers.Dense(4, activation=&#39;softmax&#39;, name=&#39;predictions&#39;)(x) model = tf.keras.Model(inputs, predictions) model.compile( loss=&#39;sparse_categorical_crossentropy&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;]) . Train the model . epochs = 5 # Fit the model using the train and test datasets. history = model.fit( train_ds, validation_data=val_ds, epochs=epochs) . model.summary() . Evaluate the model . loss, accuracy = model.evaluate(test_ds) print(&quot;Loss: &quot;, loss) print(&quot;Accuracy: &quot;, accuracy) .",
            "url": "https://rafaelsf80.github.io/notebooks/natural%20language%20processing/2020/06/25/text-classifier-with-textvectorization-layer-code-ONLY.html",
            "relUrl": "/natural%20language%20processing/2020/06/25/text-classifier-with-textvectorization-layer-code-ONLY.html",
            "date": " • Jun 25, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Explainable AI in Spanish",
            "content": "Introducci&#243;n . Aviso: la versión original en inglés de este notebook está en este Codelab . Instalaci&#243;n . import itertools import numpy as np import pandas as pd import tensorflow as tf import json import matplotlib.pyplot as plt from sklearn.utils import shuffle from sklearn.metrics import confusion_matrix . Carga de datos . Es un dataset sintético de Kaggle, con trasacciones fraudulentas. Son 6.3 millones de registros, y 8000 son fraudulentas (sólo el 0.1!) . !gsutil cp gs://financial_fraud_detection/fraud_data_kaggle.csv . . Copying gs://financial_fraud_detection/fraud_data_kaggle.csv... | [1 files][470.7 MiB/470.7 MiB] Operation completed over 1 objects/470.7 MiB. . data = pd.read_csv(&#39;fraud_data_kaggle.csv&#39;) data.head() data.size . 69988820 . data[&#39;isFraud&#39;].value_counts() . 0 6354407 1 8213 Name: isFraud, dtype: int64 . Corregir datos desbalanceados . Usamos DOWNSAMPLING: consiste en usar todos los fraudulentos (8000) y solo 0.005 (31000) de los no-fraudulentos (clase minoritaria) . fraud = data[data[&#39;isFraud&#39;] == 1] not_fraud = data[data[&#39;isFraud&#39;] == 0] . # Take a random sample of non fraud rows not_fraud_sample = not_fraud.sample(random_state=2, frac=.005) # Put it back together and shuffle df = pd.concat([not_fraud_sample,fraud]) df = shuffle(df, random_state=2) # Remove a few columns (isFraud is the label column we&#39;ll use, not isFlaggedFraud) df = df.drop(columns=[&#39;nameOrig&#39;, &#39;nameDest&#39;, &#39;isFlaggedFraud&#39;]) # Preview the updated dataset df.head() . step type amount oldbalanceOrg newbalanceOrig oldbalanceDest newbalanceDest isFraud . 5777870 400 | PAYMENT | 65839.41 | 0.00 | 0.00 | 0.0 | 0.0 | 0 | . 6362412 726 | TRANSFER | 561446.32 | 561446.32 | 0.00 | 0.0 | 0.0 | 1 | . 5927827 404 | PAYMENT | 3828.08 | 10455.17 | 6627.09 | 0.0 | 0.0 | 0 | . 5987904 410 | TRANSFER | 557950.06 | 557950.06 | 0.00 | 0.0 | 0.0 | 1 | . 5706694 398 | PAYMENT | 1376.57 | 368349.14 | 366972.57 | 0.0 | 0.0 | 0 | . df[&#39;isFraud&#39;].value_counts() . 0 31772 1 8213 Name: isFraud, dtype: int64 . Dividir entre set de entrenamiento y prueba (test) . train_test_split = int(len(df) * .8) train_set = df[:train_test_split] test_set = df[train_test_split:] train_labels = train_set.pop(&#39;isFraud&#39;) test_labels = test_set.pop(&#39;isFraud&#39;) . Definir features . fc = tf.feature_column CATEGORICAL_COLUMNS = [&#39;type&#39;] NUMERIC_COLUMNS = [&#39;step&#39;, &#39;amount&#39;, &#39;oldbalanceOrg&#39;, &#39;newbalanceOrig&#39;, &#39;oldbalanceDest&#39;, &#39;newbalanceDest&#39;] . def one_hot_cat_column(feature_name, vocab): return tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocab)) feature_columns = [] for feature_name in CATEGORICAL_COLUMNS: vocabulary = train_set[feature_name].unique() feature_columns.append(one_hot_cat_column(feature_name, vocabulary)) for feature_name in NUMERIC_COLUMNS: feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.float32)) . Funciones . NUM_EXAMPLES = len(train_labels) def make_input_fn(X, y, n_epochs=None, shuffle=True): def input_fn(): dataset = tf.data.Dataset.from_tensor_slices((dict(X), y)) if shuffle: dataset = dataset.shuffle(NUM_EXAMPLES) dataset = dataset.repeat(n_epochs) dataset = dataset.batch(NUM_EXAMPLES) return dataset return input_fn # Define training and evaluation input functions train_input_fn = make_input_fn(train_set, train_labels) eval_input_fn = make_input_fn(test_set, test_labels, shuffle=False, n_epochs=1) . Entrenar modelo Boosted Tree . n_batches = 1 model = tf.estimator.BoostedTreesClassifier(feature_columns, n_batches_per_layer=n_batches) . INFO:tensorflow:Using default config. WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp8ko2kwtc INFO:tensorflow:Using config: {&#39;_model_dir&#39;: &#39;/tmp/tmp8ko2kwtc&#39;, &#39;_tf_random_seed&#39;: None, &#39;_save_summary_steps&#39;: 100, &#39;_save_checkpoints_steps&#39;: None, &#39;_save_checkpoints_secs&#39;: 600, &#39;_session_config&#39;: allow_soft_placement: true graph_options { rewrite_options { meta_optimizer_iterations: ONE } } , &#39;_keep_checkpoint_max&#39;: 5, &#39;_keep_checkpoint_every_n_hours&#39;: 10000, &#39;_log_step_count_steps&#39;: 100, &#39;_train_distribute&#39;: None, &#39;_device_fn&#39;: None, &#39;_protocol&#39;: None, &#39;_eval_distribute&#39;: None, &#39;_experimental_distribute&#39;: None, &#39;_experimental_max_worker_delay_secs&#39;: None, &#39;_session_creation_timeout_secs&#39;: 7200, &#39;_service&#39;: None, &#39;_cluster_spec&#39;: &lt;tensorflow.python.training.server_lib.ClusterSpec object at 0x7fcb4fb77810&gt;, &#39;_task_type&#39;: &#39;worker&#39;, &#39;_task_id&#39;: 0, &#39;_global_id_in_cluster&#39;: 0, &#39;_master&#39;: &#39;&#39;, &#39;_evaluation_master&#39;: &#39;&#39;, &#39;_is_chief&#39;: True, &#39;_num_ps_replicas&#39;: 0, &#39;_num_worker_replicas&#39;: 1} WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/canned/boosted_trees.py:369: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version. Instructions for updating: The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead. . model.train(train_input_fn, max_steps=100) . WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating: If using Keras pass *_constraint arguments to layers. WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version. Instructions for updating: Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts. INFO:tensorflow:Calling model_fn. WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4271: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version. Instructions for updating: The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead. WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/canned/boosted_trees.py:214: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating: Use `tf.cast` instead. WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/canned/head.py:437: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating: Use `tf.cast` instead. WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where INFO:tensorflow:Done calling model_fn. INFO:tensorflow:Create CheckpointSaverHook. WARNING:tensorflow:Issue encountered when serializing resources. Type is unsupported, or the types of the items don&#39;t match field type in CollectionDef. Note this is a warning and probably safe to ignore. &#39;_Resource&#39; object has no attribute &#39;name&#39; INFO:tensorflow:Graph was finalized. INFO:tensorflow:Running local_init_op. INFO:tensorflow:Done running local_init_op. WARNING:tensorflow:Issue encountered when serializing resources. Type is unsupported, or the types of the items don&#39;t match field type in CollectionDef. Note this is a warning and probably safe to ignore. &#39;_Resource&#39; object has no attribute &#39;name&#39; INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmp8ko2kwtc/model.ckpt. WARNING:tensorflow:Issue encountered when serializing resources. Type is unsupported, or the types of the items don&#39;t match field type in CollectionDef. Note this is a warning and probably safe to ignore. &#39;_Resource&#39; object has no attribute &#39;name&#39; INFO:tensorflow:loss = 0.6931538, step = 0 WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize. INFO:tensorflow:global_step/sec: 1.56855 INFO:tensorflow:loss = 0.022666413, step = 99 (63.755 sec) INFO:tensorflow:Saving checkpoints for 100 into /tmp/tmp8ko2kwtc/model.ckpt. WARNING:tensorflow:Issue encountered when serializing resources. Type is unsupported, or the types of the items don&#39;t match field type in CollectionDef. Note this is a warning and probably safe to ignore. &#39;_Resource&#39; object has no attribute &#39;name&#39; INFO:tensorflow:Loss for final step: 0.022666413. . &lt;tensorflow_estimator.python.estimator.canned.boosted_trees.BoostedTreesClassifier at 0x7fcb4fb71dd0&gt; . result = model.evaluate(eval_input_fn) print(pd.Series(result)) . INFO:tensorflow:Calling model_fn. WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/metrics_impl.py:2026: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating: Deprecated in favor of operator or tf.math.divide. WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to &#34;careful_interpolation&#34; instead. WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to &#34;careful_interpolation&#34; instead. INFO:tensorflow:Done calling model_fn. INFO:tensorflow:Starting evaluation at 2020-06-22T08:11:13Z INFO:tensorflow:Graph was finalized. INFO:tensorflow:Restoring parameters from /tmp/tmp8ko2kwtc/model.ckpt-100 INFO:tensorflow:Running local_init_op. INFO:tensorflow:Done running local_init_op. INFO:tensorflow:Finished evaluation at 2020-06-22-08:11:15 INFO:tensorflow:Saving dict for global step 100: accuracy = 0.9949981, accuracy_baseline = 0.8006753, auc = 0.99874365, auc_precision_recall = 0.99814403, average_loss = 0.024392342, global_step = 100, label/mean = 0.19932474, loss = 0.024392342, precision = 0.97903824, prediction/mean = 0.2005636, recall = 0.9962359 WARNING:tensorflow:Issue encountered when serializing resources. Type is unsupported, or the types of the items don&#39;t match field type in CollectionDef. Note this is a warning and probably safe to ignore. &#39;_Resource&#39; object has no attribute &#39;name&#39; INFO:tensorflow:Saving &#39;checkpoint_path&#39; summary for global step 100: /tmp/tmp8ko2kwtc/model.ckpt-100 accuracy 0.994998 accuracy_baseline 0.800675 auc 0.998744 auc_precision_recall 0.998144 average_loss 0.024392 label/mean 0.199325 loss 0.024392 precision 0.979038 prediction/mean 0.200564 recall 0.996236 global_step 100.000000 dtype: float64 . pred_dicts = list(model.predict(eval_input_fn)) probabilities = pd.Series([pred[&#39;logistic&#39;][0] for pred in pred_dicts]) for i,val in enumerate(probabilities[:30]): print(&#39;Predicted: &#39;, round(val), &#39;Actual: &#39;, test_labels.iloc[i]) print() . INFO:tensorflow:Calling model_fn. INFO:tensorflow:Done calling model_fn. INFO:tensorflow:Graph was finalized. INFO:tensorflow:Restoring parameters from /tmp/tmp8ko2kwtc/model.ckpt-100 INFO:tensorflow:Running local_init_op. INFO:tensorflow:Done running local_init_op. Predicted: 0 Actual: 0 Predicted: 1 Actual: 1 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 1 Actual: 1 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 1 Actual: 1 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 Predicted: 0 Actual: 0 . Confusion matrix . y_pred = [] for i in probabilities.values: y_pred.append(int(round(i))) . cm = confusion_matrix(test_labels.values, y_pred) print(cm) . [[6369 34] [ 6 1588]] . def plot_confusion_matrix(cm, classes, normalize=False, title=&#39;Confusion matrix&#39;, cmap=plt.cm.Blues): &quot;&quot;&quot; This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. &quot;&quot;&quot; plt.imshow(cm, interpolation=&#39;nearest&#39;, cmap=cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes, rotation=45) plt.yticks(tick_marks, classes) if normalize: cm = np.round(cm.astype(&#39;float&#39;) / cm.sum(axis=1)[:, np.newaxis], 3) thresh = cm.max() / 2. for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): plt.text(j, i, cm[i, j], horizontalalignment=&quot;center&quot;, color=&quot;white&quot; if cm[i, j] &gt; thresh else &quot;black&quot;) plt.tight_layout() plt.ylabel(&#39;True label&#39;) plt.xlabel(&#39;Predicted label&#39;) . classes = [&#39;not fraud&#39;, &#39;fraud&#39;] plot_confusion_matrix(cm, classes, normalize=True) . Preparando el despliegue del modelo en AI Platform (con Explainability) . GCP_PROJECT = &#39;windy-site-254307&#39; MODEL_BUCKET = &#39;gs://fraud-detection-explainable-ai&#39; !gsutil mb $MODEL_BUCKET . Creating gs://fraud-detection-explainable-ai/... ServiceException: 409 Bucket fraud-detection-explainable-ai already exists. . Exportamos el modelo en TensorFlow 1.x . import tensorflow.compat.v1 as tf tf.disable_v2_behavior() def json_serving_input_fn(): inputs = {} for feat in feature_columns: if feat.name == &quot;type_indicator&quot;: inputs[&#39;type&#39;] = tf.placeholder(shape=[None], name=feat.name, dtype=tf.string) else: inputs[feat.name] = tf.placeholder(shape=[None], name=feat.name, dtype=feat.dtype) return tf.estimator.export.ServingInputReceiver(inputs, inputs) export_path = model.export_saved_model( MODEL_BUCKET + &#39;/explanations&#39;, serving_input_receiver_fn=json_serving_input_fn ).decode(&#39;utf-8&#39;) tf.enable_v2_behavior() . !saved_model_cli show --dir $export_path --all . MetaGraphDef with tag-set: &#39;serve&#39; contains the following SignatureDefs: signature_def[&#39;predict&#39;]: The given SavedModel SignatureDef contains the following input(s): inputs[&#39;amount&#39;] tensor_info: dtype: DT_FLOAT shape: (-1) name: amount:0 inputs[&#39;newbalanceDest&#39;] tensor_info: dtype: DT_FLOAT shape: (-1) name: newbalanceDest:0 inputs[&#39;newbalanceOrig&#39;] tensor_info: dtype: DT_FLOAT shape: (-1) name: newbalanceOrig:0 inputs[&#39;oldbalanceDest&#39;] tensor_info: dtype: DT_FLOAT shape: (-1) name: oldbalanceDest:0 inputs[&#39;oldbalanceOrg&#39;] tensor_info: dtype: DT_FLOAT shape: (-1) name: oldbalanceOrg:0 inputs[&#39;step&#39;] tensor_info: dtype: DT_FLOAT shape: (-1) name: step:0 inputs[&#39;type&#39;] tensor_info: dtype: DT_STRING shape: (-1) name: type_indicator:0 The given SavedModel SignatureDef contains the following output(s): outputs[&#39;all_class_ids&#39;] tensor_info: dtype: DT_INT32 shape: (-1, 2) name: boosted_trees/head/predictions/Tile:0 outputs[&#39;all_classes&#39;] tensor_info: dtype: DT_STRING shape: (-1, 2) name: boosted_trees/head/predictions/Tile_1:0 outputs[&#39;class_ids&#39;] tensor_info: dtype: DT_INT64 shape: (-1, 1) name: boosted_trees/head/predictions/ExpandDims:0 outputs[&#39;classes&#39;] tensor_info: dtype: DT_STRING shape: (-1, 1) name: boosted_trees/head/predictions/str_classes:0 outputs[&#39;logistic&#39;] tensor_info: dtype: DT_FLOAT shape: (-1, 1) name: boosted_trees/head/predictions/logistic:0 outputs[&#39;logits&#39;] tensor_info: dtype: DT_FLOAT shape: (-1, 1) name: boosted_trees/BoostedTreesPredict:0 outputs[&#39;probabilities&#39;] tensor_info: dtype: DT_FLOAT shape: (-1, 2) name: boosted_trees/head/predictions/probabilities:0 Method name is: tensorflow/serving/predict . not_fraud.median() . step 239.00 amount 74684.72 oldbalanceOrg 14069.00 newbalanceOrig 0.00 oldbalanceDest 133311.80 newbalanceDest 214881.70 isFraud 0.00 isFlaggedFraud 0.00 dtype: float64 . not_fraud[&#39;type&#39;].value_counts() . CASH_OUT 2233384 PAYMENT 2151495 CASH_IN 1399284 TRANSFER 528812 DEBIT 41432 Name: type, dtype: int64 . !gsutil cp explanation_metadata.json $export_path . Copying file://explanation_metadata.json [Content-Type=application/json]... / [1 files][ 718.0 B/ 718.0 B] Operation completed over 1 objects/718.0 B. . !gsutil cp gs://fraud-detection-explainable-ai/explanations/1592218671/explanation_metadata.json . !gsutil cp explanation_metadata.json $export_path . Despliegue e AI Platform explanations . MODEL = &#39;fraud_detection_4&#39; VERSION = &#39;v3&#39; . !gcloud ai-platform models create $MODEL . WARNING: Using endpoint [https://ml.googleapis.com/] WARNING: Please explicitly specify a region. Using [us-central1] by default on https://ml.googleapis.com. Please note that your model will be inaccessible from https://us-central1-ml.googelapis.com Learn more about regional endpoints and see a list of available regions: https://cloud.google.com/ai-platform/prediction/docs/regional-endpoints ERROR: (gcloud.ai-platform.models.create) Resource in project [windy-site-254307] is the subject of a conflict: Field: model.name Error: A model with the same name already exists. - &#39;@type&#39;: type.googleapis.com/google.rpc.BadRequest fieldViolations: - description: A model with the same name already exists. field: model.name . !gcloud beta ai-platform versions create v3 --model $MODEL --origin $export_path --runtime-version 1.15 --framework TENSORFLOW --python-version 3.7 --machine-type n1-standard-4 --explanation-method &#39;sampled-shapley&#39; --num-paths 10 . WARNING: Using endpoint [https://ml.googleapis.com/] Explanations reflect patterns in your model, but don&#39;t necessarily reveal fundamental relationships about your data population. See https://cloud.google.com/ml-engine/docs/ai-explanations/limitations for more information. ERROR: (gcloud.beta.ai-platform.versions.create) ALREADY_EXISTS: Field: version.name Error: A version with the same name already exists. - &#39;@type&#39;: type.googleapis.com/google.rpc.BadRequest fieldViolations: - description: A version with the same name already exists. field: version.name . !gcloud ai-platform versions describe $VERSION --model $MODEL . WARNING: Using endpoint [https://ml.googleapis.com/] createTime: &#39;2020-06-15T14:20:08Z&#39; deploymentUri: gs://fraud-detection-explainable-ai/explanations/1592230714 etag: yuDcVfyBSAQ= explanationConfig: sampledShapleyAttribution: numPaths: 10 framework: TENSORFLOW isDefault: true lastUseTime: &#39;2020-06-18T07:20:48Z&#39; machineType: n1-standard-4 name: projects/windy-site-254307/models/fraud_detection_4/versions/v3 pythonVersion: &#39;3.7&#39; runtimeVersion: &#39;1.15&#39; state: READY . Predicci&#243;n con explicabilidad . Paso 1: preparamos datos (data.txt) . fraud_indices = [] for i,val in enumerate(test_labels): if val == 1: fraud_indices.append(i) . num_test_examples = 5 import numpy as np def convert(o): if isinstance(o, np.generic): return o.item() raise TypeError for i in range(num_test_examples): test_json = {} ex = test_set.iloc[fraud_indices[i]] keys = ex.keys().tolist() vals = ex.values.tolist() for idx in range(len(keys)): test_json[keys[idx]] = vals[idx] print(test_json) with open(&#39;data.txt&#39;, &#39;a&#39;) as outfile: json.dump(test_json, outfile, default=convert) outfile.write(&#39; n&#39;) . {&#39;step&#39;: 476, &#39;type&#39;: &#39;TRANSFER&#39;, &#39;amount&#39;: 1048.63, &#39;oldbalanceOrg&#39;: 1048.63, &#39;newbalanceOrig&#39;: 0.0, &#39;oldbalanceDest&#39;: 0.0, &#39;newbalanceDest&#39;: 0.0} {&#39;step&#39;: 390, &#39;type&#39;: &#39;TRANSFER&#39;, &#39;amount&#39;: 638693.49, &#39;oldbalanceOrg&#39;: 638693.49, &#39;newbalanceOrig&#39;: 0.0, &#39;oldbalanceDest&#39;: 0.0, &#39;newbalanceDest&#39;: 0.0} {&#39;step&#39;: 355, &#39;type&#39;: &#39;CASH_OUT&#39;, &#39;amount&#39;: 5338162.8, &#39;oldbalanceOrg&#39;: 5338162.8, &#39;newbalanceOrig&#39;: 0.0, &#39;oldbalanceDest&#39;: 181895.58, &#39;newbalanceDest&#39;: 5520058.37} {&#39;step&#39;: 356, &#39;type&#39;: &#39;TRANSFER&#39;, &#39;amount&#39;: 357226.8, &#39;oldbalanceOrg&#39;: 357226.8, &#39;newbalanceOrig&#39;: 0.0, &#39;oldbalanceDest&#39;: 0.0, &#39;newbalanceDest&#39;: 0.0} {&#39;step&#39;: 345, &#39;type&#39;: &#39;TRANSFER&#39;, &#39;amount&#39;: 128936.95, &#39;oldbalanceOrg&#39;: 128936.95, &#39;newbalanceOrig&#39;: 0.0, &#39;oldbalanceDest&#39;: 0.0, &#39;newbalanceDest&#39;: 0.0} . !cat data.txt . {&#34;step&#34;: 476, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 1048.63, &#34;oldbalanceOrg&#34;: 1048.63, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 390, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 638693.49, &#34;oldbalanceOrg&#34;: 638693.49, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 355, &#34;type&#34;: &#34;CASH_OUT&#34;, &#34;amount&#34;: 5338162.8, &#34;oldbalanceOrg&#34;: 5338162.8, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 181895.58, &#34;newbalanceDest&#34;: 5520058.37} {&#34;step&#34;: 356, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 357226.8, &#34;oldbalanceOrg&#34;: 357226.8, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 345, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 128936.95, &#34;oldbalanceOrg&#34;: 128936.95, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 476, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 1048.63, &#34;oldbalanceOrg&#34;: 1048.63, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 390, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 638693.49, &#34;oldbalanceOrg&#34;: 638693.49, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 355, &#34;type&#34;: &#34;CASH_OUT&#34;, &#34;amount&#34;: 5338162.8, &#34;oldbalanceOrg&#34;: 5338162.8, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 181895.58, &#34;newbalanceDest&#34;: 5520058.37} {&#34;step&#34;: 356, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 357226.8, &#34;oldbalanceOrg&#34;: 357226.8, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 345, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 128936.95, &#34;oldbalanceOrg&#34;: 128936.95, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 476, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 1048.63, &#34;oldbalanceOrg&#34;: 1048.63, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 390, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 638693.49, &#34;oldbalanceOrg&#34;: 638693.49, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 355, &#34;type&#34;: &#34;CASH_OUT&#34;, &#34;amount&#34;: 5338162.8, &#34;oldbalanceOrg&#34;: 5338162.8, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 181895.58, &#34;newbalanceDest&#34;: 5520058.37} {&#34;step&#34;: 356, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 357226.8, &#34;oldbalanceOrg&#34;: 357226.8, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 345, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 128936.95, &#34;oldbalanceOrg&#34;: 128936.95, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 476, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 1048.63, &#34;oldbalanceOrg&#34;: 1048.63, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 390, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 638693.49, &#34;oldbalanceOrg&#34;: 638693.49, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 355, &#34;type&#34;: &#34;CASH_OUT&#34;, &#34;amount&#34;: 5338162.8, &#34;oldbalanceOrg&#34;: 5338162.8, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 181895.58, &#34;newbalanceDest&#34;: 5520058.37} {&#34;step&#34;: 356, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 357226.8, &#34;oldbalanceOrg&#34;: 357226.8, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 345, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 128936.95, &#34;oldbalanceOrg&#34;: 128936.95, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 476, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 1048.63, &#34;oldbalanceOrg&#34;: 1048.63, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 390, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 638693.49, &#34;oldbalanceOrg&#34;: 638693.49, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 355, &#34;type&#34;: &#34;CASH_OUT&#34;, &#34;amount&#34;: 5338162.8, &#34;oldbalanceOrg&#34;: 5338162.8, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 181895.58, &#34;newbalanceDest&#34;: 5520058.37} {&#34;step&#34;: 356, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 357226.8, &#34;oldbalanceOrg&#34;: 357226.8, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 345, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 128936.95, &#34;oldbalanceOrg&#34;: 128936.95, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 476, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 1048.63, &#34;oldbalanceOrg&#34;: 1048.63, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 390, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 638693.49, &#34;oldbalanceOrg&#34;: 638693.49, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 355, &#34;type&#34;: &#34;CASH_OUT&#34;, &#34;amount&#34;: 5338162.8, &#34;oldbalanceOrg&#34;: 5338162.8, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 181895.58, &#34;newbalanceDest&#34;: 5520058.37} {&#34;step&#34;: 356, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 357226.8, &#34;oldbalanceOrg&#34;: 357226.8, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 345, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 128936.95, &#34;oldbalanceOrg&#34;: 128936.95, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 476, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 1048.63, &#34;oldbalanceOrg&#34;: 1048.63, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 390, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 638693.49, &#34;oldbalanceOrg&#34;: 638693.49, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 355, &#34;type&#34;: &#34;CASH_OUT&#34;, &#34;amount&#34;: 5338162.8, &#34;oldbalanceOrg&#34;: 5338162.8, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 181895.58, &#34;newbalanceDest&#34;: 5520058.37} {&#34;step&#34;: 356, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 357226.8, &#34;oldbalanceOrg&#34;: 357226.8, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 345, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 128936.95, &#34;oldbalanceOrg&#34;: 128936.95, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 476, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 1048.63, &#34;oldbalanceOrg&#34;: 1048.63, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 390, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 638693.49, &#34;oldbalanceOrg&#34;: 638693.49, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 355, &#34;type&#34;: &#34;CASH_OUT&#34;, &#34;amount&#34;: 5338162.8, &#34;oldbalanceOrg&#34;: 5338162.8, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 181895.58, &#34;newbalanceDest&#34;: 5520058.37} {&#34;step&#34;: 356, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 357226.8, &#34;oldbalanceOrg&#34;: 357226.8, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 345, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 128936.95, &#34;oldbalanceOrg&#34;: 128936.95, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 476, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 1048.63, &#34;oldbalanceOrg&#34;: 1048.63, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 390, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 638693.49, &#34;oldbalanceOrg&#34;: 638693.49, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 355, &#34;type&#34;: &#34;CASH_OUT&#34;, &#34;amount&#34;: 5338162.8, &#34;oldbalanceOrg&#34;: 5338162.8, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 181895.58, &#34;newbalanceDest&#34;: 5520058.37} {&#34;step&#34;: 356, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 357226.8, &#34;oldbalanceOrg&#34;: 357226.8, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} {&#34;step&#34;: 345, &#34;type&#34;: &#34;TRANSFER&#34;, &#34;amount&#34;: 128936.95, &#34;oldbalanceOrg&#34;: 128936.95, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;newbalanceDest&#34;: 0.0} . Step 2: enviamos data.txt al modelo con gcloud beta ai-platform explain . explanations = !gcloud beta ai-platform explain --model $MODEL --version $VERSION --json-instances=&#39;data.txt&#39; --verbosity error . &#39;{ &#34;explanations&#34;: [ { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.07222782395555333, &#34;attributions&#34;: { &#34;amount&#34;: 0.6966777056455612, &#34;newbalanceDest&#34;: 0.1639272928237915, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.16475289762020112, &#34;oldbalanceOrg&#34;: -0.15600235760211945, &#34;step&#34;: 0.1192602276802063, &#34;type&#34;: -0.027230754494667053 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9744548797607422, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.004716743469869621, &#34;attributions&#34;: { &#34;amount&#34;: -0.0019459187984466552, &#34;newbalanceDest&#34;: 0.007967007160186768, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.008247834444046021, &#34;oldbalanceOrg&#34;: 0.9784113168716431, &#34;step&#34;: 0.0, &#34;type&#34;: -0.007150697708129883 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9985994100570679, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.0024439054423102693, &#34;attributions&#34;: { &#34;amount&#34;: -0.002712637186050415, &#34;newbalanceDest&#34;: -0.0005802184343338013, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;oldbalanceOrg&#34;: 0.9835709899663925, &#34;step&#34;: 0.0, &#34;type&#34;: 0.0 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9933480024337769, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.014761328724187625, &#34;attributions&#34;: { &#34;amount&#34;: -0.02181842029094696, &#34;newbalanceDest&#34;: 0.029705092310905457, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.038540315628051755, &#34;oldbalanceOrg&#34;: 0.9493075281381607, &#34;step&#34;: 0.0, &#34;type&#34;: -0.011098328232765197 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9977060556411743, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.036956189992043476, &#34;attributions&#34;: { &#34;amount&#34;: -0.05543297529220581, &#34;newbalanceDest&#34;: 0.09204374849796296, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.09319761395454407, &#34;oldbalanceOrg&#34;: 0.8222974985837936, &#34;step&#34;: 0.0, &#34;type&#34;: 0.028654450178146364 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9938302040100098, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.07222782395555333, &#34;attributions&#34;: { &#34;amount&#34;: 0.6966777056455612, &#34;newbalanceDest&#34;: 0.1639272928237915, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.16475289762020112, &#34;oldbalanceOrg&#34;: -0.15600235760211945, &#34;step&#34;: 0.1192602276802063, &#34;type&#34;: -0.027230754494667053 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9744548797607422, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.004716743469869621, &#34;attributions&#34;: { &#34;amount&#34;: -0.0019459187984466552, &#34;newbalanceDest&#34;: 0.007967007160186768, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.008247834444046021, &#34;oldbalanceOrg&#34;: 0.9784113168716431, &#34;step&#34;: 0.0, &#34;type&#34;: -0.007150697708129883 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9985994100570679, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.0024439054423102693, &#34;attributions&#34;: { &#34;amount&#34;: -0.002712637186050415, &#34;newbalanceDest&#34;: -0.0005802184343338013, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;oldbalanceOrg&#34;: 0.9835709899663925, &#34;step&#34;: 0.0, &#34;type&#34;: 0.0 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9933480024337769, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.014761328724187625, &#34;attributions&#34;: { &#34;amount&#34;: -0.02181842029094696, &#34;newbalanceDest&#34;: 0.029705092310905457, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.038540315628051755, &#34;oldbalanceOrg&#34;: 0.9493075281381607, &#34;step&#34;: 0.0, &#34;type&#34;: -0.011098328232765197 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9977060556411743, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.036956189992043476, &#34;attributions&#34;: { &#34;amount&#34;: -0.05543297529220581, &#34;newbalanceDest&#34;: 0.09204374849796296, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.09319761395454407, &#34;oldbalanceOrg&#34;: 0.8222974985837936, &#34;step&#34;: 0.0, &#34;type&#34;: 0.028654450178146364 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9938302040100098, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.07222782395555333, &#34;attributions&#34;: { &#34;amount&#34;: 0.6966777056455612, &#34;newbalanceDest&#34;: 0.1639272928237915, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.16475289762020112, &#34;oldbalanceOrg&#34;: -0.15600235760211945, &#34;step&#34;: 0.1192602276802063, &#34;type&#34;: -0.027230754494667053 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9744548797607422, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.004716743469869621, &#34;attributions&#34;: { &#34;amount&#34;: -0.0019459187984466552, &#34;newbalanceDest&#34;: 0.007967007160186768, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.008247834444046021, &#34;oldbalanceOrg&#34;: 0.9784113168716431, &#34;step&#34;: 0.0, &#34;type&#34;: -0.007150697708129883 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9985994100570679, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.0024439054423102693, &#34;attributions&#34;: { &#34;amount&#34;: -0.002712637186050415, &#34;newbalanceDest&#34;: -0.0005802184343338013, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;oldbalanceOrg&#34;: 0.9835709899663925, &#34;step&#34;: 0.0, &#34;type&#34;: 0.0 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9933480024337769, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.014761328724187625, &#34;attributions&#34;: { &#34;amount&#34;: -0.02181842029094696, &#34;newbalanceDest&#34;: 0.029705092310905457, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.038540315628051755, &#34;oldbalanceOrg&#34;: 0.9493075281381607, &#34;step&#34;: 0.0, &#34;type&#34;: -0.011098328232765197 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9977060556411743, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.036956189992043476, &#34;attributions&#34;: { &#34;amount&#34;: -0.05543297529220581, &#34;newbalanceDest&#34;: 0.09204374849796296, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.09319761395454407, &#34;oldbalanceOrg&#34;: 0.8222974985837936, &#34;step&#34;: 0.0, &#34;type&#34;: 0.028654450178146364 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9938302040100098, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.07222782395555333, &#34;attributions&#34;: { &#34;amount&#34;: 0.6966777056455612, &#34;newbalanceDest&#34;: 0.1639272928237915, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.16475289762020112, &#34;oldbalanceOrg&#34;: -0.15600235760211945, &#34;step&#34;: 0.1192602276802063, &#34;type&#34;: -0.027230754494667053 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9744548797607422, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.004716743469869621, &#34;attributions&#34;: { &#34;amount&#34;: -0.0019459187984466552, &#34;newbalanceDest&#34;: 0.007967007160186768, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.008247834444046021, &#34;oldbalanceOrg&#34;: 0.9784113168716431, &#34;step&#34;: 0.0, &#34;type&#34;: -0.007150697708129883 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9985994100570679, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.0024439054423102693, &#34;attributions&#34;: { &#34;amount&#34;: -0.002712637186050415, &#34;newbalanceDest&#34;: -0.0005802184343338013, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;oldbalanceOrg&#34;: 0.9835709899663925, &#34;step&#34;: 0.0, &#34;type&#34;: 0.0 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9933480024337769, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.014761328724187625, &#34;attributions&#34;: { &#34;amount&#34;: -0.02181842029094696, &#34;newbalanceDest&#34;: 0.029705092310905457, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.038540315628051755, &#34;oldbalanceOrg&#34;: 0.9493075281381607, &#34;step&#34;: 0.0, &#34;type&#34;: -0.011098328232765197 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9977060556411743, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.036956189992043476, &#34;attributions&#34;: { &#34;amount&#34;: -0.05543297529220581, &#34;newbalanceDest&#34;: 0.09204374849796296, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.09319761395454407, &#34;oldbalanceOrg&#34;: 0.8222974985837936, &#34;step&#34;: 0.0, &#34;type&#34;: 0.028654450178146364 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9938302040100098, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.07222782395555333, &#34;attributions&#34;: { &#34;amount&#34;: 0.6966777056455612, &#34;newbalanceDest&#34;: 0.1639272928237915, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.16475289762020112, &#34;oldbalanceOrg&#34;: -0.15600235760211945, &#34;step&#34;: 0.1192602276802063, &#34;type&#34;: -0.027230754494667053 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9744548797607422, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.004716743469869621, &#34;attributions&#34;: { &#34;amount&#34;: -0.0019459187984466552, &#34;newbalanceDest&#34;: 0.007967007160186768, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.008247834444046021, &#34;oldbalanceOrg&#34;: 0.9784113168716431, &#34;step&#34;: 0.0, &#34;type&#34;: -0.007150697708129883 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9985994100570679, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.0024439054423102693, &#34;attributions&#34;: { &#34;amount&#34;: -0.002712637186050415, &#34;newbalanceDest&#34;: -0.0005802184343338013, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;oldbalanceOrg&#34;: 0.9835709899663925, &#34;step&#34;: 0.0, &#34;type&#34;: 0.0 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9933480024337769, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.014761328724187625, &#34;attributions&#34;: { &#34;amount&#34;: -0.02181842029094696, &#34;newbalanceDest&#34;: 0.029705092310905457, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.038540315628051755, &#34;oldbalanceOrg&#34;: 0.9493075281381607, &#34;step&#34;: 0.0, &#34;type&#34;: -0.011098328232765197 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9977060556411743, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.036956189992043476, &#34;attributions&#34;: { &#34;amount&#34;: -0.05543297529220581, &#34;newbalanceDest&#34;: 0.09204374849796296, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.09319761395454407, &#34;oldbalanceOrg&#34;: 0.8222974985837936, &#34;step&#34;: 0.0, &#34;type&#34;: 0.028654450178146364 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9938302040100098, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.07222782395555333, &#34;attributions&#34;: { &#34;amount&#34;: 0.6966777056455612, &#34;newbalanceDest&#34;: 0.1639272928237915, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.16475289762020112, &#34;oldbalanceOrg&#34;: -0.15600235760211945, &#34;step&#34;: 0.1192602276802063, &#34;type&#34;: -0.027230754494667053 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9744548797607422, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.004716743469869621, &#34;attributions&#34;: { &#34;amount&#34;: -0.0019459187984466552, &#34;newbalanceDest&#34;: 0.007967007160186768, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.008247834444046021, &#34;oldbalanceOrg&#34;: 0.9784113168716431, &#34;step&#34;: 0.0, &#34;type&#34;: -0.007150697708129883 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9985994100570679, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.0024439054423102693, &#34;attributions&#34;: { &#34;amount&#34;: -0.002712637186050415, &#34;newbalanceDest&#34;: -0.0005802184343338013, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;oldbalanceOrg&#34;: 0.9835709899663925, &#34;step&#34;: 0.0, &#34;type&#34;: 0.0 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9933480024337769, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.014761328724187625, &#34;attributions&#34;: { &#34;amount&#34;: -0.02181842029094696, &#34;newbalanceDest&#34;: 0.029705092310905457, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.038540315628051755, &#34;oldbalanceOrg&#34;: 0.9493075281381607, &#34;step&#34;: 0.0, &#34;type&#34;: -0.011098328232765197 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9977060556411743, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.036956189992043476, &#34;attributions&#34;: { &#34;amount&#34;: -0.05543297529220581, &#34;newbalanceDest&#34;: 0.09204374849796296, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.09319761395454407, &#34;oldbalanceOrg&#34;: 0.8222974985837936, &#34;step&#34;: 0.0, &#34;type&#34;: 0.028654450178146364 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9938302040100098, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.07222782395555333, &#34;attributions&#34;: { &#34;amount&#34;: 0.6966777056455612, &#34;newbalanceDest&#34;: 0.1639272928237915, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.16475289762020112, &#34;oldbalanceOrg&#34;: -0.15600235760211945, &#34;step&#34;: 0.1192602276802063, &#34;type&#34;: -0.027230754494667053 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9744548797607422, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.004716743469869621, &#34;attributions&#34;: { &#34;amount&#34;: -0.0019459187984466552, &#34;newbalanceDest&#34;: 0.007967007160186768, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.008247834444046021, &#34;oldbalanceOrg&#34;: 0.9784113168716431, &#34;step&#34;: 0.0, &#34;type&#34;: -0.007150697708129883 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9985994100570679, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.0024439054423102693, &#34;attributions&#34;: { &#34;amount&#34;: -0.002712637186050415, &#34;newbalanceDest&#34;: -0.0005802184343338013, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;oldbalanceOrg&#34;: 0.9835709899663925, &#34;step&#34;: 0.0, &#34;type&#34;: 0.0 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9933480024337769, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.014761328724187625, &#34;attributions&#34;: { &#34;amount&#34;: -0.02181842029094696, &#34;newbalanceDest&#34;: 0.029705092310905457, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.038540315628051755, &#34;oldbalanceOrg&#34;: 0.9493075281381607, &#34;step&#34;: 0.0, &#34;type&#34;: -0.011098328232765197 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9977060556411743, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.036956189992043476, &#34;attributions&#34;: { &#34;amount&#34;: -0.05543297529220581, &#34;newbalanceDest&#34;: 0.09204374849796296, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.09319761395454407, &#34;oldbalanceOrg&#34;: 0.8222974985837936, &#34;step&#34;: 0.0, &#34;type&#34;: 0.028654450178146364 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9938302040100098, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.07222782395555333, &#34;attributions&#34;: { &#34;amount&#34;: 0.6966777056455612, &#34;newbalanceDest&#34;: 0.1639272928237915, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.16475289762020112, &#34;oldbalanceOrg&#34;: -0.15600235760211945, &#34;step&#34;: 0.1192602276802063, &#34;type&#34;: -0.027230754494667053 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9744548797607422, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.004716743469869621, &#34;attributions&#34;: { &#34;amount&#34;: -0.0019459187984466552, &#34;newbalanceDest&#34;: 0.007967007160186768, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.008247834444046021, &#34;oldbalanceOrg&#34;: 0.9784113168716431, &#34;step&#34;: 0.0, &#34;type&#34;: -0.007150697708129883 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9985994100570679, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.0024439054423102693, &#34;attributions&#34;: { &#34;amount&#34;: -0.002712637186050415, &#34;newbalanceDest&#34;: -0.0005802184343338013, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;oldbalanceOrg&#34;: 0.9835709899663925, &#34;step&#34;: 0.0, &#34;type&#34;: 0.0 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9933480024337769, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.014761328724187625, &#34;attributions&#34;: { &#34;amount&#34;: -0.02181842029094696, &#34;newbalanceDest&#34;: 0.029705092310905457, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.038540315628051755, &#34;oldbalanceOrg&#34;: 0.9493075281381607, &#34;step&#34;: 0.0, &#34;type&#34;: -0.011098328232765197 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9977060556411743, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.036956189992043476, &#34;attributions&#34;: { &#34;amount&#34;: -0.05543297529220581, &#34;newbalanceDest&#34;: 0.09204374849796296, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.09319761395454407, &#34;oldbalanceOrg&#34;: 0.8222974985837936, &#34;step&#34;: 0.0, &#34;type&#34;: 0.028654450178146364 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9938302040100098, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.07222782395555333, &#34;attributions&#34;: { &#34;amount&#34;: 0.6966777056455612, &#34;newbalanceDest&#34;: 0.1639272928237915, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.16475289762020112, &#34;oldbalanceOrg&#34;: -0.15600235760211945, &#34;step&#34;: 0.1192602276802063, &#34;type&#34;: -0.027230754494667053 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9744548797607422, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.004716743469869621, &#34;attributions&#34;: { &#34;amount&#34;: -0.0019459187984466552, &#34;newbalanceDest&#34;: 0.007967007160186768, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.008247834444046021, &#34;oldbalanceOrg&#34;: 0.9784113168716431, &#34;step&#34;: 0.0, &#34;type&#34;: -0.007150697708129883 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9985994100570679, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.0024439054423102693, &#34;attributions&#34;: { &#34;amount&#34;: -0.002712637186050415, &#34;newbalanceDest&#34;: -0.0005802184343338013, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.0, &#34;oldbalanceOrg&#34;: 0.9835709899663925, &#34;step&#34;: 0.0, &#34;type&#34;: 0.0 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9933480024337769, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.014761328724187625, &#34;attributions&#34;: { &#34;amount&#34;: -0.02181842029094696, &#34;newbalanceDest&#34;: 0.029705092310905457, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.038540315628051755, &#34;oldbalanceOrg&#34;: 0.9493075281381607, &#34;step&#34;: 0.0, &#34;type&#34;: -0.011098328232765197 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9977060556411743, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] }, { &#34;attributions_by_label&#34;: [ { &#34;approx_error&#34;: 0.036956189992043476, &#34;attributions&#34;: { &#34;amount&#34;: -0.05543297529220581, &#34;newbalanceDest&#34;: 0.09204374849796296, &#34;newbalanceOrig&#34;: 0.0, &#34;oldbalanceDest&#34;: 0.09319761395454407, &#34;oldbalanceOrg&#34;: 0.8222974985837936, &#34;step&#34;: 0.0, &#34;type&#34;: 0.028654450178146364 }, &#34;baseline_score&#34;: 0.013069868087768555, &#34;example_score&#34;: 0.9938302040100098, &#34;label_index&#34;: 0, &#34;output_name&#34;: &#34;prob&#34; } ] } ] }&#39; . explain_dict = json.loads(explanations.s) . ### Step 3: análisis de datos (notad el baseline) . print(&#39;Model baseline for fraud cases: &#39;, explain_dict[&#39;explanations&#39;][0][&#39;attributions_by_label&#39;][0][&#39;baseline_score&#39;], &#39; n&#39;) . Model baseline for fraud cases: 0.013069868087768555 . for i in explain_dict[&#39;explanations&#39;]: prediction_score = i[&#39;attributions_by_label&#39;][0][&#39;example_score&#39;] attributions = i[&#39;attributions_by_label&#39;][0][&#39;attributions&#39;] print(&#39;Model prediction:&#39;, prediction_score) fig, ax = plt.subplots() ax.barh(list(attributions.keys()), list(attributions.values()), align=&#39;center&#39;) plt.show() . Model prediction: 0.9744548797607422 . Model prediction: 0.9985994100570679 . Model prediction: 0.9933480024337769 . Model prediction: 0.9977060556411743 . Model prediction: 0.9938302040100098 . Model prediction: 0.9744548797607422 . Model prediction: 0.9985994100570679 . Model prediction: 0.9933480024337769 . Model prediction: 0.9977060556411743 . Model prediction: 0.9938302040100098 . Model prediction: 0.9744548797607422 . Model prediction: 0.9985994100570679 . Model prediction: 0.9933480024337769 . Model prediction: 0.9977060556411743 . Model prediction: 0.9938302040100098 . Model prediction: 0.9744548797607422 . Model prediction: 0.9985994100570679 . Model prediction: 0.9933480024337769 . Model prediction: 0.9977060556411743 . Model prediction: 0.9938302040100098 . Model prediction: 0.9744548797607422 . Model prediction: 0.9985994100570679 . Model prediction: 0.9933480024337769 . Model prediction: 0.9977060556411743 . Model prediction: 0.9938302040100098 . Model prediction: 0.9744548797607422 . Model prediction: 0.9985994100570679 . Model prediction: 0.9933480024337769 . Model prediction: 0.9977060556411743 . Model prediction: 0.9938302040100098 . Model prediction: 0.9744548797607422 . Model prediction: 0.9985994100570679 . Model prediction: 0.9933480024337769 . Model prediction: 0.9977060556411743 . Model prediction: 0.9938302040100098 . Model prediction: 0.9744548797607422 . Model prediction: 0.9985994100570679 . Model prediction: 0.9933480024337769 . Model prediction: 0.9977060556411743 . Model prediction: 0.9938302040100098 . Model prediction: 0.9744548797607422 . Model prediction: 0.9985994100570679 . Model prediction: 0.9933480024337769 . Model prediction: 0.9977060556411743 . Model prediction: 0.9938302040100098 .",
            "url": "https://rafaelsf80.github.io/notebooks/structured/data/2020/06/20/xai-fraud-detection.html",
            "relUrl": "/structured/data/2020/06/20/xai-fraud-detection.html",
            "date": " • Jun 20, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "Introduccion a las redes neuronales",
            "content": "Importar TensorFlow . Instalamos TensorFlow. Se puede instalar con pipen Colab, pero el magic command es más rápido. También accesible en este enlace. . %tensorflow_version 2.x import tensorflow as tf print(&quot;You are using TensorFlow version&quot;, tf.__version__) if len(tf.config.list_physical_devices(&#39;GPU&#39;)) &gt; 0: print(&quot;You have a GPU enabled.&quot;) else: print(&quot;Enable a GPU before running this notebook.&quot;) . Colab tiene varias GPUS disponibles (se asigna una aleatoria, dependiendo de la disponibilidad). Para ver tipos de GPUs, se debe ejecutar !nvidia-smi en una celda. . # In this notebook, we&#39;ll use Keras: TensorFlow&#39;s user-friendly API to # define neural networks. Let&#39;s import Keras now. from tensorflow import keras import matplotlib.pyplot as plt . Descargar el dataset de MNIST . MNIST contiene 70,000 imágenes en blanco y negro en 10 categorías. La resolución es baja (28 x 28 pixels). Siempre es importante explorar un dataset antes de usarlo. . dataset = keras.datasets.mnist (train_images, train_labels), (test_images, test_labels) = dataset.load_data() . Hay 60,000 imágenes para entrenar: . print(train_images.shape) . Y 10,000 imágenes en el set de prueba: . print(test_images.shape) . Cada etiqueta es un número entero 0-9: . print(train_labels) . Preprocesar los datos . Normalizamos los valores de píxeles entre 0 y 1. Importante hacerlo tanto en el set de entrenamiento como el de prueba: . train_images = train_images / 255.0 test_images = test_images / 255.0 . Vemos 25 imágenes con sus etiquetas: . plt.figure(figsize=(10,10)) for i in range(25): plt.subplot(5,5,i+1) plt.xticks([]) plt.yticks([]) plt.grid(False) plt.imshow(train_images[i], cmap=plt.cm.binary) plt.xlabel(train_labels[i]) plt.show() . Crear las capas . Neural networks are made up of layers. Here, you&#39;ll define the layers, and assemble them into a model. We will start with a single Dense layer. . What does a layer do? . The basic building block of a neural network is the layer. Layers extract representations from the data fed into them. For example: . The first layer in a network might receives the pixel values as input. From these, it learns to detect edges (combinations of pixels). . | The next layer in the network receives edges as input, and may learn to detect lines (combinations of edges). . | If you added another layer, it might learn to detect shapes (combinations of edges). . | . The &quot;Deep&quot; in &quot;Deep Learning&quot; refers to the depth of the network. Deeper networks can learn increasingly abstract patterns. Roughly, the width of a layer (in terms of the number of neurons) refers to the number of patterns it can learn of each type. . Most of deep learning consists of chaining together simple layers. Most layers, such as tf.keras.layers.Dense, have parameters that are initialized randomly, then tuned (or learned) during training by gradient descent. . # A linear model model = keras.Sequential([ keras.layers.Flatten(input_shape=(28, 28)), keras.layers.Dense(10, activation=&#39;softmax&#39;) ]) . La primera capa, tf.keras.layers.Flatten, transforma el formato de las imágenes desde un array 2D (de 28 x 28 pixels) a uno unidimensional (de 28 * 28 = 784 pixels). Es como aplanar la imagen y poner los pixels en línea. Esta capa no tiene parámetros para aprender y es necesaria porque las capas densas necesitan arrays como entrada. . Después de aplanar la imagen, el modelo tiene una única capa densa. Es una capa densa completamente conectada. La capa densa tiene 10 unidades con una activación tipo softmax, que devuelve un array con 10 notas de probabilidad que suman 1. . Después de clasificar cada imagen, cada neurona contiene una nota (puntuación) con la probabilidad de que la imagen pertenezca a uno de las 10 clases. . Compilar el modelo . Before the model is ready for training, it needs a few more settings. These are added during the model&#39;s compile step: . Loss function — This measures how accurate the model is during training. You want to minimize this function to &quot;steer&quot; the model in the right direction. . Optimizer — This is how the model is updated based on the data it sees and its loss function. . Metrics — Used to monitor the training and testing steps. The following example uses accuracy, the fraction of the images that are correctly classified. . model.compile(optimizer=&#39;adam&#39;, loss=&#39;sparse_categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) . Entrenar el modelo . Training the neural network model requires the following steps: . Feed the training data to the model. In this example, the training data is in the train_images and train_labels arrays. . | The model learns to associate images and labels. . | You ask the model to make predictions about a test set—in this example, the test_images array. . | Verify that the predictions match the labels from the test_labels array. . | To begin training, call the model.fit method — so called because it &quot;fits&quot; the model to the training data: . EPOCHS=10 model.fit(train_images, train_labels, epochs=EPOCHS) . As the model trains, the loss and accuracy metrics are displayed. This model reaches an accuracy of about 0.90 (or 90%) on the training data. Accuracy may be slightly different each time you run this code, since the parameters inside the Dense layer are randomly initialized. . Precisi&#243;n . Next, compare how the model performs on the test dataset: . test_loss, test_acc = model.evaluate(test_images, test_labels) print(&#39; nTest accuracy:&#39;, test_acc) . It turns out that the accuracy on the test dataset is a little less than the accuracy on the training dataset. This gap between training accuracy and test accuracy represents overfitting. Overfitting is when a machine learning model performs worse on new, previously unseen inputs than on the training data. An overfitted model &quot;memorizes&quot; the training data—with less accuracy on testing data. . Realizar una predicci&#243;n . Con el modelo ya entrenado, vamos a realizar una predicción sobre imágenes nuevas . predictions = model.predict(test_images) . Here, the model has predicted the label for each image in the testing set. Let&#39;s take a look at the first prediction: . print(predictions[0]) . A prediction is an array of 10 numbers. They represent the model&#39;s &quot;confidence&quot; that the image corresponds to each of the 10 digits. You can see which label has the highest confidence value: . print(tf.argmax(predictions[0])) .",
            "url": "https://rafaelsf80.github.io/notebooks/computer/vision/2020/06/01/mnist.html",
            "relUrl": "/computer/vision/2020/06/01/mnist.html",
            "date": " • Jun 1, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "Introducción a las redes convolucionales",
            "content": "Redes convolucionales: Cats and Dogs . Se entrenará una CNN que distingue entre imágenes de perros y gatos (clasificación binaria). El dataset está disponible en Kaggle como competición . Descargar dataset . La descarga no se hace sobre WiFi, sino sobre Colab, con lo que debería ser rápida. . import os import tensorflow as tf . # El dataset está en Internet origin = &#39;https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip&#39; path_to_zip = tf.keras.utils.get_file(&#39;cats_and_dogs.zip&#39;, origin=origin, extract=True) path_to_folder = os.path.join(os.path.dirname(path_to_zip), &#39;cats_and_dogs_filtered&#39;) . Contenido del zip descomprimido: . cats_and_dogs_filtered |__ train |______ cats: [cat.0.jpg, cat.1.jpg, cat.2.jpg ....] |______ dogs: [dog.0.jpg, dog.1.jpg, dog.2.jpg ...] |__ validation |______ cats: [cat.2000.jpg, cat.2001.jpg, cat.2002.jpg ....] |______ dogs: [dog.2000.jpg, dog.2001.jpg, dog.2002.jpg ...] . El dataset está dividido en train y validation. Creamos variables que apunten a esos directorios . train_dir = os.path.join(path_to_folder, &#39;train&#39;) validation_dir = os.path.join(path_to_folder, &#39;validation&#39;) . train_cats_dir = os.path.join(train_dir, &#39;cats&#39;) train_dogs_dir = os.path.join(train_dir, &#39;dogs&#39;) validation_cats_dir = os.path.join(validation_dir, &#39;cats&#39;) validation_dogs_dir = os.path.join(validation_dir, &#39;dogs&#39;) . Contamos el número de imágenes . num_cats_tr = len(os.listdir(train_cats_dir)) num_dogs_tr = len(os.listdir(train_dogs_dir)) num_cats_val = len(os.listdir(validation_cats_dir)) num_dogs_val = len(os.listdir(validation_dogs_dir)) total_train = num_cats_tr + num_dogs_tr total_val = num_cats_val + num_dogs_val print(&#39;Total training cat images:&#39;, num_cats_tr) print(&#39;Total training dog images:&#39;, num_dogs_tr) print(&#39;Total validation cat images:&#39;, num_cats_val) print(&#39;Total validation dog images:&#39;, num_dogs_val) print(&#39;&#39;) print(&quot;Total training images:&quot;, total_train) print(&quot;Total validation images:&quot;, total_val) . Hay 3000 imágenes (2000 para entrenar y 1000 para validar). Y está balanceado (mismo número de imágenes de perros y gatos) . Nota: se pueden ejecutar comandos shell en colab (ejemplo, !ls $train_cats_dir). . !ls $train_cats_dir . Mostramos algunas imágenes. . import matplotlib.pyplot as plt . _ = plt.imshow(plt.imread(os.path.join(train_cats_dir, &quot;cat.0.jpg&quot;))) . _ = plt.imshow(plt.imread(os.path.join(train_cats_dir, &quot;cat.1.jpg&quot;))) . Las imágenes tienen distinto tamaño. Hay que igualarlo antes de introducirlas en la red neuronal. . Preprocesado de datos . Para preprocesarva, vamos a: . Leer imágenes de disco. | Decodificar contenido y convertirlo en RGB. | Convertir valores de enteros a coma flotante (float). | Reescalado a valores entre 0 y 1 (mejor para redes neuronales, esto previene posibles overflows al multiplicar por pesos). | . Todas las operacfiones anteriores las realiza la clase ImageDataGenerator del paquete tf.keras. Lee las imágenes y las almacena en arrays. . from tensorflow.keras.preprocessing.image import ImageDataGenerator . # Let&#39;s resize images to this size IMG_HEIGHT = 150 IMG_WIDTH = 150 . # Rescale the pixel values to range between 0 and 1 train_generator = ImageDataGenerator(rescale=1./255) val_generator = ImageDataGenerator(rescale=1./255) . After defining the generators for training and validation images, the flow_from_directory method load images from the disk, applies rescaling, and resizes the images into the required dimensions. . batch_size = 32 # Read a batch of 64 images at each step . train_data_gen = train_generator.flow_from_directory(batch_size=batch_size, directory=train_dir, shuffle=True, target_size=(IMG_HEIGHT, IMG_WIDTH), class_mode=&#39;binary&#39;) . val_data_gen = val_generator.flow_from_directory(batch_size=batch_size, directory=validation_dir, target_size=(IMG_HEIGHT, IMG_WIDTH), class_mode=&#39;binary&#39;) . Usamos generators para mostrar algunas im&#225;genes y sus etiquetas . Next, we will extract a batch of images from the training generator, then plot several of them with matplotlib. The next function returns a batch from the dataset. The return value of next function is in form of (x_train, y_train) where x_train is the pixel values and y_train is the labels. . image_batch, labels_batch = next(train_data_gen) . # The shape will be (32, 150, 150, 3) # This means a list of 32 images, each of which is 150x150x3. # The 3 at the end refers to the R,G,B color channels. # A grayscale image would be (for example) 150x150x1 print(image_batch.shape) . # The shape (32,) means a list of 64 numbers # each of these will either be 0 or 1 print(labels_batch.shape) . # This function will plot images returned by the generator # in a grid with 1 row and 5 columns def plot_images(images): fig, axes = plt.subplots(1, 5, figsize=(10,10)) axes = axes.flatten() for img, ax in zip(images, axes): ax.imshow(img) ax.axis(&#39;off&#39;) plt.tight_layout() plt.show() . plot_images(image_batch[:5]) . Next, let&#39;s retrieve the labels. All images will be labeled either 0 or 1, since this is a binary classification problem. . # Here are the first 5 labels from the dataset # that correspond to the images above print(labels_batch[:5]) . # Here, we can see that &quot;0&quot; maps to cat, # and &quot;1&quot; maps to dog print(train_data_gen.class_indices) . Crear modelo . El modelo tiene 3 capas convolucionales con max pooling. Hay al final una capa completamente conectada con 256 unidades. la salida es 0 ó 1 con una función de activación sigmoid. Si cerca de 1, es un perro, si no, es un gato. . from tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPooling2D from tensorflow.keras.models import Sequential . model = Sequential([ Conv2D(32, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;, input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)), MaxPooling2D(), Conv2D(32, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;), MaxPooling2D(), Conv2D(64, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;), MaxPooling2D(), Flatten(), Dense(256, activation=&#39;relu&#39;), Dense(1, activation=&#39;sigmoid&#39;) ]) . Compilamos el modelo, y seleccionamos el optimizador Adam para el descenso de gradientes, y binary cross entropy para la función de pérdidas (cross entropy mide aproximadamente la distancia entre la predicción de la red y la que querríamos que tuviera). . model.compile(optimizer=&#39;adam&#39;, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) . Vemos un resumen con el método summary: . model.summary() . Notar que este modelo tiene 5M de parámetros (ó pesos). El modelo está listo para entrenar, usando las salidas de antes de ImagedataGenerator . Entrenar el modelo . Use the fit method to train the network. You will train the model for 15 epochs (an epoch is one &quot;sweep&quot; over the training set, where each image is used once to perform a round of gradient descent, and update the models parameters). This will take one to two minutes, so let&#39;s start it now: . epochs = 15 . history = model.fit( train_data_gen, epochs=epochs, validation_data=val_data_gen, ) . Inside model.fit, TensorFlow uses gradient descent to find useful values for all the weights in the model. When you create the model, the weights are initialized randomly, then gradually improved over time. The data generator is used to load batches of data off disk. Then, for each batch: . The model performs a forward pass (the images are classified by the network). | Then, the model performs a backward pass (the error is computed, then each weight is slightly adjusted using gradient descent to improve the accuracy on the next iteration). | . Gradient descent is an iterative process. The longer you train the model, the more accurate it will become on the training set. But, the more likely it is to overfit! Meaning, the model will begin to memorize the training images, rather than learn patterns that enable it generalize to new images not included in the training set. . We can see whether overfitting is present by comparing the accuracy on the training and validation data. | . If you look at the accuracy figures reported above, you should see that training accuracy is over 90%, while validation accuracy is only around 70%. . Comprobar overfitting . El precisión en el set de validación es importante: it helps you estimate how well our model is likely to work on new, unseen data in the future. To see how much overfitting is present (and when it occurs), we will create two plots, one for accuracy, and another for loss. Roughly, loss (or error) is the inverse of accuracy (lower is better). Unlike accuracy, loss takes the confidence of a prediction into account (a confidently wrong predicitions has a higher loss than one that is only slightly wrong). . acc = history.history[&#39;accuracy&#39;] val_acc = history.history[&#39;val_accuracy&#39;] loss = history.history[&#39;loss&#39;] val_loss = history.history[&#39;val_loss&#39;] epochs_range = range(epochs) plt.figure(figsize=(8, 8)) plt.subplot(1, 2, 1) plt.plot(epochs_range, acc, label=&#39;Training Accuracy&#39;) plt.plot(epochs_range, val_acc, label=&#39;Validation Accuracy&#39;) plt.legend(loc=&#39;lower right&#39;) plt.title(&#39;Training and Validation Accuracy&#39;) plt.subplot(1, 2, 2) plt.plot(epochs_range, loss, label=&#39;Training Loss&#39;) plt.plot(epochs_range, val_loss, label=&#39;Validation Loss&#39;) plt.legend(loc=&#39;upper right&#39;) plt.title(&#39;Training and Validation Loss&#39;) plt.show() . Overfitting occurs when the validation loss stops decreasing. In this case, that occurs around epoch 5 (give or take). Your results may be slightly different each time you run this code (since the weights are initialized randomly). . Why does overfitting happen? When there are only a &quot;small&quot; number of training examples, the model sometimes learns from noises or unwanted details, to an extent that it negatively impacts the performance of the model on new examples. It means that the model will have a difficult time &quot;generalizing&quot; on a new dataset (making accurate predictions on images that weren&#39;t included in the training set). . Optional: reducir overfitting . Instructions . In this exercise, you will use data augmentation and dropout to improve your model. Follow along by reading and running the code below. There are two TODOs for you to complete, and a solution is given below. . Data augmentation . Overfitting occurs when there are a &quot;small&quot; number of training examples. One way to fix this problem is to increase the size of the training set, by gathering more data (the larger and more diverse the dataset, the better!) . We can also use a technique called &quot;data augmentation&quot; to increase the size of the training set, by generating new examples from existing ones by applying random transformations (for example, rotation) that yield believable-looking images. . This is especially effective when working with images. For example, our training set may only contain images of cats that are right side up. If our validation set contains images of cats that are upside down, our model may have trouble classifying them correctly. To help teach it that cats can appear in any orientation, we will randomly rotate images from our training set during training. This helps expose the model to more aspects of the data, and can lead to better generalization. . Data augmentation is built into the ImageDataGenerator. You can specifiy different transformations, and it will take care of applying then during the training. . # Let&#39;s create new data generators, this time with # data augmentation enabled train_generator = ImageDataGenerator( rescale=1./255, rotation_range=45, width_shift_range=.15, height_shift_range=.15, horizontal_flip=True, zoom_range=0.5 ) . train_data_gen = train_generator.flow_from_directory(batch_size=32, directory=train_dir, shuffle=True, target_size=(IMG_HEIGHT, IMG_WIDTH), class_mode=&#39;binary&#39;) . The next cell will show how the same training image appears when used with five different types of data augmentation. . augmented_images = [train_data_gen[0][0][0] for i in range(5)] plot_images(augmented_images) . We only apply data augmentation to the training examples, so our validation generator looks the same as before. . val_generator = ImageDataGenerator(rescale=1./255) . val_data_gen = val_generator.flow_from_directory(batch_size=32, directory=validation_dir, target_size=(IMG_HEIGHT, IMG_WIDTH), class_mode=&#39;binary&#39;) . Dropout . Another technique to reduce overfitting is to introduce dropout to the network. Dropout is a form of regularization that makes it more difficult for the network to memorize rare details (instead, it is forced to learn more general patterns). . When you apply dropout to a layer it randomly drops out (set to zero) a number of activations during training. Dropout takes a fractional number as its input value, in the form such as 0.1, 0.2, 0.4, etc. This means dropping out 10%, 20% or 40% of the output units randomly from the applied layer. . When appling 0.1 dropout to a certain layer, it randomly deactivates 10% of the output units in each training epoch. . Create a new model using Dropout. You&#39;ll reuse the model definition from above, and add a Dropout layer. . from tensorflow.keras.layers import Dropout . # TODO: Your code here # Create a new CNN that takes advantage of Dropout. # 1) Reuse the model declared in tutorial above. # 2) Add a new line that says &quot;Dropout(0.2),&quot; immediately # before the line that says &quot;Flatten()&quot;. . Solution . #@title model = Sequential([ Conv2D(32, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;, input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)), MaxPooling2D(), Conv2D(32, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;), MaxPooling2D(), Conv2D(64, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;), MaxPooling2D(), Dropout(0.2), Flatten(), Dense(256, activation=&#39;relu&#39;), Dense(1, activation=&#39;sigmoid&#39;) ]) . After introducing dropout to the network, compile your model and view the layers summary. You should see a Dropout layer right before flatten. . model.compile(optimizer=&#39;adam&#39;, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) model.summary() . Train your new model . Add code to train your new model. Previously, we trained for 15 epochs. You will need to train this new modek for more epochs, as data augmentation and dropout make it more difficult for a CNN to memorize the training data (this is what we want!). . Here, you&#39;ll train this model for 25 epochs. This may take a few minutes, and you may need to train it for longer to reach peak accuracy. If you like, you can continue experimenting with that at home. . epochs = 25 . # TODO: your code here # Add code to call model.fit, using your new # data generators with image augmentation # For reference, see the &quot;Train the model&quot; # section above . Solution . #@title history = model.fit( train_data_gen, epochs=epochs, validation_data=val_data_gen, ) . Evaluate your new model . Finally, let&#39;s again create plots of accuracy and loss (we use these plots often in practice!) Now, compare the loss and accuracy curves for the training and validation data. Were you able to achieve a higher validation accuracy than before? Note that even this model will eventually overfit. To prevent that, we use a technique called early stopping (we stop training when the validation loss is no longer decreasing). . acc = history.history[&#39;accuracy&#39;] val_acc = history.history[&#39;val_accuracy&#39;] loss = history.history[&#39;loss&#39;] val_loss = history.history[&#39;val_loss&#39;] epochs_range = range(epochs) plt.figure(figsize=(8, 8)) plt.subplot(1, 2, 1) plt.plot(epochs_range, acc, label=&#39;Training Accuracy&#39;) plt.plot(epochs_range, val_acc, label=&#39;Validation Accuracy&#39;) plt.legend(loc=&#39;lower right&#39;) plt.title(&#39;Training and Validation Accuracy&#39;) plt.subplot(1, 2, 2) plt.plot(epochs_range, loss, label=&#39;Training Loss&#39;) plt.plot(epochs_range, val_loss, label=&#39;Validation Loss&#39;) plt.legend(loc=&#39;upper right&#39;) plt.title(&#39;Training and Validation Loss&#39;) plt.show() .",
            "url": "https://rafaelsf80.github.io/notebooks/computer%20vision/2020/06/01/cats-dogs.html",
            "relUrl": "/computer%20vision/2020/06/01/cats-dogs.html",
            "date": " • Jun 1, 2020"
        }
        
    
  
    
        ,"post18": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://rafaelsf80.github.io/notebooks/quickrecipes/2020/02/20/test.html",
            "relUrl": "/quickrecipes/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post19": {
            "title": "Transfer Google Drive to GCS in Colab",
            "content": "from google.colab import drive . drive.mount(&#39;/content/drive&#39;) . project_id = &lt;YOUR_PROJECT_ID&gt; . !gcloud config set project $project_id . !gsutil ls . !gcloud auth login . !gsutil ls . !gsutil -m cp -r /content/drive/My Drive/a/06/* gs://BUCKET_NAME/06/ .",
            "url": "https://rafaelsf80.github.io/notebooks/quickrecipes/2020/02/01/transfer-Drive2GCS.html",
            "relUrl": "/quickrecipes/2020/02/01/transfer-Drive2GCS.html",
            "date": " • Feb 1, 2020"
        }
        
    
  
    
        ,"post20": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://rafaelsf80.github.io/notebooks/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Site under construction . This is my Github page . This is my LinkedIn page .",
          "url": "https://rafaelsf80.github.io/notebooks/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://rafaelsf80.github.io/notebooks/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}