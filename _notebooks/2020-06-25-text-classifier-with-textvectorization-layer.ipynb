{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Text Classification using TextVectorization layer\n",
        "> Multiclass text classification from scratchusing the new Keras TextVectorization layer\n",
        "\n",
        "- toc: true\n",
        "- badges: true\n",
        "- comments: true\n",
        "- categories: tensorflow, keras, googlecloud\n",
        "- image: images/chart-preview.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab_type": "text",
        "id": "Ic4_occAAiAT"
      },
      "outputs": [],
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "ioaprt5q5US7"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "yCl0eTNH5RS3",
        "colab": {}
      },
      "source": [
        "#@title MIT License\n",
        "#\n",
        "# Copyright (c) 2017 Fran√ßois Chollet\n",
        "#\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a\n",
        "# copy of this software and associated documentation files (the \"Software\"),\n",
        "# to deal in the Software without restriction, including without limitation\n",
        "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
        "# and/or sell copies of the Software, and to permit persons to whom the\n",
        "# Software is furnished to do so, subject to the following conditions:\n",
        "#\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "#\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
        "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
        "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
        "# DEALINGS IN THE SOFTWARE."
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ItXfxkxvosLH"
      },
      "source": [
        "# Multiclass text classification from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hKY4XMc9o8iB"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/keras/text_classification\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/keras/text_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/text_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/keras/text_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Eg62Pmz3o83v"
      },
      "source": [
        "This notebook contains a walkthrough of text classification from scratch, starting from a directory of plain text files (a common scenario in practice). The first part demonstrates sentiment analysis, using an IMDB dataset. We demonstrate multiclass text classification, using a dataset of Stack Overflow questions.\n",
        "\n",
        "** TODO ** Also in the CL, update the title of the other text classification notebook to use transfer learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8RZOuS9LWQvv",
        "colab": {},
        "tags": []
      },
      "source": [
        "!pip install -q tf-nightly\n",
        "import tensorflow as tf"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "ERROR:root:code for hash md5 was not found.\nTraceback (most recent call last):\n  File \"/usr/local/Cellar/python@2/2.7.16/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py\", line 147, in <module>\n    globals()[__func_name] = __get_hash(__func_name)\n  File \"/usr/local/Cellar/python@2/2.7.16/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py\", line 97, in __get_builtin_constructor\n    raise ValueError('unsupported hash type ' + name)\nValueError: unsupported hash type md5\nERROR:root:code for hash sha1 was not found.\nTraceback (most recent call last):\n  File \"/usr/local/Cellar/python@2/2.7.16/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py\", line 147, in <module>\n    globals()[__func_name] = __get_hash(__func_name)\n  File \"/usr/local/Cellar/python@2/2.7.16/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py\", line 97, in __get_builtin_constructor\n    raise ValueError('unsupported hash type ' + name)\nValueError: unsupported hash type sha1\nERROR:root:code for hash sha224 was not found.\nTraceback (most recent call last):\n  File \"/usr/local/Cellar/python@2/2.7.16/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py\", line 147, in <module>\n    globals()[__func_name] = __get_hash(__func_name)\n  File \"/usr/local/Cellar/python@2/2.7.16/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py\", line 97, in __get_builtin_constructor\n    raise ValueError('unsupported hash type ' + name)\nValueError: unsupported hash type sha224\nERROR:root:code for hash sha256 was not found.\nTraceback (most recent call last):\n  File \"/usr/local/Cellar/python@2/2.7.16/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py\", line 147, in <module>\n    globals()[__func_name] = __get_hash(__func_name)\n  File \"/usr/local/Cellar/python@2/2.7.16/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py\", line 97, in __get_builtin_constructor\n    raise ValueError('unsupported hash type ' + name)\nValueError: unsupported hash type sha256\nERROR:root:code for hash sha384 was not found.\nTraceback (most recent call last):\n  File \"/usr/local/Cellar/python@2/2.7.16/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py\", line 147, in <module>\n    globals()[__func_name] = __get_hash(__func_name)\n  File \"/usr/local/Cellar/python@2/2.7.16/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py\", line 97, in __get_builtin_constructor\n    raise ValueError('unsupported hash type ' + name)\nValueError: unsupported hash type sha384\nERROR:root:code for hash sha512 was not found.\nTraceback (most recent call last):\n  File \"/usr/local/Cellar/python@2/2.7.16/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py\", line 147, in <module>\n    globals()[__func_name] = __get_hash(__func_name)\n  File \"/usr/local/Cellar/python@2/2.7.16/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py\", line 97, in __get_builtin_constructor\n    raise ValueError('unsupported hash type ' + name)\nValueError: unsupported hash type sha512\nTraceback (most recent call last):\n  File \"/usr/local/bin/pip\", line 8, in <module>\n    sys.exit(main())\n  File \"/usr/local/lib/python2.7/site-packages/pip/_internal/cli/main.py\", line 73, in main\n    command = create_command(cmd_name, isolated=(\"--isolated\" in cmd_args))\n  File \"/usr/local/lib/python2.7/site-packages/pip/_internal/commands/__init__.py\", line 96, in create_command\n    module = importlib.import_module(module_path)\n  File \"/usr/local/Cellar/python@2/2.7.16/Frameworks/Python.framework/Versions/2.7/lib/python2.7/importlib/__init__.py\", line 37, in import_module\n    __import__(name)\n  File \"/usr/local/lib/python2.7/site-packages/pip/_internal/commands/install.py\", line 24, in <module>\n    from pip._internal.cli.req_command import RequirementCommand\n  File \"/usr/local/lib/python2.7/site-packages/pip/_internal/cli/req_command.py\", line 15, in <module>\n    from pip._internal.index.package_finder import PackageFinder\n  File \"/usr/local/lib/python2.7/site-packages/pip/_internal/index/package_finder.py\", line 21, in <module>\n    from pip._internal.index.collector import parse_links\n  File \"/usr/local/lib/python2.7/site-packages/pip/_internal/index/collector.py\", line 12, in <module>\n    from pip._vendor import html5lib, requests\n  File \"/usr/local/lib/python2.7/site-packages/pip/_vendor/requests/__init__.py\", line 43, in <module>\n    from pip._vendor import urllib3\n  File \"/usr/local/lib/python2.7/site-packages/pip/_vendor/urllib3/__init__.py\", line 7, in <module>\n    from .connectionpool import HTTPConnectionPool, HTTPSConnectionPool, connection_from_url\n  File \"/usr/local/lib/python2.7/site-packages/pip/_vendor/urllib3/connectionpool.py\", line 29, in <module>\n    from .connection import (\n  File \"/usr/local/lib/python2.7/site-packages/pip/_vendor/urllib3/connection.py\", line 40, in <module>\n    from .util.ssl_ import (\n  File \"/usr/local/lib/python2.7/site-packages/pip/_vendor/urllib3/util/__init__.py\", line 7, in <module>\n    from .ssl_ import (\n  File \"/usr/local/lib/python2.7/site-packages/pip/_vendor/urllib3/util/ssl_.py\", line 8, in <module>\n    from hashlib import md5, sha1, sha256\nImportError: cannot import name md5\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2ew7HTbPpCJH",
        "colab": {},
        "tags": []
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras import preprocessing\n",
        "print(tf.__version__)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2.2.0-dev20200507\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E47jant_rPVu",
        "colab_type": "text"
      },
      "source": [
        "## Multiclass text classification\n",
        "\n",
        "The second part of this notebook classifies Stack Overflow posts as one of the most used languages today, namely Java, Javascript, Python or C#. This is an example of *multiclass* classification, different from the binary classification showed in Part 1.\n",
        "\n",
        "We will use a [public dataset about Stack Overflow questions](https://console.cloud.google.com/marketplace/details/stack-exchange/stack-overflow) available in Google Cloud marketplace. You can explore the dataset in [BigQuery](https://cloud.google.com/bigquery/) just by following the instructions of the former link. In this notebook, you will build a model to predict the tags of questions from Stack Overflow, using a pre-processed table already built and coming from the BigQuery dataset. To keep things simple our pre-processed table includes questions containing 4 possible programming-related tags: Java, Javascript, Python or C#\n",
        "\n",
        "As in Part 1, Part 2 of this notebook uses [tf.keras](https://www.tensorflow.org/guide/keras) to build and train models in TensorFlow, as well as some TensorFlow experimental features, like the `TextVectorization` layer for word splitting & indexing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jxfg2FcfsD8f",
        "colab_type": "text"
      },
      "source": [
        "### Download the BigQuery dataset\n",
        "\n",
        "BigQuery has a  public dataset that includes more than 17 million Stack Overflow questions. We are going to download some posts labeled as one of the four most used languages today: java, javascript, python and C#, but to make this a harder problem to our model, we have replaced every instance of that word with another less used language today (but well-known some decades ago) called `fortran`. Otherwise, it will be very easy for the model to detect that a post is a java-related post just by finding the word `java` on it.\n",
        "\n",
        "You can access the pre-processed fortran-filled dataset as a tar file [here](PENDING). Each of the four labels has approximate 10k samples for training/eval and 10k samples for test.\n",
        "\n",
        "Let's first import libraries to make Part 2 of this notebook independent from Part 1, and then download our pre-processed dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8eRO0ng5r1A",
        "colab_type": "code",
        "colab": {},
        "tags": []
      },
      "source": [
        "!gsutil cp gs://tensorflow-blog-rnn/so_posts_4labels_blank_80k.tar.gz .\n",
        "# TODO: MOVE DATASET TO TF.ORG\n",
        "!tar -xf so_posts_4labels_blank_80k.tar.gz"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n\nUpdates are available for some Cloud SDK components.  To install them,\nplease run:\n  $ gcloud components update\n\nCopying gs://tensorflow-blog-rnn/so_posts_4labels_blank_80k.tar.gz...\n/ [0 files][    0.0 B/ 29.0 MiB]-- [0 files][  2.1 MiB/ 29.0 MiB]\\\\ [0 files][ 13.4 MiB/ 29.0 MiB]|// [0 files][ 24.0 MiB/ 29.0 MiB]-- [1 files][ 29.0 MiB/ 29.0 MiB]\nOperation completed over 1 objects/29.0 MiB.                                     \n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkqtzHOb51ff",
        "colab_type": "code",
        "colab": {},
        "tags": []
      },
      "source": [
        "batch_size = 32\n",
        "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'train', batch_size=batch_size, validation_split=0.2, subset='training', seed=42)\n",
        "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'train', batch_size=batch_size, validation_split=0.2, subset='validation', seed=42)\n",
        "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'test', batch_size=batch_size)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Found 40000 files belonging to 4 classes.\nUsing 32000 files for training.\nFound 40000 files belonging to 4 classes.\nUsing 8000 files for validation.\nFound 40000 files belonging to 4 classes.\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1UH5Prj4cqi",
        "colab_type": "code",
        "colab": {},
        "tags": []
      },
      "source": [
        "import time\n",
        "start = time.time()\n",
        "for text_batch, label_batch in raw_train_ds:\n",
        "    pass\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "8.353302001953125\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UUbMuK35bp_",
        "colab_type": "code",
        "colab": {},
        "tags": []
      },
      "source": [
        "import time\n",
        "start = time.time()\n",
        "for text_batch, label_batch in raw_train_ds:\n",
        "    pass\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2.060072183609009\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZyy_aew4-aD",
        "colab_type": "code",
        "colab": {},
        "tags": []
      },
      "source": [
        "raw_train_ds = raw_train_ds.cache()\n",
        "import time\n",
        "start = time.time()\n",
        "for text_batch, label_batch in raw_train_ds:\n",
        "    pass\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2.046962022781372\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDpqmGDK5Klw",
        "colab_type": "code",
        "colab": {},
        "tags": []
      },
      "source": [
        "import time\n",
        "start = time.time()\n",
        "for text_batch, label_batch in raw_train_ds:\n",
        "    pass\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "0.14096736907958984\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRLs33z1tUi_",
        "colab_type": "text"
      },
      "source": [
        "### Explore the data\n",
        "\n",
        "The dataset comes pre-processed, by replacing key words java, javascript, C# or python by `fortran`. In total, there are 4 labels (classes).\n",
        "\n",
        "Note again that, as in Part 1, we can evaluate tensors using `.numpy()`, thanks to the eager execution of TensorFlow 2:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQlq9CmP6tJX",
        "colab_type": "code",
        "colab": {},
        "tags": []
      },
      "source": [
        "import time\n",
        "\n",
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "  for i in range(5):\n",
        "    print(text_batch.numpy()[i])\n",
        "    print(label_batch.numpy()[i])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "b'how do i find missing dates in a list of sorted dates? in blank how do i find all the missing days in a sorted list of dates?\\n'\n3\nb'\"find the sequences of numbers in the list? there is a task to find all the sequences of numbers in the list, then add them another list. for example, there is such a sequence of numbers in list ...  12222533343332...only numbers must appear in the resulting list like this 44 77 88 000 a prerequisite is that repeated numbers must stand side by side .for example, so ...  5122225333433325...5 should not fall into the resulting list because they are not near each other, respectively (not a sequence)..list&lt;integer&gt; toplist = new arraylist&lt;&gt;();.    list&lt;integer&gt; result = new arraylist&lt;&gt;();.    int count = 0;.    boolean flag = true;..    while (count &lt; toplist.size()){.        while (flag) {.            for (int j = count + 1; j &lt; toplist.size(); j++) {.                if (toplist.get(count).equals(toplist.get(j))) {.                    result.add(toplist.get(j));.                    system.out.println(result);.                    flag = false;.                }else {.                    flag = true;.                }.            }.            count++;.        }.    }...i try to compare the elements in pairs and add them to the sheet, but it is added to a couple of more elements for example instead of 22222, i get 222222. and instead of 333 and one more sequence 333. i get 333 and 33. how can i improve?\"\\n'\n1\nb'\"is there a standard function code like `lambda x, y: x.custom_method(y)`? i know that i can call magic methods using functions from operator module, for example:..operator.add(a, b)...is equal to..a.__add__(b)...is there a standard function for calling a custom method (like operator.methodcaller but also accepts method arguments when called)?.currently i have code like this:..def methodapply(name):.    \"\"\"\"\"\"apply a custom method...    usage:.        methodapply(\\'some\\')(a, *args, **kwargs) =&gt; a.some(*args, **kwargs)..    \"\"\"\"\"\".    def func(instance, *args, **kwargs):.        return getattr(instance, name)(*args, **kwargs).    func.__doc__ = \"\"\"\"\"\"call {!r} instance method\"\"\"\"\"\".format(name).    return func\"\\n'\n3\nb'\"blank: refer to objects dynamically apologies if this is a silly question...i have a list of potential dictionary keys here:.. form_fields = [\\'sex\\',.                \\'birth\\',.                \\'location\\',.                \\'politics\\']...i am currently manually adding values to these keys like so:..        self.participant.vars[\"\"sex\"\"] =  [constants.fields_dict[\"\"sex\"\"][0], constants.fields_dict[\"\"sex\"\"][1], self.player.name].        self.participant.vars[\"\"birth\"\"] = [constants.fields_dict[\"\"birth\"\"][0], constants.fields_dict[\"\"birth\"\"][1],self.player.age].        self.participant.vars[\"\"location\"\"] = [constants.fields_dict[\"\"location\"\"][0], constants.fields_dict[\"\"location\"\"][1],self.player.politics]...i\\'d like to be able to do a use a for loop to do this all at once like so:..for i in form_fields:.    self.participant.vars[i] =  [constants.fields_dict[i][0], constants.fields_dict[i][1], self.player.`i`]...obviously, however, i can\\'t reference the object self.player.i like that.  is there a way to reference that object dynamically?\"\\n'\n3\nb'\"list.split output trouble i was practicing some coding and i decided to make a parrot translator. the basic point of this game is, that after every word in a sentence, you should put the syllable \"\"pa\"\". i had written the code for that:..    print(\"\"this is the parrot translator!\"\").    original = input(\"\"please enter a sentence you want to translate: \"\")..    words = list(original.split())..    for words in words:.         print(words + \"\"pa\"\")...but the problem i have and i dont know how to fix is, when i split the sentence, the output wont be in the same line, but every word will be at it\\'s own.\"\\n'\n3\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFcyzpUoAEXh",
        "colab_type": "text"
      },
      "source": [
        "Each label is an integer value between 0 and 3, correponsing to one of our four labels (0 to 3):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmXSZZBNt8aU",
        "colab_type": "text"
      },
      "source": [
        "### Prepare data for training\n",
        "\n",
        "Since the data is pre-processed, we do not need to make any additional steps like removing HTML tags, as we did in Part 1 of this notebook.\n",
        "\n",
        "We can go directly to instantiate our text vectorization layer (experimental feature). We are using this layer to normalize, split, and map strings to integers, so we set our `output_mode` to `int`. We also set the same constants as Part 1 for the model, like an explicit maximum `sequence_length`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKVrmuB-7fAm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "max_features = 5000\n",
        "embedding_dim = 128\n",
        "sequence_length = 500\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqO9ZzPV7kOL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make a text-only dataset (no labels) and call adapt\n",
        "text_ds = raw_train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(text_ds)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX6zq78yu3Fv",
        "colab_type": "text"
      },
      "source": [
        "### Vectorize the data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3gXSDzt7qni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return vectorize_layer(text), label\n",
        "\n",
        "# Vectorize the data.\n",
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)\n",
        "test_ds = raw_test_ds.map(vectorize_text)\n",
        "\n",
        "# Do async prefetching / buffering of the data for best performance on GPU.\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=10)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNfILbijqG5Z",
        "colab_type": "text"
      },
      "source": [
        "The vectorization layer transforms each input word of a sentence into a numerical representation, i.e. a list of token indices or vocabulary, with size defined by `max_features` (5000). Note that the output size is fixed, truncated by `sequence_length` (500), regardless of how many tokens resulted from the previous step, and this will be the input to our model.\n",
        "\n",
        "Let's take a moment to understand the output of the vectorization layer. The output of each sentence is fixed to 500 integers, as stated by `sequence_length`. It should be noted that most of the values are zero, and this is due to the fact that there is no corresponding token in our vocabulary.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQb51U7gRhYd",
        "colab_type": "code",
        "colab": {},
        "tags": []
      },
      "source": [
        "for text_batch, label_batch in train_ds.take(1):\n",
        "  for i in range(5):\n",
        "    print(text_batch.numpy()[i])\n",
        "    print(label_batch.numpy()[i])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[  22   40    3  139  490 1037    6    5   53    9 1131 1037    6   16\n   22   40    3  139   73    2  490  711    6    5 1131   53    9 1037\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0]\n3\n[ 139    2 3705    9  170    6    2   53   66    7    5  604    4  139\n   73    2 3705    9  170    6    2   53   87  132  181  159   53   12\n  138   66    7  300    5  846    9  170    6   53    1  170  310  918\n    6    2 2217   53   46   13 2046 3045 2928 1177    5    1    7   14\n 1732  170  310 4466  779   80  779   12  138   51    1   91   21 3240\n   99    2 2217   53  193  208   60   21 2668  115  142 3880   21    5\n    1    1   15 1790 3831  131   15 1790   29  185   19  248 1008   89\n  111  185   61    1  111 1008   12   29  172  185   27  172   61    1\n  172   11    1    1    1 1008  113   54 1008   89  185    3  118    4\n  501    2  283    6 1491    8  132  181    4    2 1854   23   10    7\n  407    4    5 1512    9  169  283   12  138  262    9    1    3   41\n    1    8  262    9    1    8   71  169  846    1    3   41    1    8\n 1325   22   34    3 2094    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0]\n1\n[   7   66    5 1013   39   28   46 1390   85  157    1    3   95   14\n    3   34  148 2146  251   48  303   33  570  415   12    1    1  580\n    1   66    5 1013   39   12  446    5  648   64   46    1   23  173\n 1859   64  503   47    1    3   17   28   46 2810    1 1120    5  648\n   64 1315    1  155 2061   58    1 2061  162    1  155 2061   25    1\n    1 2061    1  148  296  256    1   25 1432    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0]\n3\n[  16 1652    4  222  736 4105   11   13    7    5 2685 3811   17    5\n   53    9 3815  367  623  105    1    1 2954  492    1   35  412 1252\n  421  128    4  227  623   46   51    1    1    1    1    1    1    1\n    1    1    1   46    4   32  229    4   40    5   70    5   12  143\n    4   40   13   73   62  441   46    1    3    6    1    1    1    1\n    1  241    3  158  339    2   63    1   46   14    7   66    5   81\n    4  339   14   63  736    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0]\n3\n[   1  126  718    3  120    1   83  832    8    3 1972    4  112    5\n    1    1    2  668  311    9   13  253    7   14  151  236  213    6\n    5  891   56   91  286    2    1    1    3  532  486    2   28   12\n   14    1    7    2    1    1  562 2389  187    5  891   56   43    4\n 2175  330    1   12  330    6  330    1    1    2  121    3   17    8\n    3  130   95   22    4  399    7   47    3  502    2  891    2  126\n  612   32    6    2  116   72   23  236  213   74   32   62   97  622\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0]\n3\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evYhQerxvAq9",
        "colab_type": "text"
      },
      "source": [
        "### Build the model\n",
        "\n",
        "The input data consists of an array of integer-encoded vocabulary, with a fixed size. The labels to predict are between 0 and 3, so instead of using a binary classifier, we will use a softmax classifier. We compile the model with an Adam optimizer and a different loss function from Part 1 (Sparse categorical crossentropy).\n",
        "\n",
        "One of the parameters of the embedding layer is `max_features+1`and not `max_features`, and the reason is to add an extra token for an unknown word to our vocabulary in the input string.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMQEUVGM74iQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "# A integer input for vocab indices.\n",
        "inputs = tf.keras.Input(shape=(None,), dtype='int64')\n",
        "\n",
        "x = layers.Embedding(max_features + 1, embedding_dim)(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(128))(x)\n",
        "predictions = layers.Dense(4, activation='softmax', name='predictions')(x)\n",
        "\n",
        "model = tf.keras.Model(inputs, predictions)\n",
        "\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AHy6MOCvbm9",
        "colab_type": "text"
      },
      "source": [
        "### Train the model\n",
        "\n",
        "Train the model by passing the `Dataset` object to the model's fit function. Set the number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gs78f_N58MO9",
        "colab_type": "code",
        "colab": {},
        "tags": []
      },
      "source": [
        "epochs = 5\n",
        "\n",
        "# Fit the model using the train and test datasets.\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/5\n  11/1000 [..............................] - ETA: 8:49 - loss: 1.3928 - accuracy: 0.2472"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-25f03af6a77d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     epochs=epochs)\n\u001b[0m",
            "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    905\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    906\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 907\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    908\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    764\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    791\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    794\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2810\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2811\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2812\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2814\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1836\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m   1837\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1838\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1840\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1913\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1914\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1915\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1916\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1917\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    550\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5XaJfR1Pucj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hm8g4isKvebz",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate the model\n",
        "\n",
        "And let's see how the model performs. Two values will be returned: loss (a number which represents our error, lower values are better), and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbGYQ8sERUT7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss, accuracy = model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-LTtKU1r7lB",
        "colab_type": "text"
      },
      "source": [
        "## Learn more\n",
        "\n",
        "This notebook uses [tf.keras](https://www.tensorflow.org/guide/keras), a high-level API to build and train models in TensorFlow. For a more advanced text classification tutorial using `tf.keras`, see the [MLCC Text Classification Guide](https://developers.google.com/machine-learning/guides/text-classification/). In this notebook, we also use some TensorFlow experimental features, like the `TextVectorization` layer for word splitting & indexing."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Text classifier from scratch with TextVectorizaon layer.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}