{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Clasificaci贸n de texto usando la nueva capa de vectorizaci贸n\n",
        "> Clasificaci贸n de texto usando la nueva capa de vectorizaci贸n de Keras\n",
        "\n",
        "- toc: true\n",
        "- badges: true\n",
        "- comments: true\n",
        "- categories: [\"Natural language processing\"]\n",
        "- image: images/keras.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Eg62Pmz3o83v"
      },
      "source": [
        "This notebook contains a walkthrough of text classification from scratch, starting from a directory of plain text files (a common scenario in practice). We demonstrate multiclass text classification using a dataset of Stack Overflow questions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8RZOuS9LWQvv",
        "colab": {},
        "tags": []
      },
      "source": [
        "!pip3 install -q tf-nightly\n",
        "import tensorflow as tf"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2ew7HTbPpCJH",
        "colab": {},
        "tags": []
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras import preprocessing\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2.2.0-dev20200507\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E47jant_rPVu",
        "colab_type": "text"
      },
      "source": [
        "## Multiclass text classification\n",
        "\n",
        "This notebook shows a multicalss classifier of Stack Overflow posts as one of the most used languages today, namely Java, Javascript, Python or C#. This is an example of *multiclass* classification.\n",
        "\n",
        "We will use a [public dataset about Stack Overflow questions](https://console.cloud.google.com/marketplace/details/stack-exchange/stack-overflow) available in Google Cloud marketplace. You can explore the dataset in [BigQuery](https://cloud.google.com/bigquery/) just by following the instructions of the former link. In this notebook, you will build a model to predict the tags of questions from Stack Overflow, using a pre-processed table already built and coming from the BigQuery dataset. To keep things simple our pre-processed table includes questions containing 4 possible programming-related tags: Java, Javascript, Python or Csharp.\n",
        "\n",
        "This notebook uses [tf.keras](https://www.tensorflow.org/guide/keras) to build and train models in TensorFlow, as well as some TensorFlow experimental features, like the `TextVectorization` layer for word splitting & indexing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jxfg2FcfsD8f",
        "colab_type": "text"
      },
      "source": [
        "### Download the BigQuery dataset\n",
        "\n",
        "BigQuery has a  public dataset that includes more than 17 million Stack Overflow questions. We are going to download some posts labeled as one of the four most used languages today: java, javascript, python and C#, but to make this a harder problem to our model, we have replaced every instance of that word with another less used language today (but well-known some decades ago) called `blank`. Otherwise, it will be very easy for the model to detect that a post is a java-related post just by finding the word `java` on it.\n",
        "\n",
        "You can access the pre-processed blank-filled dataset as a tar file [here in Google Cloud Storage](gs://tensorflow-blog-rnn/so_posts_4labels_blank_80k.tar.gz). Each of the four labels has approximate 10k samples for training/eval and 10k samples for test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8eRO0ng5r1A",
        "colab_type": "code",
        "colab": {},
        "tags": []
      },
      "source": [
        "!gsutil cp gs://tensorflow-blog-rnn/so_posts_4labels_blank_80k.tar.gz .\n",
        "!tar -xf so_posts_4labels_blank_80k.tar.gz"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n\nUpdates are available for some Cloud SDK components.  To install them,\nplease run:\n  $ gcloud components update\n\nCopying gs://tensorflow-blog-rnn/so_posts_4labels_blank_80k.tar.gz...\n\nOperation completed over 1 objects/29.0 MiB.                                     \n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkqtzHOb51ff",
        "colab_type": "code",
        "colab": {},
        "tags": []
      },
      "source": [
        "batch_size = 32\n",
        "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'train', batch_size=batch_size, validation_split=0.2, subset='training', seed=42)\n",
        "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'train', batch_size=batch_size, validation_split=0.2, subset='validation', seed=42)\n",
        "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'test', batch_size=batch_size)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Found 40000 files belonging to 4 classes.\nUsing 32000 files for training.\nFound 40000 files belonging to 4 classes.\nUsing 8000 files for validation.\nFound 40000 files belonging to 4 classes.\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Caching may reduce the processing time. Let's prove it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1UH5Prj4cqi",
        "colab_type": "code",
        "colab": {},
        "tags": []
      },
      "source": [
        "import time\n",
        "start = time.time()\n",
        "for text_batch, label_batch in raw_train_ds:\n",
        "    pass\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "7.62075400352478\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UUbMuK35bp_",
        "colab_type": "code",
        "colab": {},
        "tags": []
      },
      "source": [
        "import time\n",
        "start = time.time()\n",
        "for text_batch, label_batch in raw_train_ds:\n",
        "    pass\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "1.898169994354248\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZyy_aew4-aD",
        "colab_type": "code",
        "colab": {},
        "tags": []
      },
      "source": [
        "raw_train_ds = raw_train_ds.cache()\n",
        "import time\n",
        "start = time.time()\n",
        "for text_batch, label_batch in raw_train_ds:\n",
        "    pass\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2.132780075073242\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDpqmGDK5Klw",
        "colab_type": "code",
        "colab": {},
        "tags": []
      },
      "source": [
        "import time\n",
        "start = time.time()\n",
        "for text_batch, label_batch in raw_train_ds:\n",
        "    pass\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "0.1296229362487793\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRLs33z1tUi_",
        "colab_type": "text"
      },
      "source": [
        "### Explore the data\n",
        "\n",
        "The dataset comes pre-processed, by replacing key words java, javascript, C# or python by `blank`. In total, there are 4 labels (classes).\n",
        "\n",
        "Note we can evaluate tensors using `.numpy()`, thanks to the eager execution of TensorFlow 2:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQlq9CmP6tJX",
        "colab_type": "code",
        "colab": {},
        "tags": []
      },
      "source": [
        "import time\n",
        "\n",
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "  for i in range(5):\n",
        "    print(text_batch.numpy()[i])\n",
        "    print(label_batch.numpy()[i])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "b'how do i find missing dates in a list of sorted dates? in blank how do i find all the missing days in a sorted list of dates?\\n'\n3\nb'\"find the sequences of numbers in the list? there is a task to find all the sequences of numbers in the list, then add them another list. for example, there is such a sequence of numbers in list ...  12222533343332...only numbers must appear in the resulting list like this 44 77 88 000 a prerequisite is that repeated numbers must stand side by side .for example, so ...  5122225333433325...5 should not fall into the resulting list because they are not near each other, respectively (not a sequence)..list&lt;integer&gt; toplist = new arraylist&lt;&gt;();.    list&lt;integer&gt; result = new arraylist&lt;&gt;();.    int count = 0;.    boolean flag = true;..    while (count &lt; toplist.size()){.        while (flag) {.            for (int j = count + 1; j &lt; toplist.size(); j++) {.                if (toplist.get(count).equals(toplist.get(j))) {.                    result.add(toplist.get(j));.                    system.out.println(result);.                    flag = false;.                }else {.                    flag = true;.                }.            }.            count++;.        }.    }...i try to compare the elements in pairs and add them to the sheet, but it is added to a couple of more elements for example instead of 22222, i get 222222. and instead of 333 and one more sequence 333. i get 333 and 33. how can i improve?\"\\n'\n1\nb'\"is there a standard function code like `lambda x, y: x.custom_method(y)`? i know that i can call magic methods using functions from operator module, for example:..operator.add(a, b)...is equal to..a.__add__(b)...is there a standard function for calling a custom method (like operator.methodcaller but also accepts method arguments when called)?.currently i have code like this:..def methodapply(name):.    \"\"\"\"\"\"apply a custom method...    usage:.        methodapply(\\'some\\')(a, *args, **kwargs) =&gt; a.some(*args, **kwargs)..    \"\"\"\"\"\".    def func(instance, *args, **kwargs):.        return getattr(instance, name)(*args, **kwargs).    func.__doc__ = \"\"\"\"\"\"call {!r} instance method\"\"\"\"\"\".format(name).    return func\"\\n'\n3\nb'\"blank: refer to objects dynamically apologies if this is a silly question...i have a list of potential dictionary keys here:.. form_fields = [\\'sex\\',.                \\'birth\\',.                \\'location\\',.                \\'politics\\']...i am currently manually adding values to these keys like so:..        self.participant.vars[\"\"sex\"\"] =  [constants.fields_dict[\"\"sex\"\"][0], constants.fields_dict[\"\"sex\"\"][1], self.player.name].        self.participant.vars[\"\"birth\"\"] = [constants.fields_dict[\"\"birth\"\"][0], constants.fields_dict[\"\"birth\"\"][1],self.player.age].        self.participant.vars[\"\"location\"\"] = [constants.fields_dict[\"\"location\"\"][0], constants.fields_dict[\"\"location\"\"][1],self.player.politics]...i\\'d like to be able to do a use a for loop to do this all at once like so:..for i in form_fields:.    self.participant.vars[i] =  [constants.fields_dict[i][0], constants.fields_dict[i][1], self.player.`i`]...obviously, however, i can\\'t reference the object self.player.i like that.  is there a way to reference that object dynamically?\"\\n'\n3\nb'\"list.split output trouble i was practicing some coding and i decided to make a parrot translator. the basic point of this game is, that after every word in a sentence, you should put the syllable \"\"pa\"\". i had written the code for that:..    print(\"\"this is the parrot translator!\"\").    original = input(\"\"please enter a sentence you want to translate: \"\")..    words = list(original.split())..    for words in words:.         print(words + \"\"pa\"\")...but the problem i have and i dont know how to fix is, when i split the sentence, the output wont be in the same line, but every word will be at it\\'s own.\"\\n'\n3\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFcyzpUoAEXh",
        "colab_type": "text"
      },
      "source": [
        "Each label is an integer value between 0 and 3, correponsing to one of our four labels (0 to 3):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmXSZZBNt8aU",
        "colab_type": "text"
      },
      "source": [
        "### Prepare data for training\n",
        "\n",
        "Since the data is pre-processed, we do not need to make any additional steps like removing HTML tags, as we did in Part 1 of this notebook.\n",
        "\n",
        "We can go directly to instantiate our text vectorization layer (experimental feature). We are using this layer to normalize, split, and map strings to integers, so we set our `output_mode` to `int`. We also set the same constants as Part 1 for the model, like an explicit maximum `sequence_length`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKVrmuB-7fAm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "max_features = 5000\n",
        "embedding_dim = 128\n",
        "sequence_length = 500\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqO9ZzPV7kOL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make a text-only dataset (no labels) and call adapt\n",
        "text_ds = raw_train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(text_ds)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX6zq78yu3Fv",
        "colab_type": "text"
      },
      "source": [
        "### Vectorize the data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3gXSDzt7qni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return vectorize_layer(text), label\n",
        "\n",
        "# Vectorize the data.\n",
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)\n",
        "test_ds = raw_test_ds.map(vectorize_text)\n",
        "\n",
        "# Do async prefetching / buffering of the data for best performance on GPU.\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=10)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNfILbijqG5Z",
        "colab_type": "text"
      },
      "source": [
        "The vectorization layer transforms each input word of a sentence into a numerical representation, i.e. a list of token indices or vocabulary, with size defined by `max_features` (5000). Note that the output size is fixed, truncated by `sequence_length` (500), regardless of how many tokens resulted from the previous step, and this will be the input to our model.\n",
        "\n",
        "Let's take a moment to understand the output of the vectorization layer. The output of each sentence is fixed to 500 integers, as stated by `sequence_length`. It should be noted that most of the values are zero, and this is due to the fact that there is no corresponding token in our vocabulary.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQb51U7gRhYd",
        "colab_type": "code",
        "colab": {},
        "tags": []
      },
      "source": [
        "for text_batch, label_batch in train_ds.take(1):\n",
        "  for i in range(5):\n",
        "    print(text_batch.numpy()[i])\n",
        "    print(label_batch.numpy()[i])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[  22   40    3  139  490 1037    6    5   53    9 1131 1037    6   16\n   22   40    3  139   73    2  490  711    6    5 1131   53    9 1037\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0]\n3\n[ 139    2 3705    9  170    6    2   53   66    7    5  604    4  139\n   73    2 3705    9  170    6    2   53   87  132  181  159   53   12\n  138   66    7  300    5  846    9  170    6   53    1  170  310  918\n    6    2 2217   53   46   13 2046 3045 2928 1177    5    1    7   14\n 1732  170  310 4466  779   80  779   12  138   51    1   91   21 3240\n   99    2 2217   53  193  208   60   21 2668  115  142 3880   21    5\n    1    1   15 1790 3831  131   15 1790   29  185   19  248 1008   89\n  111  185   61    1  111 1008   12   29  172  185   27  172   61    1\n  172   11    1    1    1 1008  113   54 1008   89  185    3  118    4\n  501    2  283    6 1491    8  132  181    4    2 1854   23   10    7\n  407    4    5 1512    9  169  283   12  138  262    9    1    3   41\n    1    8  262    9    1    8   71  169  846    1    3   41    1    8\n 1325   22   34    3 2094    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0]\n1\n[   7   66    5 1013   39   28   46 1390   85  157    1    3   95   14\n    3   34  148 2146  251   48  303   33  570  415   12    1    1  580\n    1   66    5 1013   39   12  446    5  648   64   46    1   23  173\n 1859   64  503   47    1    3   17   28   46 2810    1 1120    5  648\n   64 1315    1  155 2061   58    1 2061  162    1  155 2061   25    1\n    1 2061    1  148  296  256    1   25 1432    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0]\n3\n[  16 1652    4  222  736 4105   11   13    7    5 2685 3811   17    5\n   53    9 3815  367  623  105    1    1 2954  492    1   35  412 1252\n  421  128    4  227  623   46   51    1    1    1    1    1    1    1\n    1    1    1   46    4   32  229    4   40    5   70    5   12  143\n    4   40   13   73   62  441   46    1    3    6    1    1    1    1\n    1  241    3  158  339    2   63    1   46   14    7   66    5   81\n    4  339   14   63  736    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0]\n3\n[   1  126  718    3  120    1   83  832    8    3 1972    4  112    5\n    1    1    2  668  311    9   13  253    7   14  151  236  213    6\n    5  891   56   91  286    2    1    1    3  532  486    2   28   12\n   14    1    7    2    1    1  562 2389  187    5  891   56   43    4\n 2175  330    1   12  330    6  330    1    1    2  121    3   17    8\n    3  130   95   22    4  399    7   47    3  502    2  891    2  126\n  612   32    6    2  116   72   23  236  213   74   32   62   97  622\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0]\n3\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evYhQerxvAq9",
        "colab_type": "text"
      },
      "source": [
        "### Build the model\n",
        "\n",
        "The input data consists of an array of integer-encoded vocabulary, with a fixed size. The labels to predict are between 0 and 3, so instead of using a binary classifier, we will use a softmax classifier. We compile the model with an Adam optimizer and a different loss function from Part 1 (Sparse categorical crossentropy).\n",
        "\n",
        "One of the parameters of the embedding layer is `max_features+1`and not `max_features`, and the reason is to add an extra token for an unknown word to our vocabulary in the input string.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMQEUVGM74iQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "# A integer input for vocab indices.\n",
        "inputs = tf.keras.Input(shape=(None,), dtype='int64')\n",
        "\n",
        "x = layers.Embedding(max_features + 1, embedding_dim)(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(128))(x)\n",
        "predictions = layers.Dense(4, activation='softmax', name='predictions')(x)\n",
        "\n",
        "model = tf.keras.Model(inputs, predictions)\n",
        "\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AHy6MOCvbm9",
        "colab_type": "text"
      },
      "source": [
        "### Train the model\n",
        "\n",
        "Train the model by passing the `Dataset` object to the model's fit function. Set the number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gs78f_N58MO9",
        "colab_type": "code",
        "colab": {},
        "tags": []
      },
      "source": [
        "epochs = 5\n",
        "\n",
        "# Fit the model using the train and test datasets.\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/5\n 225/1000 [=====>........................] - ETA: 11:23 - loss: 1.1989 - accuracy: 0.4439"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-25f03af6a77d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     epochs=epochs)\n\u001b[0m",
            "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    905\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    906\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 907\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    908\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    764\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    791\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    794\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2810\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2811\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2812\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2814\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1836\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m   1837\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1838\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1840\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1913\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1914\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1915\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1916\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1917\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    550\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5XaJfR1Pucj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hm8g4isKvebz",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate the model\n",
        "\n",
        "And let's see how the model performs. Two values will be returned: loss (a number which represents our error, lower values are better), and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbGYQ8sERUT7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss, accuracy = model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-LTtKU1r7lB",
        "colab_type": "text"
      },
      "source": [
        "## Learn more\n",
        "\n",
        "This notebook uses [tf.keras](https://www.tensorflow.org/guide/keras), a high-level API to build and train models in TensorFlow. For a more advanced text classification tutorial using `tf.keras`, see the [MLCC Text Classification Guide](https://developers.google.com/machine-learning/guides/text-classification/). In this notebook, we also use some TensorFlow experimental features, like the `TextVectorization` layer for word splitting & indexing."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Text classifier from scratch with TextVectorizaon layer.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}