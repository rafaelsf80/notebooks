{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Text Calssification using TextVectorization layer\n",
        "> Multiclass text classification from scratch\n",
        "\n",
        "- toc: true\n",
        "- badges: true\n",
        "- comments: true\n",
        "- categories: jupyter\n",
        "- image: images/chart-preview.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "Ic4_occAAiAT"
      },
      "outputs": [],
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "ioaprt5q5US7"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "yCl0eTNH5RS3",
        "colab": {}
      },
      "source": [
        "#@title MIT License\n",
        "#\n",
        "# Copyright (c) 2017 Fran√ßois Chollet\n",
        "#\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a\n",
        "# copy of this software and associated documentation files (the \"Software\"),\n",
        "# to deal in the Software without restriction, including without limitation\n",
        "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
        "# and/or sell copies of the Software, and to permit persons to whom the\n",
        "# Software is furnished to do so, subject to the following conditions:\n",
        "#\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "#\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
        "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
        "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
        "# DEALINGS IN THE SOFTWARE."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ItXfxkxvosLH"
      },
      "source": [
        "# Multiclass text classification from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hKY4XMc9o8iB"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/keras/text_classification\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/keras/text_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/text_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/keras/text_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Eg62Pmz3o83v"
      },
      "source": [
        "This notebook contains a walkthrough of text classification from scratch, starting from a directory of plain text files (a common scenario in practice). The first part demonstrates sentiment analysis, using an IMDB dataset. We demonstrate multiclass text classification, using a dataset of Stack Overflow questions.\n",
        "\n",
        "** TODO ** Also in the CL, update the title of the other text classification notebook to use transfer learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8RZOuS9LWQvv",
        "colab": {}
      },
      "source": [
        "!pip install -q tf-nightly\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2ew7HTbPpCJH",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras import preprocessing\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E47jant_rPVu",
        "colab_type": "text"
      },
      "source": [
        "## Multiclass text classification\n",
        "\n",
        "The second part of this notebook classifies Stack Overflow posts as one of the most used languages today, namely Java, Javascript, Python or C#. This is an example of *multiclass* classification, different from the binary classification showed in Part 1.\n",
        "\n",
        "We will use a [public dataset about Stack Overflow questions](https://console.cloud.google.com/marketplace/details/stack-exchange/stack-overflow) available in Google Cloud marketplace. You can explore the dataset in [BigQuery](https://cloud.google.com/bigquery/) just by following the instructions of the former link. In this notebook, you will build a model to predict the tags of questions from Stack Overflow, using a pre-processed table already built and coming from the BigQuery dataset. To keep things simple our pre-processed table includes questions containing 4 possible programming-related tags: Java, Javascript, Python or C#\n",
        "\n",
        "As in Part 1, Part 2 of this notebook uses [tf.keras](https://www.tensorflow.org/guide/keras) to build and train models in TensorFlow, as well as some TensorFlow experimental features, like the `TextVectorization` layer for word splitting & indexing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jxfg2FcfsD8f",
        "colab_type": "text"
      },
      "source": [
        "### Download the BigQuery dataset\n",
        "\n",
        "BigQuery has a  public dataset that includes more than 17 million Stack Overflow questions. We are going to download some posts labeled as one of the four most used languages today: java, javascript, python and C#, but to make this a harder problem to our model, we have replaced every instance of that word with another less used language today (but well-known some decades ago) called `fortran`. Otherwise, it will be very easy for the model to detect that a post is a java-related post just by finding the word `java` on it.\n",
        "\n",
        "You can access the pre-processed fortran-filled dataset as a tar file [here](PENDING). Each of the four labels has approximate 10k samples for training/eval and 10k samples for test.\n",
        "\n",
        "Let's first import libraries to make Part 2 of this notebook independent from Part 1, and then download our pre-processed dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8eRO0ng5r1A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gsutil cp gs://tensorflow-blog-rnn/so_posts_4labels_blank_80k.tar.gz .\n",
        "# TODO: MOVE DATASET TO TF.ORG\n",
        "!tar -xf so_posts_4labels_blank_80k.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkqtzHOb51ff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'train', batch_size=batch_size, validation_split=0.2, subset='training', seed=42)\n",
        "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'train', batch_size=batch_size, validation_split=0.2, subset='validation', seed=42)\n",
        "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'test', batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1UH5Prj4cqi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "start = time.time()\n",
        "for text_batch, label_batch in raw_train_ds:\n",
        "    pass\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UUbMuK35bp_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "start = time.time()\n",
        "for text_batch, label_batch in raw_train_ds:\n",
        "    pass\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZyy_aew4-aD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_train_ds = raw_train_ds.cache()\n",
        "import time\n",
        "start = time.time()\n",
        "for text_batch, label_batch in raw_train_ds:\n",
        "    pass\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDpqmGDK5Klw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "start = time.time()\n",
        "for text_batch, label_batch in raw_train_ds:\n",
        "    pass\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRLs33z1tUi_",
        "colab_type": "text"
      },
      "source": [
        "### Explore the data\n",
        "\n",
        "The dataset comes pre-processed, by replacing key words java, javascript, C# or python by `fortran`. In total, there are 4 labels (classes).\n",
        "\n",
        "Note again that, as in Part 1, we can evaluate tensors using `.numpy()`, thanks to the eager execution of TensorFlow 2:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQlq9CmP6tJX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "  for i in range(5):\n",
        "    print(text_batch.numpy()[i])\n",
        "    print(label_batch.numpy()[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFcyzpUoAEXh",
        "colab_type": "text"
      },
      "source": [
        "Each label is an integer value between 0 and 3, correponsing to one of our four labels (0 to 3):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmXSZZBNt8aU",
        "colab_type": "text"
      },
      "source": [
        "### Prepare data for training\n",
        "\n",
        "Since the data is pre-processed, we do not need to make any additional steps like removing HTML tags, as we did in Part 1 of this notebook.\n",
        "\n",
        "We can go directly to instantiate our text vectorization layer (experimental feature). We are using this layer to normalize, split, and map strings to integers, so we set our `output_mode` to `int`. We also set the same constants as Part 1 for the model, like an explicit maximum `sequence_length`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKVrmuB-7fAm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "max_features = 5000\n",
        "embedding_dim = 128\n",
        "sequence_length = 500\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqO9ZzPV7kOL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make a text-only dataset (no labels) and call adapt\n",
        "text_ds = raw_train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(text_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX6zq78yu3Fv",
        "colab_type": "text"
      },
      "source": [
        "### Vectorize the data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3gXSDzt7qni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return vectorize_layer(text), label\n",
        "\n",
        "# Vectorize the data.\n",
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)\n",
        "test_ds = raw_test_ds.map(vectorize_text)\n",
        "\n",
        "# Do async prefetching / buffering of the data for best performance on GPU.\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNfILbijqG5Z",
        "colab_type": "text"
      },
      "source": [
        "The vectorization layer transforms each input word of a sentence into a numerical representation, i.e. a list of token indices or vocabulary, with size defined by `max_features` (5000). Note that the output size is fixed, truncated by `sequence_length` (500), regardless of how many tokens resulted from the previous step, and this will be the input to our model.\n",
        "\n",
        "Let's take a moment to understand the output of the vectorization layer. The output of each sentence is fixed to 500 integers, as stated by `sequence_length`. It should be noted that most of the values are zero, and this is due to the fact that there is no corresponding token in our vocabulary.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQb51U7gRhYd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for text_batch, label_batch in train_ds.take(1):\n",
        "  for i in range(5):\n",
        "    print(text_batch.numpy()[i])\n",
        "    print(label_batch.numpy()[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evYhQerxvAq9",
        "colab_type": "text"
      },
      "source": [
        "### Build the model\n",
        "\n",
        "The input data consists of an array of integer-encoded vocabulary, with a fixed size. The labels to predict are between 0 and 3, so instead of using a binary classifier, we will use a softmax classifier. We compile the model with an Adam optimizer and a different loss function from Part 1 (Sparse categorical crossentropy).\n",
        "\n",
        "One of the parameters of the embedding layer is `max_features+1`and not `max_features`, and the reason is to add an extra token for an unknown word to our vocabulary in the input string.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMQEUVGM74iQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "# A integer input for vocab indices.\n",
        "inputs = tf.keras.Input(shape=(None,), dtype='int64')\n",
        "\n",
        "x = layers.Embedding(max_features + 1, embedding_dim)(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(128))(x)\n",
        "predictions = layers.Dense(4, activation='softmax', name='predictions')(x)\n",
        "\n",
        "model = tf.keras.Model(inputs, predictions)\n",
        "\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AHy6MOCvbm9",
        "colab_type": "text"
      },
      "source": [
        "### Train the model\n",
        "\n",
        "Train the model by passing the `Dataset` object to the model's fit function. Set the number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gs78f_N58MO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 5\n",
        "\n",
        "# Fit the model using the train and test datasets.\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5XaJfR1Pucj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hm8g4isKvebz",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate the model\n",
        "\n",
        "And let's see how the model performs. Two values will be returned: loss (a number which represents our error, lower values are better), and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbGYQ8sERUT7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss, accuracy = model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-LTtKU1r7lB",
        "colab_type": "text"
      },
      "source": [
        "## Learn more\n",
        "\n",
        "This notebook uses [tf.keras](https://www.tensorflow.org/guide/keras), a high-level API to build and train models in TensorFlow. For a more advanced text classification tutorial using `tf.keras`, see the [MLCC Text Classification Guide](https://developers.google.com/machine-learning/guides/text-classification/). In this notebook, we also use some TensorFlow experimental features, like the `TextVectorization` layer for word splitting & indexing."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Text classifier from scratch with TextVectorizaon layer.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}